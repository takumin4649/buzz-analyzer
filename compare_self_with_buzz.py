"""è‡ªåˆ†ã®æŠ•ç¨¿ã¨ãƒã‚ºæŠ•ç¨¿ã®è©³ç´°æ¯”è¼ƒåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime

import pandas as pd
import numpy as np


def load_self_posts(csv_path):
    """CSVã‹ã‚‰è‡ªåˆ†ã®æŠ•ç¨¿ã‚’èª­ã¿è¾¼ã‚€"""
    df = pd.read_csv(csv_path, encoding='utf-8')
    # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€
    df = df.rename(columns={
        'ãƒ†ã‚­ã‚¹ãƒˆ': 'æœ¬æ–‡',
        'ã„ã„ã­æ•°': 'ã„ã„ã­æ•°',
        'imp': 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°',
        'æŠ•ç¨¿æ—¥æ™‚': 'æŠ•ç¨¿æ—¥æ™‚',
        'ãƒªãƒ—ãƒ©ã‚¤æ•°': 'ãƒªãƒ—ãƒ©ã‚¤æ•°',
        'RTæ•°': 'ãƒªãƒã‚¹ãƒˆæ•°'
    })
    print(f"è‡ªåˆ†ã®æŠ•ç¨¿: {len(df)}ä»¶")
    return df


def load_buzz_posts(xlsx_path):
    """Excelã‹ã‚‰ãƒã‚ºæŠ•ç¨¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰"""
    df = pd.read_excel(xlsx_path)
    print(f"ãƒã‚ºæŠ•ç¨¿: {len(df)}ä»¶")

    # ç‚ä¸Šç³»é™¤å¤–
    exclude_keywords = [
        "è‘—ä½œæ¨©", "ç‰ˆæ¨©", "æµ·è³Šç‰ˆ", "åç›ŠåŒ–åœæ­¢", "åç›ŠåŒ–ãŒåœæ­¢",
        "å‰¥å¥ª", "ä¾µå®³", "ã‚¤ãƒ³ãƒ—ãƒ¬ã‚¾ãƒ³ãƒ“"
    ]
    def should_exclude(text):
        for kw in exclude_keywords:
            if kw in str(text):
                return True
        return False
    df = df[~df["æœ¬æ–‡"].apply(should_exclude)].copy()

    # åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼é‡è¤‡é™¤å»
    df = df.sort_values("ã„ã„ã­æ•°", ascending=False)
    df = df.drop_duplicates(subset=["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"], keep="first")
    print(f"ãƒã‚ºæŠ•ç¨¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰: {len(df)}ä»¶")
    return df.reset_index(drop=True)


def classify_opening_pattern(text):
    """å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡"""
    first_line = text.split("\n")[0].strip()

    if re.search(r"[\?ï¼Ÿ]", first_line):
        return "ç–‘å•ãƒ»å•ã„ã‹ã‘å‹"
    elif re.search(r"[!ï¼]{1,}", first_line) and len(first_line) < 30:
        return "ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçŸ­æ–‡å‹"
    elif re.search(r"\d+[é¸ã¤å€‹ä¸‡å††%]", first_line):
        return "æ•°å­—ãƒªã‚¹ãƒˆå‹"
    elif re.search(r"(çŸ¥ã‚‰ãªã„|çŸ¥ã‚‰ãªã‹ã£ãŸ|ã¾ã |å®Ÿã¯|ã¶ã£ã¡ã‚ƒã‘|æ­£ç›´|ã‚¬ãƒã§|ãƒã‚¸ã§)", first_line):
        return "ç§˜åŒ¿ãƒ»è¡æ’ƒäº‹å®Ÿå‹"
    elif re.search(r"(ã‚„ã‚|ã™ã‚‹ãª|ãƒ€ãƒ¡|ç¦æ­¢|æ³¨æ„|å±é™º|ãƒ¤ãƒã„|ã‚„ã°ã„)", first_line):
        return "è­¦å‘Šãƒ»å¦å®šå‹"
    elif re.search(r"(æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|ã‚¹ãƒ†ãƒƒãƒ—|æ‰‹é †|å§‹ã‚æ–¹|ç¨¼ãæ–¹|ç¨¼ã’ã‚‹)", first_line):
        return "ãƒã‚¦ãƒã‚¦æç¤ºå‹"
    elif re.search(r"(ã“ã‚Œ|ã“ã®|ã‚ã®|ã‚ã‚Œ)", first_line) and len(first_line) < 30:
        return "æŒ‡ç¤ºèªãƒ•ãƒƒã‚¯å‹"
    elif re.search(r"(åƒ•|ç§|ä¿º|è‡ªåˆ†|ãƒ¯ã‚¤)", first_line):
        return "ä½“é¨“è«‡ãƒ»è‡ªå·±é–‹ç¤ºå‹"
    elif re.search(r"(ãŠã™ã™ã‚|æœ€å¼·|ç¥|ä¾¿åˆ©|ç„¡æ–™|0å††)", first_line):
        return "æ¨è–¦ãƒ»çµ¶è³›å‹"
    else:
        return "ãã®ä»–"


def classify_structure_pattern(text):
    """æ–‡ç« æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡"""
    lines = [l.strip() for l in text.split("\n") if l.strip()]

    has_url = bool(re.search(r"https?://", text))
    has_list = bool(re.search(r"^[ãƒ»\-ãƒ»â–¶â–¸âœ…â˜‘âœ“â—†â– â—â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\d+[\.\)ï¼‰]]", text, re.MULTILINE))
    has_question = bool(re.search(r"[\?ï¼Ÿ]", lines[0] if lines else ""))
    has_cta = bool(re.search(r"(ãƒ•ã‚©ãƒ­ãƒ¼|ã„ã„ã­|ãƒªãƒ—|RT|ä¿å­˜|ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ|æ‹¡æ•£|ã‚·ã‚§ã‚¢|ãƒ–ã‚¯ãƒ|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯|ãƒ—ãƒ­ãƒ•|å›ºãƒ„ã‚¤|ãƒªãƒ³ã‚¯|è©³ç´°|ç¶šã)", text))
    has_self_story = bool(re.search(r"(åƒ•ã¯|ç§ã¯|ä¿ºã¯|è‡ªåˆ†ã¯|ãƒ¯ã‚¤ã¯|ã€œã—ãŸ|ã€œã ã£ãŸ|ã€œã—ã¦ãŸ|çµŒé¨“|ä½“é¨“)", text))

    if has_list and has_url:
        return "ãƒªã‚¹ãƒˆå‹ â†’ URLèª˜å°"
    elif has_list and has_cta:
        return "ãƒªã‚¹ãƒˆå‹ â†’ CTA"
    elif has_list:
        return "ãƒªã‚¹ãƒˆå‹ï¼ˆå˜ç‹¬ï¼‰"
    elif has_question and has_cta:
        return "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ CTA"
    elif has_question and has_url:
        return "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ URL"
    elif has_question:
        return "å•é¡Œæèµ· â†’ å›ç­”"
    elif has_self_story and has_cta:
        return "ä½“é¨“è«‡ â†’ æ•™è¨“ â†’ CTA"
    elif has_self_story and has_url:
        return "ä½“é¨“è«‡ â†’ URLèª˜å°"
    elif has_self_story:
        return "ä½“é¨“è«‡ â†’ æ•™è¨“"
    elif has_url and has_cta:
        return "ä¸»å¼µ â†’ URL + CTA"
    elif has_url:
        return "ä¸»å¼µ â†’ URLèª˜å°"
    elif has_cta:
        return "ä¸»å¼µ â†’ CTA"
    else:
        return "ä¸»å¼µã®ã¿ï¼ˆçŸ­æ–‡å®Œçµï¼‰"


def has_cta(text):
    """CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
    return bool(re.search(
        r"(ãƒ•ã‚©ãƒ­ãƒ¼|ã„ã„ã­|ãƒªãƒ—|RT|ä¿å­˜|ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ|æ‹¡æ•£|ã‚·ã‚§ã‚¢|ãƒ–ã‚¯ãƒ|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯|ãƒ—ãƒ­ãƒ•|å›ºãƒ„ã‚¤|ãƒªãƒ³ã‚¯|è©³ç´°|ç¶šã|DM|ğŸ‘‡|â†“|â¬‡)",
        text
    ))


def classify_emotion_trigger(text):
    """æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã‚’åˆ†é¡ï¼ˆè¤‡æ•°è©²å½“å¯èƒ½ï¼‰"""
    triggers = []

    # å…±æ„Ÿãƒ»è¦ªè¿‘æ„Ÿ
    if re.search(r"(ã‚ã‹ã‚‹|ãã†|ãã†ãã†|ã‚ã‚‹ã‚ã‚‹|å…±æ„Ÿ|åŒã˜|åƒ•ã‚‚|ç§ã‚‚|ä¿ºã‚‚)", text):
        triggers.append("å…±æ„Ÿãƒ»è¦ªè¿‘æ„Ÿ")

    # é©šããƒ»è¡æ’ƒ
    if re.search(r"(ãƒ¤ãƒã„|ã‚„ã°ã„|ãƒã‚¸ã§|ã‚¬ãƒã§|ã™ã”ã„|å‡„ã„|è¡æ’ƒ|ã³ã£ãã‚Š|ãƒ“ãƒƒã‚¯ãƒª|ãˆãã„|ã‚¨ã‚°ã„|çŸ¥ã‚‰ãªã‹ã£ãŸ)", text):
        triggers.append("é©šããƒ»è¡æ’ƒ")

    # ææ€–ãƒ»ä¸å®‰
    if re.search(r"(å±é™º|æ³¨æ„|ãƒ€ãƒ¡|ç¦æ­¢|ã‚„ã‚|æ°—ã‚’ã¤ã‘|çŸ¥ã‚‰ãªã„ã¨|æ|å¤±æ•—|å¾Œæ‚”|æ€–ã„)", text):
        triggers.append("ææ€–ãƒ»ä¸å®‰")

    # æœŸå¾…ãƒ»ãƒ¯ã‚¯ãƒ¯ã‚¯
    if re.search(r"(ç„¡æ–™|0å††|ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ|ç°¡å˜|ã™ã|ä»Šã™ã|ç¨¼ã’ã‚‹|å„²ã‹ã‚‹|æœ€å¼·|ç¥|ä¾¿åˆ©|ãŠã™ã™ã‚)", text):
        triggers.append("æœŸå¾…ãƒ»ãƒ¯ã‚¯ãƒ¯ã‚¯")

    # å¥½å¥‡å¿ƒ
    if re.search(r"(çŸ¥ã£ã¦ã‚‹|çŸ¥ã‚‰ãªã„|å®Ÿã¯|æœ¬å½“ã¯|æ„å¤–|ç§˜å¯†|è£æŠ€|ã‚³ãƒ„|æ–¹æ³•|ã‚„ã‚Šæ–¹)", text):
        triggers.append("å¥½å¥‡å¿ƒ")

    # æ€’ã‚Šãƒ»ä¸æº€
    if re.search(r"(ã²ã©ã„|é…·ã„|æœ€æ‚ª|è¨±ã›ãªã„|ãƒ ã‚«ã¤ã|è…¹ç«‹ã¤|ã‚¤ãƒ©ã‚¤ãƒ©)", text):
        triggers.append("æ€’ã‚Šãƒ»ä¸æº€")

    return triggers if triggers else ["æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—"]


def calculate_buzz_score(text, likes=0, rts=0, replies=0):
    """ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-100ç‚¹ï¼‰"""
    score = 0

    # 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ20ç‚¹ï¼‰
    opening_pattern = classify_opening_pattern(text)
    high_performing_patterns = ["ç–‘å•ãƒ»å•ã„ã‹ã‘å‹", "ç§˜åŒ¿ãƒ»è¡æ’ƒäº‹å®Ÿå‹", "è­¦å‘Šãƒ»å¦å®šå‹", "ãƒã‚¦ãƒã‚¦æç¤ºå‹"]
    if opening_pattern in high_performing_patterns:
        score += 20
    elif opening_pattern != "ãã®ä»–":
        score += 10

    # 2. æ–‡å­—æ•°ï¼ˆ15ç‚¹ï¼‰
    text_length = len(text)
    if 100 <= text_length <= 200:
        score += 15
    elif 80 <= text_length <= 250:
        score += 10
    elif text_length > 50:
        score += 5

    # 3. CTAã‚ã‚Šï¼ˆ15ç‚¹ï¼‰
    if has_cta(text):
        score += 15

    # 4. æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ï¼ˆ20ç‚¹ï¼‰
    triggers = classify_emotion_trigger(text)
    trigger_count = len([t for t in triggers if t != "æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—"])
    score += min(trigger_count * 7, 20)

    # 5. æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ15ç‚¹ï¼‰
    structure = classify_structure_pattern(text)
    high_performing_structures = ["ãƒªã‚¹ãƒˆå‹ â†’ CTA", "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ CTA", "ä½“é¨“è«‡ â†’ æ•™è¨“ â†’ CTA"]
    if structure in high_performing_structures:
        score += 15
    elif "CTA" in structure or "URL" in structure:
        score += 10
    elif structure != "ä¸»å¼µã®ã¿ï¼ˆçŸ­æ–‡å®Œçµï¼‰":
        score += 5

    # 6. çµµæ–‡å­—ä½¿ç”¨ï¼ˆ5ç‚¹ï¼‰
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # é¡”æ–‡å­—
        "\U0001F300-\U0001F5FF"  # è¨˜å·ãƒ»çµµæ–‡å­—
        "\U0001F680-\U0001F6FF"  # ä¹—ã‚Šç‰©ãƒ»å ´æ‰€
        "\U0001F1E0-\U0001F1FF"  # æ——
        "]+", flags=re.UNICODE
    )
    if emoji_pattern.search(text):
        score += 5

    # 7. æ”¹è¡Œãƒ»è¦‹ã‚„ã™ã•ï¼ˆ10ç‚¹ï¼‰
    lines = [l.strip() for l in text.split("\n") if l.strip()]
    if 3 <= len(lines) <= 10:
        score += 10
    elif len(lines) >= 2:
        score += 5

    return min(score, 100)


def analyze_time_slots(df):
    """æŠ•ç¨¿æ™‚é–“å¸¯åˆ¥ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åˆ†æ"""
    if 'æŠ•ç¨¿æ—¥æ™‚' not in df.columns or 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°' not in df.columns:
        return None

    def get_time_slot(dt_str):
        try:
            dt = pd.to_datetime(dt_str)
            hour = dt.hour
            if 5 <= hour < 9:
                return "æ—©æœ(5-9æ™‚)"
            elif 9 <= hour < 12:
                return "åˆå‰(9-12æ™‚)"
            elif 12 <= hour < 15:
                return "æ˜¼(12-15æ™‚)"
            elif 15 <= hour < 18:
                return "å¤•æ–¹(15-18æ™‚)"
            elif 18 <= hour < 21:
                return "å¤œ(18-21æ™‚)"
            elif 21 <= hour < 24:
                return "æ·±å¤œ(21-24æ™‚)"
            else:
                return "æ·±å¤œ(0-5æ™‚)"
        except:
            return "ä¸æ˜"

    df['æ™‚é–“å¸¯'] = df['æŠ•ç¨¿æ—¥æ™‚'].apply(get_time_slot)
    time_stats = df.groupby('æ™‚é–“å¸¯').agg({
        'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°': ['mean', 'sum', 'count']
    }).round(0)

    return time_stats


def compare_distributions(self_df, buzz_df):
    """åˆ†å¸ƒæ¯”è¼ƒã‚’å®Ÿè¡Œ"""
    results = {}

    # 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†å¸ƒ
    self_df['å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³'] = self_df['æœ¬æ–‡'].apply(lambda x: classify_opening_pattern(str(x)))
    buzz_df['å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³'] = buzz_df['æœ¬æ–‡'].apply(lambda x: classify_opening_pattern(str(x)))

    self_opening = self_df['å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³'].value_counts(normalize=True) * 100
    buzz_opening = buzz_df['å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³'].value_counts(normalize=True) * 100
    results['opening'] = (self_opening, buzz_opening)

    # 2. æ–‡ç« æ§‹æˆã®åˆ†å¸ƒ
    self_df['æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³'] = self_df['æœ¬æ–‡'].apply(lambda x: classify_structure_pattern(str(x)))
    buzz_df['æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³'] = buzz_df['æœ¬æ–‡'].apply(lambda x: classify_structure_pattern(str(x)))

    self_structure = self_df['æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³'].value_counts(normalize=True) * 100
    buzz_structure = buzz_df['æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³'].value_counts(normalize=True) * 100
    results['structure'] = (self_structure, buzz_structure)

    # 3. æ–‡å­—æ•°ã®æ¯”è¼ƒ
    self_df['æ–‡å­—æ•°'] = self_df['æœ¬æ–‡'].apply(lambda x: len(str(x)))
    buzz_df['æ–‡å­—æ•°'] = buzz_df['æœ¬æ–‡'].apply(lambda x: len(str(x)))
    results['text_length'] = (self_df['æ–‡å­—æ•°'], buzz_df['æ–‡å­—æ•°'])

    # 4. CTAä½¿ç”¨ç‡
    self_df['CTAæœ‰ç„¡'] = self_df['æœ¬æ–‡'].apply(lambda x: has_cta(str(x)))
    buzz_df['CTAæœ‰ç„¡'] = buzz_df['æœ¬æ–‡'].apply(lambda x: has_cta(str(x)))

    self_cta_rate = self_df['CTAæœ‰ç„¡'].sum() / len(self_df) * 100
    buzz_cta_rate = buzz_df['CTAæœ‰ç„¡'].sum() / len(buzz_df) * 100
    results['cta_rate'] = (self_cta_rate, buzz_cta_rate)

    # 5. æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã®åˆ†å¸ƒ
    self_df['æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼'] = self_df['æœ¬æ–‡'].apply(lambda x: classify_emotion_trigger(str(x)))
    buzz_df['æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼'] = buzz_df['æœ¬æ–‡'].apply(lambda x: classify_emotion_trigger(str(x)))

    # æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã‚’å±•é–‹ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    self_triggers = []
    for triggers in self_df['æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼']:
        self_triggers.extend(triggers)
    buzz_triggers = []
    for triggers in buzz_df['æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼']:
        buzz_triggers.extend(triggers)

    self_emotion = pd.Series(self_triggers).value_counts(normalize=True) * 100
    buzz_emotion = pd.Series(buzz_triggers).value_counts(normalize=True) * 100
    results['emotion'] = (self_emotion, buzz_emotion)

    # 6. ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢
    self_df['ãƒã‚ºã‚¹ã‚³ã‚¢'] = self_df.apply(
        lambda row: calculate_buzz_score(
            str(row['æœ¬æ–‡']),
            row.get('ã„ã„ã­æ•°', 0),
            row.get('ãƒªãƒã‚¹ãƒˆæ•°', 0),
            row.get('ãƒªãƒ—ãƒ©ã‚¤æ•°', 0)
        ), axis=1
    )
    buzz_df['ãƒã‚ºã‚¹ã‚³ã‚¢'] = buzz_df.apply(
        lambda row: calculate_buzz_score(
            str(row['æœ¬æ–‡']),
            row.get('ã„ã„ã­æ•°', 0),
            row.get('ãƒªãƒã‚¹ãƒˆæ•°', 0),
            row.get('ãƒªãƒ—ãƒ©ã‚¤æ•°', 0)
        ), axis=1
    )
    results['buzz_score'] = (self_df['ãƒã‚ºã‚¹ã‚³ã‚¢'], buzz_df['ãƒã‚ºã‚¹ã‚³ã‚¢'])

    return results, self_df, buzz_df


def generate_comparison_report(self_df, buzz_df, results, output_path):
    """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    lines = []
    lines.append("# è‡ªåˆ†ã®æŠ•ç¨¿ vs ãƒã‚ºæŠ•ç¨¿ è©³ç´°æ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append(f"\n**åˆ†ææ—¥**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append(f"**è‡ªåˆ†ã®æŠ•ç¨¿**: {len(self_df)}ä»¶ï¼ˆ@Mr_botenï¼‰")
    lines.append(f"**ãƒã‚ºæŠ•ç¨¿**: {len(buzz_df)}ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰")
    lines.append("")

    # ===== 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†å¸ƒæ¯”è¼ƒ =====
    lines.append("---")
    lines.append("## 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†å¸ƒæ¯”è¼ƒ")
    lines.append("")

    self_opening, buzz_opening = results['opening']
    all_patterns = sorted(set(self_opening.index) | set(buzz_opening.index))

    lines.append("| å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ | å·®åˆ† |")
    lines.append("|:---|---:|---:|---:|")
    for pattern in all_patterns:
        self_pct = self_opening.get(pattern, 0)
        buzz_pct = buzz_opening.get(pattern, 0)
        diff = self_pct - buzz_pct
        diff_str = f"+{diff:.1f}%" if diff > 0 else f"{diff:.1f}%"
        lines.append(f"| {pattern} | {self_pct:.1f}% | {buzz_pct:.1f}% | {diff_str} |")
    lines.append("")

    # æœ€ã‚‚å·®ãŒã‚ã‚‹ ãƒ‘ã‚¿ãƒ¼ãƒ³
    diffs = {p: self_opening.get(p, 0) - buzz_opening.get(p, 0) for p in all_patterns}
    most_over = max(diffs.items(), key=lambda x: x[1])
    most_under = min(diffs.items(), key=lambda x: x[1])

    lines.append(f"**ğŸ“Š åˆ†æ**: è‡ªåˆ†ã¯ã€Œ{most_over[0]}ã€ãŒ{most_over[1]:.1f}%å¤šãã€ã€Œ{most_under[0]}ã€ãŒ{abs(most_under[1]):.1f}%å°‘ãªã„")
    lines.append("")

    # ===== 2. æ–‡ç« æ§‹æˆã®å‹ã®åˆ†å¸ƒæ¯”è¼ƒ =====
    lines.append("---")
    lines.append("## 2. æ–‡ç« æ§‹æˆã®å‹ã®åˆ†å¸ƒæ¯”è¼ƒ")
    lines.append("")

    self_structure, buzz_structure = results['structure']
    all_structures = sorted(set(self_structure.index) | set(buzz_structure.index))

    lines.append("| æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ | å·®åˆ† |")
    lines.append("|:---|---:|---:|---:|")
    for struct in all_structures:
        self_pct = self_structure.get(struct, 0)
        buzz_pct = buzz_structure.get(struct, 0)
        diff = self_pct - buzz_pct
        diff_str = f"+{diff:.1f}%" if diff > 0 else f"{diff:.1f}%"
        lines.append(f"| {struct} | {self_pct:.1f}% | {buzz_pct:.1f}% | {diff_str} |")
    lines.append("")

    # CTAã‚’å«ã‚€æ§‹æˆã®å‰²åˆ
    self_cta_struct = sum(self_structure.get(s, 0) for s in self_structure.index if 'CTA' in s or 'URL' in s)
    buzz_cta_struct = sum(buzz_structure.get(s, 0) for s in buzz_structure.index if 'CTA' in s or 'URL' in s)
    lines.append(f"**ğŸ“Š åˆ†æ**: CTAã¾ãŸã¯URLèª˜å°ã‚’å«ã‚€æ§‹æˆã®å‰²åˆ - è‡ªåˆ†: {self_cta_struct:.1f}%ã€ãƒã‚ºæŠ•ç¨¿: {buzz_cta_struct:.1f}%")
    lines.append("")

    # ===== 3. æ–‡å­—æ•°ã®å¹³å‡ãƒ»åˆ†å¸ƒæ¯”è¼ƒ =====
    lines.append("---")
    lines.append("## 3. æ–‡å­—æ•°ã®å¹³å‡ãƒ»åˆ†å¸ƒæ¯”è¼ƒ")
    lines.append("")

    self_length, buzz_length = results['text_length']

    lines.append("| æŒ‡æ¨™ | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ | å·®åˆ† |")
    lines.append("|:---|---:|---:|---:|")
    lines.append(f"| å¹³å‡æ–‡å­—æ•° | {self_length.mean():.0f}æ–‡å­— | {buzz_length.mean():.0f}æ–‡å­— | {self_length.mean() - buzz_length.mean():.0f}æ–‡å­— |")
    lines.append(f"| ä¸­å¤®å€¤ | {self_length.median():.0f}æ–‡å­— | {buzz_length.median():.0f}æ–‡å­— | {self_length.median() - buzz_length.median():.0f}æ–‡å­— |")
    lines.append(f"| æœ€å° | {self_length.min():.0f}æ–‡å­— | {buzz_length.min():.0f}æ–‡å­— | - |")
    lines.append(f"| æœ€å¤§ | {self_length.max():.0f}æ–‡å­— | {buzz_length.max():.0f}æ–‡å­— | - |")
    lines.append(f"| æ¨™æº–åå·® | {self_length.std():.0f}æ–‡å­— | {buzz_length.std():.0f}æ–‡å­— | - |")
    lines.append("")

    # æ–‡å­—æ•°åˆ†å¸ƒ
    lines.append("### æ–‡å­—æ•°åˆ†å¸ƒ")
    lines.append("")
    bins = [0, 50, 100, 150, 200, 250, 300, 500, 1000]
    self_dist = pd.cut(self_length, bins=bins).value_counts(normalize=True).sort_index() * 100
    buzz_dist = pd.cut(buzz_length, bins=bins).value_counts(normalize=True).sort_index() * 100

    lines.append("| æ–‡å­—æ•°ç¯„å›² | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ |")
    lines.append("|:---|---:|---:|")
    for interval in self_dist.index:
        self_pct = self_dist.get(interval, 0)
        buzz_pct = buzz_dist.get(interval, 0)
        lines.append(f"| {interval} | {self_pct:.1f}% | {buzz_pct:.1f}% |")
    lines.append("")

    optimal_range = "100-200æ–‡å­—"
    self_optimal = sum(self_dist.iloc[2:4]) if len(self_dist) >= 4 else 0
    buzz_optimal = sum(buzz_dist.iloc[2:4]) if len(buzz_dist) >= 4 else 0
    lines.append(f"**ğŸ“Š åˆ†æ**: æœ€é©æ–‡å­—æ•°ç¯„å›²ï¼ˆ{optimal_range}ï¼‰ã®å‰²åˆ - è‡ªåˆ†: {self_optimal:.1f}%ã€ãƒã‚ºæŠ•ç¨¿: {buzz_optimal:.1f}%")
    lines.append("")

    # ===== 4. CTAä½¿ç”¨ç‡ã®æ¯”è¼ƒ =====
    lines.append("---")
    lines.append("## 4. CTAä½¿ç”¨ç‡ã®æ¯”è¼ƒ")
    lines.append("")

    self_cta_rate, buzz_cta_rate = results['cta_rate']

    lines.append("| æŒ‡æ¨™ | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ | å·®åˆ† |")
    lines.append("|:---|---:|---:|---:|")
    lines.append(f"| CTAä½¿ç”¨ç‡ | {self_cta_rate:.1f}% | {buzz_cta_rate:.1f}% | {self_cta_rate - buzz_cta_rate:.1f}% |")
    lines.append(f"| CTAæœ‰ã‚Š | {self_df['CTAæœ‰ç„¡'].sum()}ä»¶ | {buzz_df['CTAæœ‰ç„¡'].sum()}ä»¶ | - |")
    lines.append(f"| CTAç„¡ã— | {len(self_df) - self_df['CTAæœ‰ç„¡'].sum()}ä»¶ | {len(buzz_df) - buzz_df['CTAæœ‰ç„¡'].sum()}ä»¶ | - |")
    lines.append("")

    lines.append(f"**ğŸ“Š åˆ†æ**: ãƒã‚ºæŠ•ç¨¿ã®æ–¹ãŒCTAä½¿ç”¨ç‡ãŒ{abs(buzz_cta_rate - self_cta_rate):.1f}%é«˜ã„" if buzz_cta_rate > self_cta_rate else f"**ğŸ“Š åˆ†æ**: è‡ªåˆ†ã®æ–¹ãŒCTAä½¿ç”¨ç‡ãŒ{abs(self_cta_rate - buzz_cta_rate):.1f}%é«˜ã„")
    lines.append("")

    # ===== 5. æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã®åˆ†å¸ƒæ¯”è¼ƒ =====
    lines.append("---")
    lines.append("## 5. æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã®åˆ†å¸ƒæ¯”è¼ƒ")
    lines.append("")

    self_emotion, buzz_emotion = results['emotion']
    all_emotions = sorted(set(self_emotion.index) | set(buzz_emotion.index))

    lines.append("| æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ | å·®åˆ† |")
    lines.append("|:---|---:|---:|---:|")
    for emotion in all_emotions:
        self_pct = self_emotion.get(emotion, 0)
        buzz_pct = buzz_emotion.get(emotion, 0)
        diff = self_pct - buzz_pct
        diff_str = f"+{diff:.1f}%" if diff > 0 else f"{diff:.1f}%"
        lines.append(f"| {emotion} | {self_pct:.1f}% | {buzz_pct:.1f}% | {diff_str} |")
    lines.append("")

    # æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—ã®å‰²åˆ
    self_no_trigger = self_emotion.get("æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—", 0)
    buzz_no_trigger = buzz_emotion.get("æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—", 0)
    lines.append(f"**ğŸ“Š åˆ†æ**: æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—ã®å‰²åˆ - è‡ªåˆ†: {self_no_trigger:.1f}%ã€ãƒã‚ºæŠ•ç¨¿: {buzz_no_trigger:.1f}%")
    lines.append("")

    # ===== 6. ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ã®å¹³å‡ãƒ»åˆ†å¸ƒæ¯”è¼ƒ =====
    lines.append("---")
    lines.append("## 6. ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ã®å¹³å‡ãƒ»åˆ†å¸ƒæ¯”è¼ƒ")
    lines.append("")

    self_score, buzz_score = results['buzz_score']

    lines.append("| æŒ‡æ¨™ | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ | å·®åˆ† |")
    lines.append("|:---|---:|---:|---:|")
    lines.append(f"| å¹³å‡ã‚¹ã‚³ã‚¢ | {self_score.mean():.1f}ç‚¹ | {buzz_score.mean():.1f}ç‚¹ | {self_score.mean() - buzz_score.mean():.1f}ç‚¹ |")
    lines.append(f"| ä¸­å¤®å€¤ | {self_score.median():.1f}ç‚¹ | {buzz_score.median():.1f}ç‚¹ | {self_score.median() - buzz_score.median():.1f}ç‚¹ |")
    lines.append(f"| æœ€é«˜ã‚¹ã‚³ã‚¢ | {self_score.max():.1f}ç‚¹ | {buzz_score.max():.1f}ç‚¹ | - |")
    lines.append(f"| æœ€ä½ã‚¹ã‚³ã‚¢ | {self_score.min():.1f}ç‚¹ | {buzz_score.min():.1f}ç‚¹ | - |")
    lines.append("")

    # ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    lines.append("### ãƒã‚ºã‚¹ã‚³ã‚¢åˆ†å¸ƒ")
    lines.append("")
    score_bins = [0, 30, 50, 70, 85, 100]
    score_labels = ['0-30ç‚¹', '31-50ç‚¹', '51-70ç‚¹', '71-85ç‚¹', '86-100ç‚¹']
    self_score_dist = pd.cut(self_score, bins=score_bins, labels=score_labels).value_counts(normalize=True).sort_index() * 100
    buzz_score_dist = pd.cut(buzz_score, bins=score_bins, labels=score_labels).value_counts(normalize=True).sort_index() * 100

    lines.append("| ã‚¹ã‚³ã‚¢ç¯„å›² | è‡ªåˆ† | ãƒã‚ºæŠ•ç¨¿ |")
    lines.append("|:---|---:|---:|")
    for label in score_labels:
        self_pct = self_score_dist.get(label, 0)
        buzz_pct = buzz_score_dist.get(label, 0)
        lines.append(f"| {label} | {self_pct:.1f}% | {buzz_pct:.1f}% |")
    lines.append("")

    lines.append(f"**ğŸ“Š åˆ†æ**: ãƒã‚ºæŠ•ç¨¿ã®å¹³å‡ã‚¹ã‚³ã‚¢ãŒ{abs(buzz_score.mean() - self_score.mean()):.1f}ç‚¹é«˜ã„" if buzz_score.mean() > self_score.mean() else f"**ğŸ“Š åˆ†æ**: è‡ªåˆ†ã®å¹³å‡ã‚¹ã‚³ã‚¢ãŒ{abs(self_score.mean() - buzz_score.mean()):.1f}ç‚¹é«˜ã„")
    lines.append("")

    # ===== 7. è‡ªåˆ†ã«è¶³ã‚Šãªã„ã‚‚ã®ã®å…·ä½“çš„æŒ‡æ‘˜ =====
    lines.append("---")
    lines.append("## 7. ğŸ” è‡ªåˆ†ã«è¶³ã‚Šãªã„ã‚‚ã®ï¼ˆå…·ä½“çš„æŒ‡æ‘˜ï¼‰")
    lines.append("")

    gaps = []

    # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³
    buzz_top_opening = buzz_opening.nlargest(3)
    for pattern in buzz_top_opening.index:
        self_pct = self_opening.get(pattern, 0)
        buzz_pct = buzz_opening.get(pattern, 0)
        if buzz_pct - self_pct > 10:
            gaps.append(f"**å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã€Œ{pattern}ã€ã®ä½¿ç”¨ãŒå°‘ãªã„**: ãƒã‚ºæŠ•ç¨¿ã¯{buzz_pct:.1f}%ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€è‡ªåˆ†ã¯{self_pct:.1f}%ã®ã¿ï¼ˆå·®: {buzz_pct - self_pct:.1f}%ï¼‰")

    # æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³
    buzz_top_structure = buzz_structure.nlargest(3)
    for struct in buzz_top_structure.index:
        self_pct = self_structure.get(struct, 0)
        buzz_pct = buzz_structure.get(struct, 0)
        if buzz_pct - self_pct > 10:
            gaps.append(f"**æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€Œ{struct}ã€ã®ä½¿ç”¨ãŒå°‘ãªã„**: ãƒã‚ºæŠ•ç¨¿ã¯{buzz_pct:.1f}%ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€è‡ªåˆ†ã¯{self_pct:.1f}%ã®ã¿ï¼ˆå·®: {buzz_pct - self_pct:.1f}%ï¼‰")

    # CTAä½¿ç”¨ç‡
    if buzz_cta_rate - self_cta_rate > 10:
        gaps.append(f"**CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã®ä½¿ç”¨ãŒå°‘ãªã„**: ãƒã‚ºæŠ•ç¨¿ã¯{buzz_cta_rate:.1f}%ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€è‡ªåˆ†ã¯{self_cta_rate:.1f}%ã®ã¿ï¼ˆå·®: {buzz_cta_rate - self_cta_rate:.1f}%ï¼‰")

    # æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼
    buzz_top_emotion = buzz_emotion.nlargest(3)
    for emotion in buzz_top_emotion.index:
        if emotion == "æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—":
            continue
        self_pct = self_emotion.get(emotion, 0)
        buzz_pct = buzz_emotion.get(emotion, 0)
        if buzz_pct - self_pct > 10:
            gaps.append(f"**æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã€Œ{emotion}ã€ã®ä½¿ç”¨ãŒå°‘ãªã„**: ãƒã‚ºæŠ•ç¨¿ã¯{buzz_pct:.1f}%ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€è‡ªåˆ†ã¯{self_pct:.1f}%ã®ã¿ï¼ˆå·®: {buzz_pct - self_pct:.1f}%ï¼‰")

    # æ–‡å­—æ•°
    if abs(self_length.mean() - buzz_length.mean()) > 30:
        if self_length.mean() > buzz_length.mean():
            gaps.append(f"**æ–‡ç« ãŒé•·ã™ãã‚‹**: è‡ªåˆ†ã®å¹³å‡{self_length.mean():.0f}æ–‡å­—ã«å¯¾ã—ã€ãƒã‚ºæŠ•ç¨¿ã¯å¹³å‡{buzz_length.mean():.0f}æ–‡å­—ï¼ˆå·®: {self_length.mean() - buzz_length.mean():.0f}æ–‡å­—ï¼‰")
        else:
            gaps.append(f"**æ–‡ç« ãŒçŸ­ã™ãã‚‹**: è‡ªåˆ†ã®å¹³å‡{self_length.mean():.0f}æ–‡å­—ã«å¯¾ã—ã€ãƒã‚ºæŠ•ç¨¿ã¯å¹³å‡{buzz_length.mean():.0f}æ–‡å­—ï¼ˆå·®: {buzz_length.mean() - self_length.mean():.0f}æ–‡å­—ï¼‰")

    # ãƒã‚ºã‚¹ã‚³ã‚¢
    if buzz_score.mean() - self_score.mean() > 5:
        gaps.append(f"**ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ãŒä½ã„**: è‡ªåˆ†ã®å¹³å‡{self_score.mean():.1f}ç‚¹ã«å¯¾ã—ã€ãƒã‚ºæŠ•ç¨¿ã¯å¹³å‡{buzz_score.mean():.1f}ç‚¹ï¼ˆå·®: {buzz_score.mean() - self_score.mean():.1f}ç‚¹ï¼‰")

    for i, gap in enumerate(gaps, 1):
        lines.append(f"{i}. {gap}")
    lines.append("")

    # ===== 8. ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã¨ã„ã„ã­æ•°ã®é–¢ä¿‚ =====
    lines.append("---")
    lines.append("## 8. ğŸ“Š ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã¨ã„ã„ã­æ•°ã®é–¢ä¿‚ï¼ˆ@Mr_botenã®ã¿ï¼‰")
    lines.append("")

    if 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°' in self_df.columns and 'ã„ã„ã­æ•°' in self_df.columns:
        # ã‚¤ãƒ³ãƒ—â†’ã„ã„ã­è»¢æ›ç‡
        self_df['è»¢æ›ç‡'] = (self_df['ã„ã„ã­æ•°'] / self_df['ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°'] * 100).fillna(0)
        avg_conversion = self_df['è»¢æ›ç‡'].mean()

        lines.append("| æŒ‡æ¨™ | å€¤ |")
        lines.append("|:---|---:|")
        lines.append(f"| å¹³å‡ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° | {self_df['ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°'].mean():.0f} |")
        lines.append(f"| å¹³å‡ã„ã„ã­æ•° | {self_df['ã„ã„ã­æ•°'].mean():.1f} |")
        lines.append(f"| å¹³å‡è»¢æ›ç‡ï¼ˆã„ã„ã­/ã‚¤ãƒ³ãƒ—ï¼‰ | {avg_conversion:.2f}% |")
        lines.append(f"| æœ€é«˜è»¢æ›ç‡ | {self_df['è»¢æ›ç‡'].max():.2f}% |")
        lines.append(f"| æœ€ä½è»¢æ›ç‡ | {self_df['è»¢æ›ç‡'].min():.2f}% |")
        lines.append("")

        # è»¢æ›ç‡TOP5ã®æŠ•ç¨¿
        lines.append("### è»¢æ›ç‡TOP5ã®æŠ•ç¨¿")
        lines.append("")
        top_conversion = self_df.nlargest(5, 'è»¢æ›ç‡')
        for idx, row in top_conversion.iterrows():
            text_preview = str(row['æœ¬æ–‡'])[:50].replace('\n', ' ')
            lines.append(f"- **{row['è»¢æ›ç‡']:.2f}%** (ã‚¤ãƒ³ãƒ—: {row['ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°']:.0f}, ã„ã„ã­: {row['ã„ã„ã­æ•°']}) - ã€Œ{text_preview}...ã€")
        lines.append("")

        lines.append(f"**ğŸ“Š åˆ†æ**: å¹³å‡è»¢æ›ç‡ã¯{avg_conversion:.2f}%ã€‚è»¢æ›ç‡ã‚’ä¸Šã’ã‚‹ã«ã¯ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’ç²å¾—ã—ãŸäººã«ã‚ˆã‚Šã€Œã„ã„ã­ã€ã—ãŸããªã‚‹å†…å®¹ã«æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚")
        lines.append("")
    else:
        lines.append("ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚åˆ†æä¸å¯ï¼‰")
        lines.append("")

    # ===== 9. æŠ•ç¨¿æ™‚é–“å¸¯åˆ¥ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° =====
    lines.append("---")
    lines.append("## 9. â° æŠ•ç¨¿æ™‚é–“å¸¯åˆ¥ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ï¼ˆ@Mr_botenã®ã¿ï¼‰")
    lines.append("")

    time_stats = analyze_time_slots(self_df)
    if time_stats is not None:
        lines.append("| æ™‚é–“å¸¯ | æŠ•ç¨¿æ•° | åˆè¨ˆã‚¤ãƒ³ãƒ— | å¹³å‡ã‚¤ãƒ³ãƒ— |")
        lines.append("|:---|---:|---:|---:|")
        for time_slot in time_stats.index:
            count = int(time_stats.loc[time_slot, ('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', 'count')])
            total = int(time_stats.loc[time_slot, ('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', 'sum')])
            avg = int(time_stats.loc[time_slot, ('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', 'mean')])
            lines.append(f"| {time_slot} | {count} | {total} | {avg} |")
        lines.append("")

        # æœ€ã‚‚ã‚¤ãƒ³ãƒ—ãŒé«˜ã„æ™‚é–“å¸¯
        best_time_idx = time_stats[('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', 'mean')].idxmax()
        best_time_avg = int(time_stats.loc[best_time_idx, ('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', 'mean')])
        lines.append(f"**ğŸ“Š åˆ†æ**: æœ€ã‚‚ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãŒé«˜ã„æ™‚é–“å¸¯ã¯ã€Œ{best_time_idx}ã€ï¼ˆå¹³å‡{best_time_avg}ã‚¤ãƒ³ãƒ—ï¼‰")
        lines.append("")
    else:
        lines.append("ï¼ˆæŠ•ç¨¿æ—¥æ™‚ã¾ãŸã¯ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚åˆ†æä¸å¯ï¼‰")
        lines.append("")

    # ===== 10. æœ€ã‚‚ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãŒé«˜ã‹ã£ãŸæŠ•ç¨¿ã®ç‰¹å¾´åˆ†æ =====
    lines.append("---")
    lines.append("## 10. ğŸ† æœ€ã‚‚ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãŒé«˜ã‹ã£ãŸæŠ•ç¨¿ã®ç‰¹å¾´åˆ†æï¼ˆ@Mr_botenã®ã¿ï¼‰")
    lines.append("")

    if 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°' in self_df.columns:
        top_imp_post = self_df.nlargest(1, 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°').iloc[0]

        lines.append(f"### ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {top_imp_post['ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°']:.0f}")
        lines.append(f"- **ã„ã„ã­æ•°**: {top_imp_post.get('ã„ã„ã­æ•°', 'N/A')}")
        lines.append(f"- **ãƒªãƒã‚¹ãƒˆæ•°**: {top_imp_post.get('ãƒªãƒã‚¹ãƒˆæ•°', 'N/A')}")
        lines.append(f"- **ãƒªãƒ—ãƒ©ã‚¤æ•°**: {top_imp_post.get('ãƒªãƒ—ãƒ©ã‚¤æ•°', 'N/A')}")
        lines.append(f"- **æŠ•ç¨¿æ—¥æ™‚**: {top_imp_post.get('æŠ•ç¨¿æ—¥æ™‚', 'N/A')}")
        lines.append(f"- **æ–‡å­—æ•°**: {top_imp_post['æ–‡å­—æ•°']}æ–‡å­—")
        lines.append(f"- **å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³**: {top_imp_post['å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³']}")
        lines.append(f"- **æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³**: {top_imp_post['æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³']}")
        lines.append(f"- **CTAæœ‰ç„¡**: {'ã‚ã‚Š' if top_imp_post['CTAæœ‰ç„¡'] else 'ãªã—'}")
        lines.append(f"- **æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼**: {', '.join(top_imp_post['æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼'])}")
        lines.append(f"- **ãƒã‚ºã‚¹ã‚³ã‚¢**: {top_imp_post['ãƒã‚ºã‚¹ã‚³ã‚¢']:.1f}ç‚¹")
        if 'è»¢æ›ç‡' in top_imp_post:
            lines.append(f"- **è»¢æ›ç‡**: {top_imp_post['è»¢æ›ç‡']:.2f}%")
        lines.append("")

        lines.append("#### å…¨æ–‡")
        lines.append("```")
        lines.append(str(top_imp_post['æœ¬æ–‡']))
        lines.append("```")
        lines.append("")

        lines.append("**ğŸ“Š åˆ†æ**: ã“ã®æŠ•ç¨¿ãŒæœ€ã‚‚ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’ç²å¾—ã—ãŸç†ç”±ã‚’åˆ†æã—ã€å†ç¾å¯èƒ½ãªè¦ç´ ã‚’æŠ½å‡ºã™ã‚‹ã€‚")
        lines.append("")
    else:
        lines.append("ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚åˆ†æä¸å¯ï¼‰")
        lines.append("")

    # ===== 11. æ‹“å·³ãŒãƒã‚ºã‚‹ãŸã‚ã«æ˜æ—¥ã‹ã‚‰å¤‰ãˆã‚‹ã¹ãã“ã¨TOP3 =====
    lines.append("---")
    lines.append("## ğŸš€ æ‹“å·³ãŒãƒã‚ºã‚‹ãŸã‚ã«æ˜æ—¥ã‹ã‚‰å¤‰ãˆã‚‹ã¹ãã“ã¨ TOP3")
    lines.append("")

    # ã‚®ãƒ£ãƒƒãƒ—ã¨çµ±è¨ˆã‹ã‚‰å„ªå…ˆé †ä½ä»˜ã‘
    recommendations = []

    # 1. CTAä½¿ç”¨ç‡
    if buzz_cta_rate - self_cta_rate > 10:
        impact = buzz_cta_rate - self_cta_rate
        recommendations.append({
            'title': 'CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã‚’æŠ•ç¨¿ã«å¿…ãšå…¥ã‚Œã‚‹',
            'impact': impact,
            'detail': f"ãƒã‚ºæŠ•ç¨¿ã®{buzz_cta_rate:.0f}%ãŒCTAã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã«å¯¾ã—ã€è‡ªåˆ†ã¯{self_cta_rate:.0f}%ã®ã¿ã€‚"
                     f"ã€Œã„ã„ã­ãƒ»RTãŠé¡˜ã„ã—ã¾ã™ã€ã€Œãƒ—ãƒ­ãƒ•ã‚‚ãƒã‚§ãƒƒã‚¯ã€ãªã©ã®è¡Œå‹•å–šèµ·ã‚’æŠ•ç¨¿ã®æœ€å¾Œã«è¿½åŠ ã™ã‚‹ã€‚",
            'example': 'ä¾‹: ã€Œå‚è€ƒã«ãªã£ãŸã‚‰ã„ã„ã­ï¼†RTãŠé¡˜ã„ã—ã¾ã™ğŸ™ã€ã€Œç¶šãã¯ãƒ—ãƒ­ãƒ•ã®ãƒªãƒ³ã‚¯ã‹ã‚‰ğŸ‘‡ã€'
        })

    # 2. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³
    for pattern in buzz_opening.nlargest(2).index:
        self_pct = self_opening.get(pattern, 0)
        buzz_pct = buzz_opening.get(pattern, 0)
        if buzz_pct - self_pct > 15:
            impact = buzz_pct - self_pct
            recommendations.append({
                'title': f'å†’é ­ã‚’ã€Œ{pattern}ã€ã«å¤‰ãˆã‚‹',
                'impact': impact,
                'detail': f"ãƒã‚ºæŠ•ç¨¿ã®{buzz_pct:.0f}%ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼ˆè‡ªåˆ†ã¯{self_pct:.0f}%ï¼‰ã€‚"
                         f"æŠ•ç¨¿ã®æœ€åˆã®1-2è¡Œã‚’å·¥å¤«ã—ã¦èª­è€…ã®èˆˆå‘³ã‚’å¼•ãã€‚",
                'example': get_opening_example(pattern)
            })

    # 3. æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼
    for emotion in buzz_emotion.nlargest(2).index:
        if emotion == "æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãªã—":
            continue
        self_pct = self_emotion.get(emotion, 0)
        buzz_pct = buzz_emotion.get(emotion, 0)
        if buzz_pct - self_pct > 15:
            impact = buzz_pct - self_pct
            recommendations.append({
                'title': f'æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã€Œ{emotion}ã€ã‚’æ„è­˜çš„ã«ä½¿ã†',
                'impact': impact,
                'detail': f"ãƒã‚ºæŠ•ç¨¿ã®{buzz_pct:.0f}%ãŒã“ã®ãƒˆãƒªã‚¬ãƒ¼ã‚’ä½¿ç”¨ï¼ˆè‡ªåˆ†ã¯{self_pct:.0f}%ï¼‰ã€‚"
                         f"èª­è€…ã®{emotion}ã‚’åˆºæ¿€ã™ã‚‹è¨€è‘‰ã‚„è¡¨ç¾ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã€‚",
                'example': get_emotion_example(emotion)
            })

    # 4. æ–‡å­—æ•°
    if abs(self_length.mean() - buzz_length.mean()) > 50:
        impact = abs(self_length.mean() - buzz_length.mean()) / 10
        if self_length.mean() > buzz_length.mean():
            recommendations.append({
                'title': 'æ–‡ç« ã‚’çŸ­ãç°¡æ½”ã«ã¾ã¨ã‚ã‚‹',
                'impact': impact,
                'detail': f"è‡ªåˆ†ã®å¹³å‡{self_length.mean():.0f}æ–‡å­—ã¯é•·ã™ãã‚‹ã€‚ãƒã‚ºæŠ•ç¨¿ã®å¹³å‡{buzz_length.mean():.0f}æ–‡å­—ã‚’ç›®æŒ‡ã™ã€‚"
                         f"æœ€é©ãªæ–‡å­—æ•°ã¯100-200æ–‡å­—ç¨‹åº¦ã€‚",
                'example': 'ä¾‹: å†—é•·ãªè¡¨ç¾ã‚’å‰Šã‚Šã€ç®‡æ¡æ›¸ãã‚„ãƒªã‚¹ãƒˆå½¢å¼ã§è¦‹ã‚„ã™ãã¾ã¨ã‚ã‚‹'
            })
        else:
            recommendations.append({
                'title': 'æ–‡ç« ã‚’ã‚‚ã†å°‘ã—è©³ã—ãæ›¸ã',
                'impact': impact,
                'detail': f"è‡ªåˆ†ã®å¹³å‡{self_length.mean():.0f}æ–‡å­—ã¯çŸ­ã™ãã‚‹ã€‚ãƒã‚ºæŠ•ç¨¿ã®å¹³å‡{buzz_length.mean():.0f}æ–‡å­—ã‚’ç›®æŒ‡ã™ã€‚"
                         f"æœ€é©ãªæ–‡å­—æ•°ã¯100-200æ–‡å­—ç¨‹åº¦ã€‚",
                'example': 'ä¾‹: ç†ç”±ã‚„å…·ä½“ä¾‹ã‚’è¿½åŠ ã—ã¦ã€èª­è€…ã«ã‚ˆã‚Šå¤šãã®ä¾¡å€¤ã‚’æä¾›ã™ã‚‹'
            })

    # 5. æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³
    for struct in buzz_structure.nlargest(2).index:
        self_pct = self_structure.get(struct, 0)
        buzz_pct = buzz_structure.get(struct, 0)
        if buzz_pct - self_pct > 15:
            impact = buzz_pct - self_pct
            recommendations.append({
                'title': f'æ§‹æˆã‚’ã€Œ{struct}ã€ã«ã™ã‚‹',
                'impact': impact,
                'detail': f"ãƒã‚ºæŠ•ç¨¿ã®{buzz_pct:.0f}%ãŒã“ã®æ§‹æˆã‚’ä½¿ç”¨ï¼ˆè‡ªåˆ†ã¯{self_pct:.0f}%ï¼‰ã€‚"
                         f"æŠ•ç¨¿å…¨ä½“ã®æµã‚Œã‚’ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æ²¿ã£ã¦çµ„ã¿ç«‹ã¦ã‚‹ã€‚",
                'example': get_structure_example(struct)
            })

    # ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé †ã«ã‚½ãƒ¼ãƒˆ
    recommendations.sort(key=lambda x: x['impact'], reverse=True)

    # TOP3ã‚’è¡¨ç¤º
    for i, rec in enumerate(recommendations[:3], 1):
        lines.append(f"### {i}ä½: {rec['title']}")
        lines.append("")
        lines.append(f"**ğŸ“Œ å„ªå…ˆåº¦**: {'é«˜' if rec['impact'] > 25 else 'ä¸­' if rec['impact'] > 15 else 'ä½'} ï¼ˆå·®åˆ†: {rec['impact']:.1f}%ï¼‰")
        lines.append("")
        lines.append(f"**ğŸ“ è©³ç´°**: {rec['detail']}")
        lines.append("")
        lines.append(f"**ğŸ’¡ å…·ä½“ä¾‹**: {rec['example']}")
        lines.append("")
        lines.append("---")
        lines.append("")

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"\næ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å®Œäº†: {output_path}")
    return output_path


def get_opening_example(pattern):
    """å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…·ä½“ä¾‹ã‚’è¿”ã™"""
    examples = {
        "ç–‘å•ãƒ»å•ã„ã‹ã‘å‹": "ä¾‹: ã€Œãªãœ9å‰²ã®äººã¯Claude Codeã§æŒ«æŠ˜ã™ã‚‹ã®ã‹ï¼Ÿã€ã€Œã‚ãªãŸã¯ã¾ã æ‰‹å‹•ã§ã‚³ãƒ¼ãƒ‰æ›¸ã„ã¦ã‚‹ã®ï¼Ÿã€",
        "ç§˜åŒ¿ãƒ»è¡æ’ƒäº‹å®Ÿå‹": "ä¾‹: ã€Œãƒã‚¸ã§çŸ¥ã‚‰ãªã‹ã£ãŸã€‚Claude Codeã£ã¦â—‹â—‹ã‚‚ã§ãã‚‹ã‚“ã ...ã€ã€Œã‚¬ãƒã§ãƒ¤ãƒã„ã€‚ã“ã‚ŒçŸ¥ã‚‰ãªã„äººæã—ã¦ã‚‹ã‚ã€",
        "è­¦å‘Šãƒ»å¦å®šå‹": "ä¾‹: ã€Œã“ã‚Œã‚„ã‚ã¨ã‘ã€‚Claude Codeã§çµ¶å¯¾ã‚„ã£ã¦ã¯ã„ã‘ãªã„3ã¤ã®ã“ã¨ã€ã€Œæ³¨æ„ï¼ã“ã®ä½¿ã„æ–¹ã¯å±é™ºã§ã™ã€",
        "ãƒã‚¦ãƒã‚¦æç¤ºå‹": "ä¾‹: ã€ŒClaude Codeã§æœˆ10ä¸‡ç¨¼ãæ–¹æ³•ã‚’æ•™ãˆã¾ã™ã€ã€Œ3ã‚¹ãƒ†ãƒƒãƒ—ã§è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«ã‚’ä½œã‚‹æ‰‹é †ã€",
        "æ•°å­—ãƒªã‚¹ãƒˆå‹": "ä¾‹: ã€ŒClaude Codeä½¿ã„å€’ã™ãŸã‚ã®5ã¤ã®ã‚³ãƒ„ã€ã€Œ3æ—¥ã§ä½œã£ãŸå‰¯æ¥­ãƒ„ãƒ¼ãƒ«ã§5ä¸‡å††ç¨¼ã„ã è©±ã€",
        "æŒ‡ç¤ºèªãƒ•ãƒƒã‚¯å‹": "ä¾‹: ã€Œã“ã‚Œã€ãƒã‚¸ã§å…¨å“¡ã‚„ã£ãŸæ–¹ãŒã„ã„ã€ã€Œã“ã®æ–¹æ³•ã€ã‚‚ã£ã¨æ—©ãçŸ¥ã‚ŠãŸã‹ã£ãŸã€",
        "ä½“é¨“è«‡ãƒ»è‡ªå·±é–‹ç¤ºå‹": "ä¾‹: ã€Œåƒ•ãŒClaude Codeã§äººç”Ÿå¤‰ã‚ã£ãŸè©±ã€ã€Œãƒ‰ç´ äººã®è‡ªåˆ†ã§ã‚‚3æ—¥ã§ãƒ„ãƒ¼ãƒ«ä½œã‚ŒãŸã€",
        "æ¨è–¦ãƒ»çµ¶è³›å‹": "ä¾‹: ã€Œã“ã‚Œæœ€å¼·ã€‚Claude Codeã®ç¥æ©Ÿèƒ½è¦‹ã¤ã‘ãŸã€ã€Œç„¡æ–™ã§ã“ã“ã¾ã§ã§ãã‚‹ã¨ã‹ã€æ§ãˆã‚ã«è¨€ã£ã¦ç¥ã€",
        "ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçŸ­æ–‡å‹": "ä¾‹: ã€Œãƒ¤ãƒã™ãã‚‹ï¼ã€ã€Œã“ã‚Œã‚¨ã‚°ã„ã€‚ã€ã€Œãƒã‚¸ã‹...ã€"
    }
    return examples.get(pattern, "ï¼ˆå…·ä½“ä¾‹ãªã—ï¼‰")


def get_emotion_example(emotion):
    """æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã®å…·ä½“ä¾‹ã‚’è¿”ã™"""
    examples = {
        "å…±æ„Ÿãƒ»è¦ªè¿‘æ„Ÿ": "ä¾‹: ã€Œã‚ã‹ã‚‹ã€ãã†ãã†ã€ã€Œåƒ•ã‚‚ãã†æ€ã£ã¦ãŸã€ã€Œã‚ã‚‹ã‚ã‚‹ã ã‚ˆã­ã€",
        "é©šããƒ»è¡æ’ƒ": "ä¾‹: ã€Œãƒã‚¸ã§ãƒ¤ãƒã„ã€ã€Œã™ã”ã™ãã‚‹ã€ã€ŒçŸ¥ã‚‰ãªã‹ã£ãŸã€ã€Œã“ã‚Œã‚¨ã‚°ã„ã€",
        "ææ€–ãƒ»ä¸å®‰": "ä¾‹: ã€Œã“ã‚ŒçŸ¥ã‚‰ãªã„ã¨æã™ã‚‹ã€ã€Œã‚„ã‚ã¨ã‘ã€ã€Œå±é™ºã€ã€Œå¤±æ•—ã™ã‚‹å‰ã«ã€",
        "æœŸå¾…ãƒ»ãƒ¯ã‚¯ãƒ¯ã‚¯": "ä¾‹: ã€Œç„¡æ–™ã§ã§ãã‚‹ã€ã€Œç°¡å˜ã«ç¨¼ã’ã‚‹ã€ã€Œä»Šã™ãè©¦ã›ã‚‹ã€ã€Œæœ€å¼·ã€",
        "å¥½å¥‡å¿ƒ": "ä¾‹: ã€Œå®Ÿã¯ã€ã€Œæ„å¤–ã¨ã€ã€ŒçŸ¥ã£ã¦ãŸï¼Ÿã€ã€Œè£æŠ€ã€ã€Œã‚³ãƒ„ã€",
        "æ€’ã‚Šãƒ»ä¸æº€": "ä¾‹: ã€Œè¨±ã›ãªã„ã€ã€Œã²ã©ã™ãã‚‹ã€ã€Œæœ€æ‚ªã€ã€Œãƒ ã‚«ã¤ãã€"
    }
    return examples.get(emotion, "ï¼ˆå…·ä½“ä¾‹ãªã—ï¼‰")


def get_structure_example(struct):
    """æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…·ä½“ä¾‹ã‚’è¿”ã™"""
    examples = {
        "ãƒªã‚¹ãƒˆå‹ â†’ CTA": "ä¾‹: ç®‡æ¡æ›¸ãã§ãƒã‚¤ãƒ³ãƒˆã‚’åˆ—æŒ™ â†’ ã€Œè©³ã—ãã¯ãƒ—ãƒ­ãƒ•ã‹ã‚‰ã€",
        "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ CTA": "ä¾‹: ã€Œãªãœç¨¼ã’ãªã„ã®ã‹ï¼Ÿã€â†’ ç†ç”±ã¨è§£æ±ºç­– â†’ ã€Œãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦æœ€æ–°æƒ…å ±ãƒã‚§ãƒƒã‚¯ã€",
        "ä½“é¨“è«‡ â†’ æ•™è¨“ â†’ CTA": "ä¾‹: è‡ªåˆ†ã®ä½“é¨“ã‚’èªã‚‹ â†’ å­¦ã‚“ã ã“ã¨ â†’ ã€ŒåŒã˜å¤±æ•—ã—ãŸããªã„äººã¯RTã€",
        "ä¸»å¼µ â†’ CTA": "ä¾‹: å¼·ã„ä¸»å¼µã‚„æ„è¦‹ â†’ ã€Œå…±æ„Ÿã—ãŸã‚‰ã„ã„ã­ã€",
        "ãƒªã‚¹ãƒˆå‹ â†’ URLèª˜å°": "ä¾‹: ãƒã‚¦ãƒã‚¦ã‚’ç®‡æ¡æ›¸ã â†’ ã€Œç¶šãã¯ã“ã¡ã‚‰ [URL]ã€",
        "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ URL": "ä¾‹: å•é¡Œã‚’æŠ•ã’ã‹ã‘ã‚‹ â†’ è§£æ±ºç­–ã‚’ç¤ºã™ â†’ ã€Œè©³ç´°è¨˜äº‹ã¯ã“ã¡ã‚‰ã€"
    }
    return examples.get(struct, "ï¼ˆå…·ä½“ä¾‹ãªã—ï¼‰")


if __name__ == "__main__":
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    self_csv = "output/TwExport_20260217_191942.csv"
    buzz_xlsx = "output/buzz_posts_20260215.xlsx"
    output_md = "output/self_comparison_20260217.md"

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("=== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ ===")
    self_df = load_self_posts(self_csv)
    buzz_df = load_buzz_posts(buzz_xlsx)

    # æ¯”è¼ƒåˆ†æå®Ÿè¡Œ
    print("\n=== æ¯”è¼ƒåˆ†æå®Ÿè¡Œä¸­ ===")
    results, self_df, buzz_df = compare_distributions(self_df, buzz_df)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n=== ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ ===")
    generate_comparison_report(self_df, buzz_df, results, output_md)

    print("\n=== åˆ†æå®Œäº† ===")
