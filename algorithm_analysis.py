"""Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Xã®å…¬é–‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆPhoenix/Grokï¼‰ã«åŸºã¥ã„ãŸæŠ•ç¨¿åˆ†æã€‚
ä»¥ä¸‹ã®åˆ†æã‚’æä¾›ï¼š
1. Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé‡ã¿ãƒ™ãƒ¼ã‚¹ï¼‰
2. è­°è«–èª˜ç™ºåº¦ï¼ˆãƒªãƒ—ãƒ©ã‚¤/ã„ã„ã­æ¯”ç‡ï¼‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¾¡å€¤
3. ã‚¹ãƒ¬ãƒƒãƒ‰æ§‹é€ æ¤œå‡º
4. å¤–éƒ¨ãƒªãƒ³ã‚¯æ¤œå‡ºã¨ãƒªãƒ¼ãƒæ¸›è¡°æ¨å®š
5. ãƒˆãƒ¼ãƒ³åˆ†æï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/å»ºè¨­çš„ï¼‰
6. æ»åœ¨æ™‚é–“æ¨å®šï¼ˆæ–‡å­—æ•°Ã—æ§‹é€ ã«ã‚ˆã‚‹æ¨å®šï¼‰
7. æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆäºˆæ¸¬

ã‚½ãƒ¼ã‚¹: xai-org/x-algorithm, twitter/the-algorithm, Grok AIåˆ†æ
"""

import re
from collections import Counter, defaultdict
from datetime import datetime

import pandas as pd

from analyze_posts import (
    classify_category,
    classify_opening_pattern,
    has_story,
    safe_get,
)


# ========================================
# Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å…¬å¼é‡ã¿ï¼ˆ2026å¹´2æœˆç‰ˆï¼‰
# ========================================

X_ALGORITHM_WEIGHTS = {
    "author_reply":     75.0,   # è‘—è€…è‡ªèº«ãŒãƒªãƒ—ã«è¿”ä¿¡
    "reply":            13.5,   # ãƒªãƒ—ãƒ©ã‚¤
    "profile_click":    12.0,   # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚¯ãƒªãƒƒã‚¯
    "conversation_click": 11.0, # ä¼šè©±ã‚¯ãƒªãƒƒã‚¯ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å±•é–‹ï¼‰
    "bookmark":         10.0,   # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯
    "retweet":           1.0,   # RT
    "like":              0.5,   # ã„ã„ã­
    "dwell_2min":       10.0,   # æ»åœ¨æ™‚é–“2åˆ†ä»¥ä¸Š
    "negative":        -74.0,   # éè¡¨ç¤ºãªã©
    "report":         -369.0,   # é€šå ±
}

# Grokæ™‚ä»£ã®è¿½åŠ ãƒ–ãƒ¼ã‚¹ãƒˆä¿‚æ•°
GROK_MULTIPLIERS = {
    "x_premium":         4.0,   # PremiumåŠ å…¥è€…ã®æŠ•ç¨¿ãƒ–ãƒ¼ã‚¹ãƒˆ
    "text_vs_video":     1.3,   # ãƒ†ã‚­ã‚¹ãƒˆã¯å‹•ç”»ã‚ˆã‚Š30%å¼·ã„
    "thread_boost":      3.0,   # ã‚¹ãƒ¬ãƒƒãƒ‰ã¯å˜ç™ºã®3å€
    "external_link_penalty": 0.5,  # å¤–éƒ¨ãƒªãƒ³ã‚¯ã§50%æ¸›
}


# ========================================
# 1. Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ã‚³ã‚¢äºˆæ¸¬
# ========================================

def calculate_algorithm_score(text, likes=0, retweets=0, replies=0,
                               bookmarks=0, has_premium=False):
    """Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ãã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆ0-100ç‚¹ï¼‰

    ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‹ã‚‰ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç¢ºç‡ã‚’æ¨å®šã—ã€
    å…¬å¼é‡ã¿ã§åŠ é‡ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã™ã‚‹ã€‚
    """
    factors = {}
    total = 0

    # --- 1. ãƒªãƒ—ãƒ©ã‚¤èª˜ç™ºåŠ› (25ç‚¹) ---
    # ãƒªãƒ—ãƒ©ã‚¤é‡ã¿13.5 + è‘—è€…è¿”ä¿¡75.0 â†’ æœ€é‡è¦
    reply_triggers = 0
    # ç–‘å•å½¢ï¼ˆãƒªãƒ—ãƒ©ã‚¤èª˜ç™ºï¼‰
    if re.search(r'[\?ï¼Ÿ]', text):
        reply_triggers += 3
    # æ„è¦‹ã‚’æ±‚ã‚ã‚‹è¡¨ç¾
    if re.search(r'(ã©ã†æ€|æ•™ãˆã¦|ã¿ã‚“ãªã¯|çš†ã•ã‚“ã¯|ã‚ãªãŸã¯|èããŸã„|çŸ¥ã‚ŠãŸã„)', text):
        reply_triggers += 4
    # è­°è«–ã‚’ç”Ÿã‚€å¯¾ç«‹æ§‹é€ 
    if re.search(r'(vs|VS|ãã‚Œã¨ã‚‚|ã©ã£ã¡|Aã‹Bã‹|è³›å¦|è­°è«–)', text):
        reply_triggers += 3
    # ä½“é¨“å…±æœ‰ã®èª˜ç™º
    if re.search(r'(åŒã˜äºº|çµŒé¨“ã‚ã‚‹|ã‚„ã£ãŸã“ã¨ã‚ã‚‹|ã‚ã‹ã‚‹äºº|å…±æ„Ÿ)', text):
        reply_triggers += 2
    # è‡ªå·±é–‹ç¤ºï¼ˆå…±æ„Ÿãƒªãƒ—ãƒ©ã‚¤èª˜ç™ºï¼‰
    if re.search(r'(æ­£ç›´|ã¶ã£ã¡ã‚ƒã‘|å®Ÿã¯|å‘Šç™½|æœ¬éŸ³)', text):
        reply_triggers += 2
    # ãƒ„ãƒƒã‚³ãƒŸã©ã“ã‚ï¼ˆæ„å›³çš„ãªéš™ï¼‰
    if re.search(r'(ã‹ã‚‚ã—ã‚Œãªã„|çŸ¥ã‚‰ã‚“ã‘ã©|ç•°è«–ã¯èªã‚ã‚‹|æ€’ã‚‰ã‚Œãã†)', text):
        reply_triggers += 2

    s = min(25, reply_triggers * 3)
    factors["ãƒªãƒ—ãƒ©ã‚¤èª˜ç™ºåŠ›"] = s
    total += s

    # --- 2. æ»åœ¨æ™‚é–“æ¨å®š (20ç‚¹) ---
    # dwell_2min = +10.0ã®é‡ã¿
    dwell_score = estimate_dwell_time_score(text)
    s = min(20, dwell_score)
    factors["æ»åœ¨æ™‚é–“"] = s
    total += s

    # --- 3. ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»ä¼šè©±æ·±åº¦ (15ç‚¹) ---
    # conversation_click = 11.0ã®é‡ã¿
    thread_info = detect_thread_structure(text)
    if thread_info["is_thread_starter"]:
        s = 15  # ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ = 3å€ãƒ–ãƒ¼ã‚¹ãƒˆ
    elif thread_info["has_continuation_hint"]:
        s = 10  # ç¶šããŒã‚ã‚Šãã†
    elif thread_info["invites_conversation"]:
        s = 8   # ä¼šè©±ã‚’èª˜ã†
    else:
        s = 0
    factors["ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»ä¼šè©±"] = s
    total += s

    # --- 4. ãƒˆãƒ¼ãƒ³è©•ä¾¡ (15ç‚¹) ---
    # GrokãŒãƒˆãƒ¼ãƒ³ã‚’ç›´æ¥è©•ä¾¡
    tone = analyze_tone(text)
    if tone["overall"] == "å»ºè¨­çš„":
        s = 15
    elif tone["overall"] == "ãƒã‚¸ãƒ†ã‚£ãƒ–":
        s = 12
    elif tone["overall"] == "ä¸­ç«‹":
        s = 8
    elif tone["overall"] == "ç…½ã‚Šï¼ˆå»ºè¨­çš„ï¼‰":
        s = 10  # å»ºè¨­çš„ãªå•é¡Œæèµ·ã¯è©•ä¾¡ã•ã‚Œã‚‹
    elif tone["overall"] == "ãƒã‚¬ãƒ†ã‚£ãƒ–":
        s = 3
    else:  # æ”»æ’ƒçš„
        s = 0
    factors["ãƒˆãƒ¼ãƒ³"] = s
    total += s

    # --- 5. ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯èª˜ç™ºåŠ› (10ç‚¹) ---
    # bookmark = 10.0ã®é‡ã¿
    bookmark_triggers = 0
    if re.search(r'(ä¿å­˜|ãƒ–ã‚¯ãƒ|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯|ãƒ¡ãƒ¢|å¾Œã§)', text):
        bookmark_triggers += 3
    # ãƒªã‚¹ãƒˆãƒ»ãƒã‚¦ãƒã‚¦ï¼ˆä¿å­˜ã—ãŸããªã‚‹ï¼‰
    if re.search(r'(é¸|ã¤ã®ã‚³ãƒ„|ã¤ã®æ–¹æ³•|ã‚¹ãƒ†ãƒƒãƒ—|æ‰‹é †|ã¾ã¨ã‚|ä¸€è¦§|ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ)', text):
        bookmark_triggers += 3
    # å…·ä½“çš„ãªæ•°å­—ï¼ˆä¿å­˜ä¾¡å€¤ãŒé«˜ã„ï¼‰
    if re.search(r'\d+[ä¸‡å††å€‹ä»¶ã¤%]', text):
        bookmark_triggers += 2
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    if re.search(r'(ãƒ†ãƒ³ãƒ—ãƒ¬|ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯|å‹|ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ|é››å½¢)', text):
        bookmark_triggers += 3

    s = min(10, bookmark_triggers * 2)
    factors["ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯èª˜ç™º"] = s
    total += s

    # --- 6. å¤–éƒ¨ãƒªãƒ³ã‚¯ãƒšãƒŠãƒ«ãƒ†ã‚£ (-15ç‚¹) ---
    link_info = detect_external_links(text)
    if link_info["has_external_link"]:
        s = -15  # 30-50%ãƒªãƒ¼ãƒæ¸›
    elif link_info["has_x_link"]:
        s = -3   # Xå†…ãƒªãƒ³ã‚¯ã¯è»½å¾®
    else:
        s = 0
    factors["å¤–éƒ¨ãƒªãƒ³ã‚¯"] = s
    total += s

    # --- 7. ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚¯ãƒªãƒƒã‚¯èª˜ç™º (10ç‚¹) ---
    # profile_click = 12.0ã®é‡ã¿
    profile_triggers = 0
    if re.search(r'(ãƒ—ãƒ­ãƒ•|å›ºãƒ„ã‚¤|å›ºå®šãƒ„ã‚¤ãƒ¼ãƒˆ|è‡ªå·±ç´¹ä»‹)', text):
        profile_triggers += 2
    # æ¨©å¨æ€§ï¼ˆèª°ï¼Ÿã¨æ°—ã«ãªã‚‹ï¼‰
    if re.search(r'(å¹´ç›®|æœˆç›®|ä¸‡ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|å®Ÿç¸¾|çµŒæ­´|å°‚é–€)', text):
        profile_triggers += 2
    # ãƒŸã‚¹ãƒ†ãƒªã‚¢ã‚¹ã•
    if re.search(r'(ç§˜å¯†|å†…ç·’|ã“ã“ã ã‘|é™å®š|éå…¬é–‹)', text):
        profile_triggers += 2
    # è‡ªå·±é–‹ç¤ºï¼ˆã‚‚ã£ã¨çŸ¥ã‚ŠãŸã„ï¼‰
    if re.search(r'(åƒ•|ç§|ä¿º).{0,10}(å®Ÿã¯|æ­£ç›´|ã¶ã£ã¡ã‚ƒã‘)', text):
        profile_triggers += 2

    s = min(10, profile_triggers * 2)
    factors["ãƒ—ãƒ­ãƒ•ã‚¯ãƒªãƒƒã‚¯èª˜ç™º"] = s
    total += s

    # --- 8. æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆäºˆæ¸¬ (5ç‚¹) ---
    # æœ€åˆã®1æ™‚é–“ã§50%ãŒæ±ºã¾ã‚‹ â†’ å³åº§ã«ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã—ã‚„ã™ã„æŠ•ç¨¿ã‹
    early_triggers = 0
    # çŸ­ã„ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯èƒ½ï¼ˆã™ãã„ã„ã­ãƒ»ãƒªãƒ—ã—ã‚„ã™ã„ï¼‰
    first_line = text.split("\n")[0] if text else ""
    if len(first_line) <= 40 and re.search(r'[ï¼!ï¼Ÿ?]', first_line):
        early_triggers += 2
    # æ„Ÿæƒ…çš„åå¿œã‚’å¼•ãå‡ºã™
    if re.search(r'(ãƒã‚¸ã§|ã‚¬ãƒã§|ãƒ¤ãƒã„|ã‚„ã°ã„|ã™ã”ã„|ç¥|æœ€å¼·|è¡æ’ƒ)', text):
        early_triggers += 2
    # çŸ­æ–‡ã§å®Œçµï¼ˆã™ãèª­ã‚ã‚‹ = ã™ããƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
    if len(text) <= 140:
        early_triggers += 1

    s = min(5, early_triggers * 2)
    factors["æ—©æœŸåå¿œæ€§"] = s
    total += s

    total = max(0, min(100, total))

    return {
        "total_score": total,
        "factors": factors,
        "algorithm_weights_used": True,
    }


# ========================================
# 2. è­°è«–èª˜ç™ºåº¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¾¡å€¤åˆ†æ
# ========================================

def analyze_discussion_algorithm_value(df):
    """è­°è«–èª˜ç™ºåº¦ã‚’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é‡ã¿ãƒ™ãƒ¼ã‚¹ã§åˆ†æ

    ãƒªãƒ—ãƒ©ã‚¤ã¯ã„ã„ã­ã®27å€ã€è‘—è€…è¿”ä¿¡ã¯150å€ã®ä¾¡å€¤ã€‚
    ã“ã®é‡ã¿ã‚’ä½¿ã£ã¦ã€ŒçœŸã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ã‚³ã‚¢ã€ã‚’æ¨å®šã™ã‚‹ã€‚
    """
    results = []

    for _, row in df.iterrows():
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        text = safe_get(row, "æœ¬æ–‡", "")
        user = safe_get(row, "ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "")

        if likes <= 0:
            continue

        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ é‡ã‚¹ã‚³ã‚¢ï¼ˆå…¬å¼é‡ã¿ãƒ™ãƒ¼ã‚¹ï¼‰
        weighted_score = (
            likes * X_ALGORITHM_WEIGHTS["like"] +
            retweets * X_ALGORITHM_WEIGHTS["retweet"] +
            replies * X_ALGORITHM_WEIGHTS["reply"]
        )

        # å¾“æ¥ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢
        simple_score = likes + retweets * 2 + replies * 3

        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ é‡ vs å˜ç´”ã‚¹ã‚³ã‚¢ã®ä¹–é›¢
        # ä¹–é›¢ãŒå¤§ãã„ = ãƒªãƒ—ãƒ©ã‚¤ãŒå¤šã„ = ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒé«˜è©•ä¾¡
        ratio = weighted_score / simple_score if simple_score > 0 else 0

        # è­°è«–èª˜ç™ºç‡
        discussion_rate = replies / likes if likes > 0 else 0

        results.append({
            "text": text[:60],
            "user": user,
            "likes": likes,
            "retweets": retweets,
            "replies": replies,
            "discussion_rate": discussion_rate,
            "weighted_score": weighted_score,
            "simple_score": simple_score,
            "algorithm_boost": ratio,
            "category": classify_category(text),
        })

    results.sort(key=lambda x: x["weighted_score"], reverse=True)

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ã‚³ã‚¢
    cat_scores = defaultdict(list)
    for r in results:
        cat_scores[r["category"]].append(r["weighted_score"])

    return {
        "top10_by_algorithm": results[:10],
        "top10_by_discussion": sorted(results, key=lambda x: x["discussion_rate"], reverse=True)[:10],
        "avg_weighted": sum(r["weighted_score"] for r in results) / len(results) if results else 0,
        "avg_discussion_rate": sum(r["discussion_rate"] for r in results) / len(results) if results else 0,
        "cat_algorithm_scores": {k: sum(v) / len(v) for k, v in cat_scores.items() if v},
        "all_results": results,
    }


# ========================================
# 3. ã‚¹ãƒ¬ãƒƒãƒ‰æ§‹é€ æ¤œå‡º
# ========================================

def detect_thread_structure(text):
    """æŠ•ç¨¿ãŒã‚¹ãƒ¬ãƒƒãƒ‰å½¢å¼ã‹ã©ã†ã‹ã‚’æ¤œå‡º

    ã‚¹ãƒ¬ãƒƒãƒ‰ã¯å˜ç™ºæŠ•ç¨¿ã®3å€ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã€‚
    ä¼šè©±ã‚¯ãƒªãƒƒã‚¯é‡ã¿ = 11.0ã€‚
    """
    indicators = {
        "is_thread_starter": False,
        "has_continuation_hint": False,
        "invites_conversation": False,
        "thread_signals": [],
    }

    # ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ã®ã‚·ã‚°ãƒŠãƒ«
    thread_start_patterns = [
        (r'[ğŸ§µã‚¹ãƒ¬ãƒƒãƒ‰]', "ã‚¹ãƒ¬ãƒƒãƒ‰æ˜ç¤º"),
        (r'(1/\d|â‘ |1\.)', "ç•ªå·ä»˜ãé–‹å§‹"),
        (r'(ä»¥ä¸‹|â†“|ğŸ‘‡|â¬‡)', "ç¶šãã‚’ç¤ºå”†"),
        (r'(é•·ããªã‚‹ã®ã§|é€£æŠ•ã—ã¾ã™|ã‚¹ãƒ¬ã«ã—ã¾ã™)', "ã‚¹ãƒ¬ãƒƒãƒ‰å®£è¨€"),
    ]
    for pattern, signal in thread_start_patterns:
        if re.search(pattern, text):
            indicators["is_thread_starter"] = True
            indicators["thread_signals"].append(signal)

    # ç¶šããŒã‚ã‚Šãã†ãªã‚·ã‚°ãƒŠãƒ«
    continuation_patterns = [
        (r'(ç¶šã|ã¤ã¥ã|ç¶šãã¯|æ¬¡ã¯)', "ç¶šãç¤ºå”†"),
        (r'(ã¾ãš|æœ€åˆã«|ç¬¬ä¸€ã«)', "é †åºé–‹å§‹"),
        (r'\.{3,}$|â€¦$', "ä½™éŸ»ï¼ˆç¶šãã‚ã‚Šï¼‰"),
    ]
    for pattern, signal in continuation_patterns:
        if re.search(pattern, text):
            indicators["has_continuation_hint"] = True
            indicators["thread_signals"].append(signal)

    # ä¼šè©±ã‚’èª˜ã†ã‚·ã‚°ãƒŠãƒ«
    conversation_patterns = [
        (r'[\?ï¼Ÿ]$', "ç–‘å•ã§çµ‚ã‚ã‚‹"),
        (r'(ã©ã†æ€|æ•™ãˆã¦|ã¿ã‚“ãªã¯|æ„è¦‹)', "æ„è¦‹ã‚’æ±‚ã‚ã‚‹"),
        (r'(ã‚ãªãŸã¯|å›ã¯|çš†ã•ã‚“ã¯)', "ç›´æ¥å•ã„ã‹ã‘"),
    ]
    for pattern, signal in conversation_patterns:
        if re.search(pattern, text):
            indicators["invites_conversation"] = True
            indicators["thread_signals"].append(signal)

    return indicators


def analyze_thread_potential(df):
    """å…¨æŠ•ç¨¿ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’åˆ†æ"""
    thread_posts = []
    non_thread_posts = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)

        thread_info = detect_thread_structure(text)
        is_thread_like = (
            thread_info["is_thread_starter"] or
            thread_info["has_continuation_hint"]
        )

        entry = {
            "text": text[:60],
            "likes": likes,
            "retweets": retweets,
            "replies": replies,
            "thread_info": thread_info,
        }

        if is_thread_like:
            thread_posts.append(entry)
        else:
            non_thread_posts.append(entry)

    # æ¯”è¼ƒçµ±è¨ˆ
    def avg(posts, key):
        if not posts:
            return 0
        return sum(p[key] for p in posts) / len(posts)

    return {
        "thread_count": len(thread_posts),
        "non_thread_count": len(non_thread_posts),
        "thread_avg_likes": avg(thread_posts, "likes"),
        "non_thread_avg_likes": avg(non_thread_posts, "likes"),
        "thread_avg_replies": avg(thread_posts, "replies"),
        "non_thread_avg_replies": avg(non_thread_posts, "replies"),
        "thread_posts": thread_posts,
        "non_thread_posts": non_thread_posts,
    }


# ========================================
# 4. å¤–éƒ¨ãƒªãƒ³ã‚¯æ¤œå‡ºã¨ãƒªãƒ¼ãƒæ¸›è¡°æ¨å®š
# ========================================

def detect_external_links(text):
    """å¤–éƒ¨ãƒªãƒ³ã‚¯ã®æ¤œå‡º

    å¤–éƒ¨ãƒªãƒ³ã‚¯ â†’ 30-50%ãƒªãƒ¼ãƒæ¸›ã€‚
    Xå†…ãƒªãƒ³ã‚¯ï¼ˆx.com, twitter.comï¼‰ã¯è»½å¾®ã€‚
    """
    url_pattern = re.compile(r'https?://([^\s/]+)')
    urls = url_pattern.findall(text)

    x_domains = {"x.com", "twitter.com", "t.co", "pbs.twimg.com"}
    external_urls = [u for u in urls if not any(d in u for d in x_domains)]
    x_urls = [u for u in urls if any(d in u for d in x_domains)]

    # ãƒªãƒ¼ãƒæ¸›è¡°æ¨å®š
    if external_urls:
        reach_multiplier = 0.5  # 50%æ¸›
    elif x_urls:
        reach_multiplier = 0.95  # 5%æ¸›
    else:
        reach_multiplier = 1.0  # æ¸›è¡°ãªã—

    return {
        "has_external_link": len(external_urls) > 0,
        "has_x_link": len(x_urls) > 0,
        "external_domains": external_urls,
        "x_links": x_urls,
        "reach_multiplier": reach_multiplier,
        "penalty_pct": round((1 - reach_multiplier) * 100),
    }


def analyze_link_impact(df):
    """å…¨æŠ•ç¨¿ã®ãƒªãƒ³ã‚¯æœ‰ç„¡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ"""
    with_external = []
    with_x_link = []
    no_link = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)

        link_info = detect_external_links(text)

        entry = {
            "likes": likes, "retweets": retweets, "replies": replies,
            "text": text[:60], "link_info": link_info,
        }

        if link_info["has_external_link"]:
            with_external.append(entry)
        elif link_info["has_x_link"]:
            with_x_link.append(entry)
        else:
            no_link.append(entry)

    def avg(posts, key):
        if not posts:
            return 0
        return sum(p[key] for p in posts) / len(posts)

    return {
        "external_count": len(with_external),
        "x_link_count": len(with_x_link),
        "no_link_count": len(no_link),
        "external_avg_likes": avg(with_external, "likes"),
        "x_link_avg_likes": avg(with_x_link, "likes"),
        "no_link_avg_likes": avg(no_link, "likes"),
        "reach_penalty_confirmed": avg(no_link, "likes") > avg(with_external, "likes"),
    }


# ========================================
# 5. ãƒˆãƒ¼ãƒ³åˆ†æï¼ˆGrokè©•ä¾¡æ¨å®šï¼‰
# ========================================

def analyze_tone(text):
    """æŠ•ç¨¿ã®ãƒˆãƒ¼ãƒ³ã‚’åˆ†æï¼ˆGrokã®è©•ä¾¡ã‚’æ¨å®šï¼‰

    Grokã¯ä»¥ä¸‹ã‚’è©•ä¾¡:
    - ãƒã‚¸ãƒ†ã‚£ãƒ–/å»ºè¨­çš„ â†’ æ‹¡æ•£ä¿ƒé€²
    - æ”»æ’ƒçš„/rage bait â†’ æŠ‘åˆ¶
    """
    scores = {
        "positive": 0,
        "constructive": 0,
        "negative": 0,
        "aggressive": 0,
        "neutral": 0,
    }

    # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚·ã‚°ãƒŠãƒ«
    positive_patterns = [
        r'å¬‰ã—ã„|æ¥½ã—ã„|å¹¸ã›|æœ€é«˜|ç´ æ™´ã‚‰ã—ã„|æ„Ÿè¬|ã‚ã‚ŠãŒã¨ã†',
        r'ãŠã™ã™ã‚|è‰¯ã„|å¥½ã|ç´ æ•µ|ç¥|ä¾¿åˆ©|åŠ©ã‹ã‚‹',
        r'æˆåŠŸ|é”æˆ|å®Ÿç¾|ã§ããŸ|ã‚„ã£ãŸ|é ‘å¼µ',
        r'ãƒ¯ã‚¯ãƒ¯ã‚¯|æœŸå¾…|æ¥½ã—ã¿|é¢ç™½ã„',
    ]
    for p in positive_patterns:
        if re.search(p, text):
            scores["positive"] += 1

    # å»ºè¨­çš„ã‚·ã‚°ãƒŠãƒ«ï¼ˆæœ€ã‚‚è©•ä¾¡ã•ã‚Œã‚‹ï¼‰
    constructive_patterns = [
        r'æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|ã‚¹ãƒ†ãƒƒãƒ—|æ‰‹é †|å§‹ã‚æ–¹',
        r'è§£æ±º|æ”¹å–„|å¯¾ç­–|ææ¡ˆ|ã‚¢ãƒ‰ãƒã‚¤ã‚¹',
        r'å­¦ã‚“ã |æ°—ã¥ã„ãŸ|ç™ºè¦‹|ã‚ã‹ã£ãŸ|ç†è§£',
        r'å…±æœ‰|ã‚·ã‚§ã‚¢|ç´¹ä»‹|ã¾ã¨ã‚|ãƒ¬ãƒ“ãƒ¥ãƒ¼',
        r'çµŒé¨“|ä½“é¨“|å®Ÿè·µ|è©¦ã—|ãƒãƒ£ãƒ¬ãƒ³ã‚¸',
    ]
    for p in constructive_patterns:
        if re.search(p, text):
            scores["constructive"] += 1

    # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚·ã‚°ãƒŠãƒ«
    negative_patterns = [
        r'æœ€æ‚ª|ã²ã©ã„|ã¤ã‚‰ã„|è¾›ã„|è‹¦ã—ã„|æ‚²ã—ã„',
        r'å¤±æ•—|å¾Œæ‚”|æ|ç„¡é§„|æ„å‘³ãªã„',
        r'ä¸å®‰|æ€–ã„|å¿ƒé…|æã‚ã—ã„',
    ]
    for p in negative_patterns:
        if re.search(p, text):
            scores["negative"] += 1

    # æ”»æ’ƒçš„ã‚·ã‚°ãƒŠãƒ«ï¼ˆGrokãŒæŠ‘åˆ¶ï¼‰
    aggressive_patterns = [
        r'ãƒã‚«|ã‚¢ãƒ›|ã‚¯ã‚½|æ­»ã­|æ¶ˆãˆã‚|ã†ã–ã„',
        r'ç‚ä¸Š|å©[ã‹ã]|æ‰¹åˆ¤|æ”»æ’ƒ|è¨±ã•ãªã„|ãµã–ã‘ã‚‹ãª',
        r'å˜˜ã¤ã|è©æ¬º|é¨™[ã—ã•]|è£åˆ‡ã‚Š',
    ]
    for p in aggressive_patterns:
        if re.search(p, text):
            scores["aggressive"] += 1

    # ç·åˆåˆ¤å®š
    max_key = max(scores, key=scores.get)
    if scores[max_key] == 0:
        overall = "ä¸­ç«‹"
    elif max_key == "constructive":
        overall = "å»ºè¨­çš„"
    elif max_key == "positive":
        overall = "ãƒã‚¸ãƒ†ã‚£ãƒ–"
    elif max_key == "negative" and scores["constructive"] > 0:
        overall = "ç…½ã‚Šï¼ˆå»ºè¨­çš„ï¼‰"  # å•é¡Œæèµ· + è§£æ±ºç­–
    elif max_key == "negative":
        overall = "ãƒã‚¬ãƒ†ã‚£ãƒ–"
    elif max_key == "aggressive":
        overall = "æ”»æ’ƒçš„"
    else:
        overall = "ä¸­ç«‹"

    return {
        "scores": scores,
        "overall": overall,
        "grok_friendly": overall in ["å»ºè¨­çš„", "ãƒã‚¸ãƒ†ã‚£ãƒ–", "ç…½ã‚Šï¼ˆå»ºè¨­çš„ï¼‰", "ä¸­ç«‹"],
    }


def analyze_tone_distribution(df):
    """å…¨æŠ•ç¨¿ã®ãƒˆãƒ¼ãƒ³åˆ†å¸ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ"""
    tone_data = defaultdict(list)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        tone = analyze_tone(text)
        tone_data[tone["overall"]].append(likes)

    # Grokãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ vs éãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼
    friendly_likes = []
    unfriendly_likes = []
    for tone_type, likes_list in tone_data.items():
        if tone_type in ["å»ºè¨­çš„", "ãƒã‚¸ãƒ†ã‚£ãƒ–", "ç…½ã‚Šï¼ˆå»ºè¨­çš„ï¼‰", "ä¸­ç«‹"]:
            friendly_likes.extend(likes_list)
        else:
            unfriendly_likes.extend(likes_list)

    return {
        "tone_distribution": {k: len(v) for k, v in tone_data.items()},
        "tone_avg_likes": {k: sum(v) / len(v) if v else 0 for k, v in tone_data.items()},
        "friendly_avg": sum(friendly_likes) / len(friendly_likes) if friendly_likes else 0,
        "unfriendly_avg": sum(unfriendly_likes) / len(unfriendly_likes) if unfriendly_likes else 0,
        "friendly_count": len(friendly_likes),
        "unfriendly_count": len(unfriendly_likes),
    }


# ========================================
# 6. æ»åœ¨æ™‚é–“æ¨å®š
# ========================================

def estimate_dwell_time_score(text):
    """æŠ•ç¨¿ã®æ¨å®šæ»åœ¨æ™‚é–“ã‚¹ã‚³ã‚¢ï¼ˆ0-20ç‚¹ï¼‰

    Xã®dwell timeé‡ã¿ = +10.0ï¼ˆ2åˆ†ä»¥ä¸Šã§ç™ºå‹•ï¼‰
    ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‹ã‚‰æ»åœ¨æ™‚é–“ã‚’æ¨å®šã™ã‚‹ã€‚
    """
    score = 0

    # æ–‡å­—æ•°ï¼ˆé•·ã„ã»ã©æ»åœ¨æ™‚é–“ãŒé•·ã„ï¼‰
    length = len(text)
    if length >= 400:
        score += 6   # èª­ã‚€ã®ã«1åˆ†ä»¥ä¸Š
    elif length >= 250:
        score += 5
    elif length >= 150:
        score += 3
    elif length >= 80:
        score += 2
    else:
        score += 1   # çŸ­æ–‡ã¯ã™ãèª­ã‚ã‚‹

    # æ”¹è¡Œãƒ»æ§‹é€ ï¼ˆèª­ã¿ã‚„ã™ã„æ§‹é€  = æœ€å¾Œã¾ã§èª­ã‚€ = æ»åœ¨æ™‚é–“å¢—ï¼‰
    line_count = text.count("\n")
    if 3 <= line_count <= 10:
        score += 3  # é©åº¦ãªæ§‹é€ 
    elif line_count > 10:
        score += 2  # é•·ã™ãã‚‹ã¨é›¢è„±
    else:
        score += 1

    # ç®‡æ¡æ›¸ãï¼ˆã‚¹ã‚­ãƒ£ãƒ³ã—ã‚„ã™ã„ = æœ€å¾Œã¾ã§è¦‹ã‚‹ï¼‰
    if re.search(r'^[ãƒ»\-âœ…â˜‘â‘ â‘¡â‘¢â‘£â‘¤\d+[\.\)ï¼‰]]', text, re.MULTILINE):
        score += 3

    # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ï¼ˆå…ˆãŒæ°—ã«ãªã‚‹ = æœ€å¾Œã¾ã§èª­ã‚€ï¼‰
    if has_story(text):
        score += 2

    # æ•°å­—ãƒ»ãƒ‡ãƒ¼ã‚¿ï¼ˆã˜ã£ãã‚Šèª­ã‚€ï¼‰
    number_count = len(re.findall(r'\d+[ä¸‡å††å€‹ä»¶ã¤%å€]', text))
    if number_count >= 3:
        score += 3
    elif number_count >= 1:
        score += 1

    # æ„Ÿæƒ…çš„ãƒ•ãƒƒã‚¯ï¼ˆç«‹ã¡æ­¢ã¾ã£ã¦èª­ã‚€ï¼‰
    if re.search(r'(è¡æ’ƒ|é©š|ãƒ¤ãƒ|ã‚„ã°|ãƒã‚¸ã§|ã‚¬ãƒã§|ä¿¡ã˜ã‚‰ã‚Œãªã„)', text):
        score += 2

    # ç”»åƒãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢ç¤ºå”†ï¼ˆè¦‹ã‚‹æ™‚é–“ãŒå¢—ãˆã‚‹ï¼‰
    if re.search(r'(ç”»åƒ|å†™çœŸ|ã‚¹ã‚¯ã‚·ãƒ§|å‹•ç”»|ğŸ“¸|ğŸ“¹|ğŸ–¼)', text):
        score += 1

    return min(20, score)


def analyze_dwell_potential(df):
    """å…¨æŠ•ç¨¿ã®æ»åœ¨æ™‚é–“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’åˆ†æ"""
    results = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        dwell_score = estimate_dwell_time_score(text)
        results.append({
            "text": text[:60],
            "likes": likes,
            "dwell_score": dwell_score,
            "length": len(text),
        })

    results.sort(key=lambda x: x["dwell_score"], reverse=True)

    # ã‚¹ã‚³ã‚¢å¸¯åˆ¥ã®å¹³å‡ã„ã„ã­
    buckets = {"é«˜(15-20)": [], "ä¸­(10-14)": [], "ä½(0-9)": []}
    for r in results:
        if r["dwell_score"] >= 15:
            buckets["é«˜(15-20)"].append(r["likes"])
        elif r["dwell_score"] >= 10:
            buckets["ä¸­(10-14)"].append(r["likes"])
        else:
            buckets["ä½(0-9)"].append(r["likes"])

    return {
        "top10": results[:10],
        "avg_dwell_score": sum(r["dwell_score"] for r in results) / len(results) if results else 0,
        "bucket_avg_likes": {
            k: sum(v) / len(v) if v else 0 for k, v in buckets.items()
        },
        "bucket_counts": {k: len(v) for k, v in buckets.items()},
        "correlation_data": [(r["dwell_score"], r["likes"]) for r in results],
    }


# ========================================
# 7. æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆäºˆæ¸¬
# ========================================

def predict_early_engagement(text):
    """æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆæŠ•ç¨¿å¾Œ1æ™‚é–“ä»¥å†…ï¼‰ã®äºˆæ¸¬

    æœ€åˆã®1æ™‚é–“ã§å…¨ä½“ã®50%ãŒæ±ºã¾ã‚‹ã€‚
    ã€Œã™ãã«ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã—ã‚„ã™ã„ã‹ã€ã‚’è©•ä¾¡ã€‚
    """
    score = 0
    signals = []

    first_line = text.split("\n")[0] if text else ""

    # 1. å†’é ­ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ã«ç›®ã‚’æ­¢ã‚ã‚‹ã‹ï¼‰
    if len(first_line) <= 30 and re.search(r'[ï¼!ï¼Ÿ?]', first_line):
        score += 3
        signals.append("çŸ­æ–‡ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆå†’é ­")
    elif re.search(r'(ãƒã‚¸ã§|ã‚¬ãƒã§|è¡æ’ƒ|é€Ÿå ±|ç·Šæ€¥)', first_line):
        score += 3
        signals.append("ç·Šæ€¥æ€§ãƒ¯ãƒ¼ãƒ‰")
    elif re.search(r'\d+[ä¸‡å††%å€]', first_line):
        score += 2
        signals.append("å†’é ­ã«å…·ä½“çš„æ•°å­—")

    # 2. å³åº§ã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯èƒ½æ€§
    if len(text) <= 140:
        score += 2
        signals.append("140å­—ä»¥å†…ï¼ˆå³èª­ã¿ï¼‰")
    elif len(text) <= 280:
        score += 1
        signals.append("280å­—ä»¥å†…ï¼ˆé€Ÿèª­å¯èƒ½ï¼‰")

    # 3. æ„Ÿæƒ…çš„å³åå¿œ
    if re.search(r'(ã‚ã‹ã‚‹|ã‚ã‚‹ã‚ã‚‹|ãã‚Œ|ã“ã‚Œ|ã»ã‚“ã“ã‚Œ)', text):
        score += 2
        signals.append("å…±æ„Ÿå³åå¿œãƒ¯ãƒ¼ãƒ‰")

    # 4. ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»è©±é¡Œæ€§ï¼ˆæ™‚æœŸä¾å­˜ã ãŒæ§‹é€ çš„ã«åˆ¤å®šï¼‰
    if re.search(r'(Claude|GPT|Grok|AI|ChatGPT|Gemini|OpenAI)', text, re.IGNORECASE):
        score += 2
        signals.append("AIè©±é¡Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰")

    # 5. å•ã„ã‹ã‘ï¼ˆå³ãƒªãƒ—ã—ã‚„ã™ã„ï¼‰
    if re.search(r'[\?ï¼Ÿ]$', text.strip()):
        score += 2
        signals.append("ç–‘å•ã§çµ‚ã‚ã‚‹")

    return {
        "score": min(10, score),
        "signals": signals,
        "predicted_velocity": "é«˜é€Ÿ" if score >= 7 else "ä¸­é€Ÿ" if score >= 4 else "ä½é€Ÿ",
    }


def analyze_early_engagement_potential(df):
    """å…¨æŠ•ç¨¿ã®æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’åˆ†æ"""
    results = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        early = predict_early_engagement(text)
        results.append({
            "text": text[:60],
            "likes": likes,
            "early_score": early["score"],
            "velocity": early["predicted_velocity"],
            "signals": early["signals"],
        })

    # é€Ÿåº¦åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    velocity_data = defaultdict(list)
    for r in results:
        velocity_data[r["velocity"]].append(r["likes"])

    return {
        "velocity_avg_likes": {k: sum(v) / len(v) if v else 0 for k, v in velocity_data.items()},
        "velocity_counts": {k: len(v) for k, v in velocity_data.items()},
        "top10_fast": sorted(
            [r for r in results if r["velocity"] == "é«˜é€Ÿ"],
            key=lambda x: x["likes"], reverse=True
        )[:10],
    }


# ========================================
# çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ========================================

def generate_algorithm_report(df_buzz, df_self=None):
    """Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    lines = []
    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

    lines.append("# Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("")
    lines.append(f"**åˆ†ææ—¥æ™‚:** {now}")
    lines.append(f"**åˆ†æå¯¾è±¡:** ãƒã‚ºæŠ•ç¨¿{len(df_buzz)}ä»¶")
    lines.append("")
    lines.append("> Xã®å…¬é–‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆPhoenix/Grokï¼‰ã®é‡ã¿ã«åŸºã¥ã„ãŸåˆ†æ")
    lines.append("> ã‚½ãƒ¼ã‚¹: xai-org/x-algorithm, Grok AI, ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé‡ã¿å…¬é–‹å€¤")
    lines.append("")
    lines.append("---")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³1: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ é‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===
    lines.append("## 1. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ é‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    lines.append("")
    lines.append("> å¾“æ¥ã®ã€Œã„ã„ã­æ•°é †ã€ã§ã¯ãªãã€Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å…¬å¼é‡ã¿ã§åŠ é‡ã—ãŸçœŸã®ã‚¹ã‚³ã‚¢")
    lines.append("> ã„ã„ã­Ã—0.5 + RTÃ—1.0 + ãƒªãƒ—ãƒ©ã‚¤Ã—13.5")
    lines.append("")

    disc_result = analyze_discussion_algorithm_value(df_buzz)

    lines.append("### 1.1 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ é‡ã‚¹ã‚³ã‚¢ TOP10")
    lines.append("")
    lines.append("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | RT | ãƒªãƒ— | åŠ é‡ã‚¹ã‚³ã‚¢ | è­°è«–ç‡ | æœ¬æ–‡ |")
    lines.append("|------|---------|--------|-----|------|----------|--------|------|")
    for i, r in enumerate(disc_result["top10_by_algorithm"], 1):
        text = r["text"].replace("|", "ï½œ").replace("\n", " ")[:30]
        lines.append(f"| {i} | @{r['user']} | {r['likes']:,} | {r['retweets']:,} | {r['replies']:,} | {r['weighted_score']:,.0f} | {r['discussion_rate']:.3f} | {text} |")
    lines.append("")

    lines.append("### 1.2 è­°è«–èª˜ç™ºåº¦ TOP10ï¼ˆãƒªãƒ—ãƒ©ã‚¤/ã„ã„ã­æ¯”ç‡ï¼‰")
    lines.append("")
    lines.append("> ãƒªãƒ—ãƒ©ã‚¤ã¯ã„ã„ã­ã®27å€ã®ä¾¡å€¤ã€‚è­°è«–èª˜ç™ºåº¦ãŒé«˜ã„æŠ•ç¨¿ = ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ€ã‚‚è©•ä¾¡")
    lines.append("")
    lines.append("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | ãƒªãƒ— | è­°è«–ç‡ | åŠ é‡ã‚¹ã‚³ã‚¢ | æœ¬æ–‡ |")
    lines.append("|------|---------|--------|------|--------|----------|------|")
    for i, r in enumerate(disc_result["top10_by_discussion"], 1):
        text = r["text"].replace("|", "ï½œ").replace("\n", " ")[:30]
        lines.append(f"| {i} | @{r['user']} | {r['likes']:,} | {r['replies']:,} | {r['discussion_rate']:.3f} | {r['weighted_score']:,.0f} | {text} |")
    lines.append("")

    lines.append("### 1.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŠ é‡ã‚¹ã‚³ã‚¢")
    lines.append("")
    lines.append("| ã‚«ãƒ†ã‚´ãƒª | å¹³å‡åŠ é‡ã‚¹ã‚³ã‚¢ |")
    lines.append("|---------|-------------|")
    for cat, score in sorted(disc_result["cat_algorithm_scores"].items(),
                              key=lambda x: x[1], reverse=True):
        lines.append(f"| {cat} | {score:,.0f} |")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: ã‚¹ãƒ¬ãƒƒãƒ‰æ§‹é€ åˆ†æ ===
    lines.append("---")
    lines.append("")
    lines.append("## 2. ã‚¹ãƒ¬ãƒƒãƒ‰æ§‹é€ åˆ†æ")
    lines.append("")
    lines.append("> ã‚¹ãƒ¬ãƒƒãƒ‰ã¯å˜ç™ºæŠ•ç¨¿ã®3å€ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆä¼šè©±ã‚¯ãƒªãƒƒã‚¯é‡ã¿=11.0ï¼‰")
    lines.append("")

    thread_result = analyze_thread_potential(df_buzz)

    lines.append(f"- **ã‚¹ãƒ¬ãƒƒãƒ‰å‹æŠ•ç¨¿:** {thread_result['thread_count']}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {thread_result['thread_avg_likes']:.0f}ï¼‰")
    lines.append(f"- **å˜ç™ºæŠ•ç¨¿:** {thread_result['non_thread_count']}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {thread_result['non_thread_avg_likes']:.0f}ï¼‰")
    if thread_result['non_thread_avg_likes'] > 0:
        ratio = thread_result['thread_avg_likes'] / thread_result['non_thread_avg_likes']
        lines.append(f"- **ã‚¹ãƒ¬ãƒƒãƒ‰å€ç‡:** {ratio:.1f}å€")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³3: å¤–éƒ¨ãƒªãƒ³ã‚¯ã®å½±éŸ¿ ===
    lines.append("---")
    lines.append("")
    lines.append("## 3. å¤–éƒ¨ãƒªãƒ³ã‚¯ã®å½±éŸ¿åˆ†æ")
    lines.append("")
    lines.append("> å¤–éƒ¨ãƒªãƒ³ã‚¯ â†’ 30-50%ãƒªãƒ¼ãƒæ¸›ï¼ˆéPremiumã¯ã»ã¼ã‚¼ãƒ­ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã‚‚ï¼‰")
    lines.append("")

    link_result = analyze_link_impact(df_buzz)

    lines.append("| ãƒªãƒ³ã‚¯ç¨®åˆ¥ | ä»¶æ•° | å¹³å‡ã„ã„ã­ |")
    lines.append("|-----------|------|----------|")
    lines.append(f"| å¤–éƒ¨ãƒªãƒ³ã‚¯ã‚ã‚Š | {link_result['external_count']} | {link_result['external_avg_likes']:.0f} |")
    lines.append(f"| Xå†…ãƒªãƒ³ã‚¯ã®ã¿ | {link_result['x_link_count']} | {link_result['x_link_avg_likes']:.0f} |")
    lines.append(f"| ãƒªãƒ³ã‚¯ãªã— | {link_result['no_link_count']} | {link_result['no_link_avg_likes']:.0f} |")
    lines.append("")

    if link_result['reach_penalty_confirmed']:
        lines.append("**ãƒ‡ãƒ¼ã‚¿ã§ç¢ºèª:** ãƒªãƒ³ã‚¯ãªã—æŠ•ç¨¿ã®æ–¹ãŒå¹³å‡ã„ã„ã­ãŒé«˜ã„ â†’ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒªãƒ³ã‚¯ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒå®Ÿãƒ‡ãƒ¼ã‚¿ã§ã‚‚ç¢ºèª")
    else:
        lines.append("**æ³¨æ„:** ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯å¤–éƒ¨ãƒªãƒ³ã‚¯ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒæ˜ç¢ºã«ç¾ã‚Œã¦ã„ãªã„ï¼ˆæŠ•ç¨¿è€…ã®å½±éŸ¿åŠ›ç­‰ã®è¦å› ï¼‰")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³4: ãƒˆãƒ¼ãƒ³åˆ†æ ===
    lines.append("---")
    lines.append("")
    lines.append("## 4. ãƒˆãƒ¼ãƒ³åˆ†æï¼ˆGrokè©•ä¾¡æ¨å®šï¼‰")
    lines.append("")
    lines.append("> Grokã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»å»ºè¨­çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ‹¡æ•£ã€æ”»æ’ƒçš„ãªrage baitã‚’æŠ‘åˆ¶")
    lines.append("")

    tone_result = analyze_tone_distribution(df_buzz)

    lines.append("### 4.1 ãƒˆãƒ¼ãƒ³åˆ†å¸ƒ")
    lines.append("")
    lines.append("| ãƒˆãƒ¼ãƒ³ | ä»¶æ•° | å¹³å‡ã„ã„ã­ |")
    lines.append("|--------|------|----------|")
    for tone, count in sorted(tone_result["tone_distribution"].items(),
                               key=lambda x: tone_result["tone_avg_likes"].get(x[0], 0),
                               reverse=True):
        avg = tone_result["tone_avg_likes"].get(tone, 0)
        lines.append(f"| {tone} | {count} | {avg:.0f} |")
    lines.append("")

    lines.append("### 4.2 Grokãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ vs éãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼")
    lines.append("")
    lines.append(f"- **Grokãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼:** {tone_result['friendly_count']}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {tone_result['friendly_avg']:.0f}ï¼‰")
    lines.append(f"- **éãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼:** {tone_result['unfriendly_count']}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {tone_result['unfriendly_avg']:.0f}ï¼‰")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³5: æ»åœ¨æ™‚é–“æ¨å®š ===
    lines.append("---")
    lines.append("")
    lines.append("## 5. æ»åœ¨æ™‚é–“æ¨å®š")
    lines.append("")
    lines.append("> æ»åœ¨æ™‚é–“2åˆ†ä»¥ä¸Š â†’ +10.0ã®é‡ã¿ã€‚XãŒæœ€é‡è¦–ã™ã‚‹æŒ‡æ¨™ã®ä¸€ã¤")
    lines.append("")

    dwell_result = analyze_dwell_potential(df_buzz)

    lines.append(f"**å¹³å‡æ»åœ¨æ™‚é–“ã‚¹ã‚³ã‚¢:** {dwell_result['avg_dwell_score']:.1f}/20ç‚¹")
    lines.append("")

    lines.append("| æ»åœ¨æ™‚é–“å¸¯ | ä»¶æ•° | å¹³å‡ã„ã„ã­ |")
    lines.append("|-----------|------|----------|")
    for bucket, avg_likes in sorted(dwell_result["bucket_avg_likes"].items(),
                                      key=lambda x: x[1], reverse=True):
        count = dwell_result["bucket_counts"][bucket]
        lines.append(f"| {bucket} | {count} | {avg_likes:.0f} |")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³6: æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆäºˆæ¸¬ ===
    lines.append("---")
    lines.append("")
    lines.append("## 6. æ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆäºˆæ¸¬")
    lines.append("")
    lines.append("> æŠ•ç¨¿å¾Œ1æ™‚é–“ä»¥å†…ã§å…¨ä½“ã®50%ãŒæ±ºã¾ã‚‹ã€‚ã€Œã™ããƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã—ã‚„ã™ã„ã‹ã€ã‚’è©•ä¾¡")
    lines.append("")

    early_result = analyze_early_engagement_potential(df_buzz)

    lines.append("| äºˆæ¸¬é€Ÿåº¦ | ä»¶æ•° | å¹³å‡ã„ã„ã­ |")
    lines.append("|---------|------|----------|")
    for velocity in ["é«˜é€Ÿ", "ä¸­é€Ÿ", "ä½é€Ÿ"]:
        count = early_result["velocity_counts"].get(velocity, 0)
        avg = early_result["velocity_avg_likes"].get(velocity, 0)
        lines.append(f"| {velocity} | {count} | {avg:.0f} |")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³7: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ã‚³ã‚¢ã§å…¨æŠ•ç¨¿ã‚’å†è©•ä¾¡ ===
    lines.append("---")
    lines.append("")
    lines.append("## 7. Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚¹ã‚³ã‚¢ TOP10 / WORST10")
    lines.append("")
    lines.append("> ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‹ã‚‰Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ï¼ˆ0-100ç‚¹ï¼‰")
    lines.append("")

    all_algo_scores = []
    for _, row in df_buzz.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        user = safe_get(row, "ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "")

        algo = calculate_algorithm_score(text)
        all_algo_scores.append({
            "text": text[:40],
            "likes": likes,
            "user": user,
            "algo_score": algo["total_score"],
            "factors": algo["factors"],
        })

    all_algo_scores.sort(key=lambda x: x["algo_score"], reverse=True)

    lines.append("### TOP10")
    lines.append("")
    lines.append("| é †ä½ | Algoã‚¹ã‚³ã‚¢ | ã„ã„ã­ | ä¸»è¦å›  | æœ¬æ–‡ |")
    lines.append("|------|----------|--------|--------|------|")
    for i, s in enumerate(all_algo_scores[:10], 1):
        top_factors = sorted(s["factors"].items(), key=lambda x: x[1], reverse=True)[:3]
        factors_str = ", ".join(f"{k}:{v}" for k, v in top_factors if v > 0)
        text = s["text"].replace("|", "ï½œ").replace("\n", " ")
        lines.append(f"| {i} | {s['algo_score']} | {s['likes']:,} | {factors_str} | {text} |")
    lines.append("")

    lines.append("### WORST10")
    lines.append("")
    lines.append("| é †ä½ | Algoã‚¹ã‚³ã‚¢ | ã„ã„ã­ | ä¸»è¦å›  | æœ¬æ–‡ |")
    lines.append("|------|----------|--------|--------|------|")
    for i, s in enumerate(all_algo_scores[-10:], 1):
        top_factors = sorted(s["factors"].items(), key=lambda x: x[1], reverse=True)[:3]
        factors_str = ", ".join(f"{k}:{v}" for k, v in top_factors if v > 0)
        text = s["text"].replace("|", "ï½œ").replace("\n", " ")
        lines.append(f"| {i} | {s['algo_score']} | {s['likes']:,} | {factors_str} | {text} |")
    lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³8: @Mr_botenã®åˆ†æ ===
    if df_self is not None and len(df_self) > 0:
        lines.append("---")
        lines.append("")
        lines.append("## 8. @Mr_botenã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æ")
        lines.append("")

        self_algo_scores = []
        for _, row in df_self.iterrows():
            text = safe_get(row, "æœ¬æ–‡", "")
            likes = safe_get(row, "ã„ã„ã­æ•°", 0)

            algo = calculate_algorithm_score(text)
            self_algo_scores.append({
                "text": text[:40],
                "likes": likes,
                "algo_score": algo["total_score"],
                "factors": algo["factors"],
            })

        self_algo_scores.sort(key=lambda x: x["algo_score"], reverse=True)

        avg_self = sum(s["algo_score"] for s in self_algo_scores) / len(self_algo_scores)
        avg_buzz = sum(s["algo_score"] for s in all_algo_scores) / len(all_algo_scores)

        lines.append(f"- **è‡ªåˆ†ã®å¹³å‡Algoã‚¹ã‚³ã‚¢:** {avg_self:.1f}ç‚¹ï¼ˆãƒã‚ºæŠ•ç¨¿å¹³å‡: {avg_buzz:.1f}ç‚¹ï¼‰")
        lines.append("")

        # è¦ç´ åˆ¥æ¯”è¼ƒ
        lines.append("### è¦ç´ åˆ¥æ¯”è¼ƒ")
        lines.append("")
        lines.append("| è¦ç´  | è‡ªåˆ†ã®å¹³å‡ | ãƒã‚ºå¹³å‡ | å·® | æ”¹å–„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ |")
        lines.append("|------|----------|--------|-----|-------------|")

        advice_map = {
            "ãƒªãƒ—ãƒ©ã‚¤èª˜ç™ºåŠ›": "ç–‘å•å½¢ã§çµ‚ã‚ã‚‹ã€æ„è¦‹ã‚’æ±‚ã‚ã‚‹è¡¨ç¾ã‚’å…¥ã‚Œã‚‹",
            "æ»åœ¨æ™‚é–“": "å…·ä½“çš„æ•°å­—ãƒ»ç®‡æ¡æ›¸ããƒ»ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹é€ ã§èª­ã¾ã›ã‚‹",
            "ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»ä¼šè©±": "ã‚¹ãƒ¬ãƒƒãƒ‰å½¢å¼ã‚’è©¦ã™ï¼ˆ3å€ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰",
            "ãƒˆãƒ¼ãƒ³": "å»ºè¨­çš„ãªãƒˆãƒ¼ãƒ³ï¼ˆå­¦ã³ãƒ»å…±æœ‰ãƒ»ä½“é¨“ï¼‰ã‚’æ„è­˜",
            "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯èª˜ç™º": "ã€Œä¿å­˜æ¨å¥¨ã€ã€Œã¾ã¨ã‚ã€ã€Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã€å½¢å¼",
            "å¤–éƒ¨ãƒªãƒ³ã‚¯": "ãƒªãƒ³ã‚¯ã¯ãƒªãƒ—ãƒ©ã‚¤ã«ã€‚æœ¬æ–‡ã«ã¯å…¥ã‚Œãªã„",
            "ãƒ—ãƒ­ãƒ•ã‚¯ãƒªãƒƒã‚¯èª˜ç™º": "ç§˜åŒ¿æ„Ÿãƒ»è‡ªå·±é–‹ç¤ºã§ã€Œèª°ï¼Ÿã€ã¨æ€ã‚ã›ã‚‹",
            "æ—©æœŸåå¿œæ€§": "å†’é ­30å­—ä»¥å†…ã«ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã€æ„Ÿæƒ…ãƒ¯ãƒ¼ãƒ‰å…¥ã‚Œã‚‹",
        }

        self_factor_avg = defaultdict(list)
        buzz_factor_avg = defaultdict(list)
        for s in self_algo_scores:
            for k, v in s["factors"].items():
                self_factor_avg[k].append(v)
        for s in all_algo_scores:
            for k, v in s["factors"].items():
                buzz_factor_avg[k].append(v)

        for factor in advice_map:
            s_vals = self_factor_avg.get(factor, [0])
            b_vals = buzz_factor_avg.get(factor, [0])
            s_avg = sum(s_vals) / len(s_vals) if s_vals else 0
            b_avg = sum(b_vals) / len(b_vals) if b_vals else 0
            diff = s_avg - b_avg
            adv = advice_map[factor]
            lines.append(f"| {factor} | {s_avg:.1f} | {b_avg:.1f} | {diff:+.1f} | {adv} |")
        lines.append("")

        # è‡ªåˆ†ã®TOP5
        lines.append("### è‡ªåˆ†ã®Algoã‚¹ã‚³ã‚¢TOP5")
        lines.append("")
        lines.append("| é †ä½ | Algoã‚¹ã‚³ã‚¢ | ã„ã„ã­ | ä¸»è¦å›  | æœ¬æ–‡ |")
        lines.append("|------|----------|--------|--------|------|")
        for i, s in enumerate(self_algo_scores[:5], 1):
            top_factors = sorted(s["factors"].items(), key=lambda x: x[1], reverse=True)[:3]
            factors_str = ", ".join(f"{k}:{v}" for k, v in top_factors if v > 0)
            text = s["text"].replace("|", "ï½œ").replace("\n", " ")
            lines.append(f"| {i} | {s['algo_score']} | {s['likes']:,} | {factors_str} | {text} |")
        lines.append("")

    # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³9: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ ===
    lines.append("---")
    lines.append("")
    lines.append("## 9. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ")
    lines.append("")
    lines.append("### æœ€é«˜å„ªå…ˆåº¦ï¼ˆå³å®Ÿè·µå¯èƒ½ï¼‰")
    lines.append("1. **ãƒªãƒ—ãƒ©ã‚¤ã«å¿…ãšè¿”ä¿¡ã™ã‚‹**ï¼ˆ150å€ãƒ–ãƒ¼ã‚¹ãƒˆã€‚ã“ã‚Œã ã‘ã§åŠ‡çš„ã«å¤‰ã‚ã‚‹ï¼‰")
    lines.append("2. **å¤–éƒ¨ãƒªãƒ³ã‚¯ã¯æœ¬æ–‡ã«å…¥ã‚Œãªã„**ï¼ˆãƒªãƒ—ãƒ©ã‚¤ã«æ›¸ãã€‚æœ¬æ–‡ã«å…¥ã‚Œã‚‹ã¨50%æ¸›ï¼‰")
    lines.append("3. **ç–‘å•å½¢ã§çµ‚ã‚ã‚‹**ï¼ˆãƒªãƒ—ãƒ©ã‚¤èª˜ç™º = 27å€ã®ä¾¡å€¤ï¼‰")
    lines.append("")
    lines.append("### é«˜å„ªå…ˆåº¦ï¼ˆæŠ•ç¨¿è¨­è¨ˆã«çµ„ã¿è¾¼ã‚€ï¼‰")
    lines.append("4. **ã‚¹ãƒ¬ãƒƒãƒ‰å½¢å¼ã‚’è©¦ã™**ï¼ˆ3å€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼‰")
    lines.append("5. **ä¿å­˜ã—ãŸããªã‚‹æ§‹é€ **ï¼ˆç®‡æ¡æ›¸ããƒ»æ•°å­—ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ â†’ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯20å€ï¼‰")
    lines.append("6. **å»ºè¨­çš„ãªãƒˆãƒ¼ãƒ³**ï¼ˆå­¦ã³ãƒ»ä½“é¨“ãƒ»ææ¡ˆ â†’ GrokãŒæ‹¡æ•£ä¿ƒé€²ï¼‰")
    lines.append("")
    lines.append("### ä¸­å„ªå…ˆåº¦ï¼ˆç¶™ç¶šçš„ã«æ„è­˜ï¼‰")
    lines.append("7. **18-21æ™‚ã«æŠ•ç¨¿**ï¼ˆæ—©æœŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæœ€å¤§åŒ–ï¼‰")
    lines.append("8. **é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆ > å‹•ç”»**ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯å‹•ç”»ã‚ˆã‚Š30%å¼·ã„ï¼‰")
    lines.append("9. **X Premiumã®æ´»ç”¨**ï¼ˆ4å€ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰")
    lines.append("")

    return "\n".join(lines)


# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    """Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æã‚’å®Ÿè¡Œ"""
    import os

    BUZZ_FILE = "output/buzz_posts_20260215.xlsx"
    SELF_FILE = "output/TwExport_20260217_191942.csv"
    DB_FILE = "data/buzz_database.db"
    OUTPUT_FILE = "output/x_algorithm_analysis_20260221.md"

    print("=" * 60)
    print("Xã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æ")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if os.path.exists(DB_FILE):
        from buzz_score_v2 import load_from_db, load_self_posts
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿: {DB_FILE}")
        df_raw = load_from_db(DB_FILE)
        self_accounts = {"Mr_boten", "mr_boten"}
        df_self = df_raw[df_raw["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"].str.lower().isin({a.lower() for a in self_accounts})]
        df_buzz = df_raw[~df_raw["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"].str.lower().isin({a.lower() for a in self_accounts})]
        df_buzz = df_buzz[df_buzz["ã„ã„ã­æ•°"] > 0].copy()
    else:
        from analyze_posts import filter_data, load_excel
        print(f"Excelã‹ã‚‰èª­ã¿è¾¼ã¿: {BUZZ_FILE}")
        df_raw = load_excel(BUZZ_FILE)
        if df_raw is None:
            return
        df_buzz, _, _ = filter_data(df_raw)

        if os.path.exists(SELF_FILE):
            from buzz_score_v2 import load_self_posts
            df_self = load_self_posts(SELF_FILE)
        else:
            df_self = pd.DataFrame()

    print(f"ãƒã‚ºæŠ•ç¨¿: {len(df_buzz)}ä»¶ / è‡ªåˆ†ã®æŠ•ç¨¿: {len(df_self)}ä»¶")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    report = generate_algorithm_report(df_buzz, df_self)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"\nãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {OUTPUT_FILE}")
    print(f"æ–‡å­—æ•°: {len(report):,}æ–‡å­—")


if __name__ == "__main__":
    main()
