"""æŠ•ç¨¿ã®æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«è©³ç´°åˆ†è§£ãƒ»åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

å„æŠ•ç¨¿ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’è‡ªå‹•åˆ†æï¼š
- å†’é ­ï¼ˆæœ€åˆã®1æ–‡ï¼‰ã®å½¹å‰²
- å±•é–‹éƒ¨ã®æ§‹é€ 
- æ„Ÿæƒ…ã®å‹•ã
- ç· ã‚æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
- æ–‡ç« ã®ãƒªã‚ºãƒ 
- ãªãœä¼¸ã³ãŸã‹ï¼ˆæ¨å®šï¼‰
"""

import os
import re
from typing import Dict, List, Tuple
import pandas as pd


class WritingAnalyzer:
    """æŠ•ç¨¿æ–‡ç« ã®è©³ç´°åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡
        self.opening_patterns = {
            "ç–‘å•å½¢": r"^.{0,50}[\?ï¼Ÿ]",
            "æ„Ÿå˜†å½¢": r"^[ï¼!]{1,}|^.{0,30}[ï¼!]{2,}",
            "å‘¼ã³ã‹ã‘å½¢": r"^(ã‚ãªãŸ|å›|çš†|ã¿ã‚“ãª|ãŠå‰|è²´æ–¹)",
            "ä»®å®šæ³•": r"^(ã‚‚ã—|ã‚‚ã—ã‚‚|ä»®ã«|ã€œã ã£ãŸã‚‰|ã€œãªã‚‰)",
            "å¦å®šå½¢": r"^(ã€œãªã„|ã¾ã |ã‚„ã‚|ã™ã‚‹ãª|ãƒ€ãƒ¡)",
            "æ•°å­—å¼·èª¿": r"^\d+[é¸ã¤å€‹ä»¶ä¸‡å††%äºº]",
            "ç§˜åŒ¿æƒ…å ±": r"^(çŸ¥ã‚‰ãªã„|çŸ¥ã‚‰ãªã‹ã£ãŸ|å®Ÿã¯|æœ¬å½“ã¯|æ­£ç›´|ã¶ã£ã¡ã‚ƒã‘|ã‚¬ãƒã§|ãƒã‚¸ã§)",
            "è‡ªå·±é–‹ç¤º": r"^(åƒ•|ç§|ä¿º|è‡ªåˆ†|ãƒ¯ã‚¤|ã†ã¡)ã¯?"
        }

        # æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.emotion_patterns = {
            "é©šã": ["ãƒ¤ãƒã„", "ã‚„ã°ã„", "ã™ã”ã„", "å‡„ã„", "ã‚¨ã‚°ã„", "ãˆãã„", "ã³ã£ãã‚Š", "ãƒ“ãƒƒã‚¯ãƒª", "ãƒã‚¸ã§", "ã‚¬ãƒã§"],
            "å…±æ„Ÿ": ["ã‚ã‹ã‚‹", "ãã†", "ã‚ã‚‹ã‚ã‚‹", "åŒã˜", "åƒ•ã‚‚", "ç§ã‚‚", "ä¿ºã‚‚"],
            "ä¸å®‰": ["æ€–ã„", "ä¸å®‰", "å¿ƒé…", "å¤§ä¸ˆå¤«", "ã©ã†ã—ã‚ˆã†"],
            "æœŸå¾…": ["ãƒ¯ã‚¯ãƒ¯ã‚¯", "æ¥½ã—ã¿", "æœŸå¾…", "å¾…ã¡é ã—ã„"],
            "å–œã³": ["å¬‰ã—ã„", "ã‚„ã£ãŸ", "æœ€é«˜", "è‰¯ã‹ã£ãŸ", "ã‚ã‚ŠãŒã¨ã†"],
            "æ€’ã‚Š": ["ãƒ ã‚«ã¤ã", "è…¹ç«‹ã¤", "ã‚¤ãƒ©ã‚¤ãƒ©", "è¨±ã›ãªã„", "æœ€æ‚ª"],
            "å›°æƒ‘": ["æ„å‘³ã‚ã‹ã‚‰ã‚“", "ã‚ã‹ã‚‰ã‚“", "ãªã‚“ã§", "ã©ã†ã—ã¦", "ãˆï¼Ÿ"],
            "æ±ºæ„": ["ã‚„ã‚‹", "ã‚„ã£ã¦ã‚„ã‚‹", "çµ¶å¯¾", "å¿…ãš", "é ‘å¼µã‚‹"]
        }

        # ç· ã‚ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.closing_patterns = {
            "è¡Œå‹•å–šèµ·": r"(ãƒ•ã‚©ãƒ­ãƒ¼|ã„ã„ã­|ãƒªãƒ—|RT|ä¿å­˜|æ‹¡æ•£|ã‚·ã‚§ã‚¢|ãƒã‚§ãƒƒã‚¯|è¦‹ã¦|è©¦ã—ã¦|ã‚„ã£ã¦).*[ï¼!ã€‚\.]*$",
            "ç–‘å•æŠ•ã’ã‹ã‘": r"[\?ï¼Ÿ]$",
            "ä½™éŸ»ãƒ»çœç•¥": r"[\.ï¼â€¦]{2,}$",
            "çµµæ–‡å­—ç· ã‚": r"[ğŸ”¥ğŸ’ªğŸ‘âœ¨ğŸ‰ğŸ™ğŸ‘‡â¬‡â†“]$",
            "æ±ºæ„è¡¨æ˜": r"(ã‚„ã‚‹|è¡Œã|é€²ã‚€|æŒ‘ã‚€|ç¶šã‘ã‚‹|ç›®æŒ‡ã™|é ‘å¼µã‚‹)[ï¼!ã€‚\.]*$",
            "æœŸå¾…ç…½ã‚Š": r"(æ¥½ã—ã¿|æœŸå¾…|ã“ã‚Œã‹ã‚‰|ä»Šå¾Œ|æ¬¡|ç¶šã)[ï¼!ã€‚\.]*$"
        }

    def analyze_opening(self, text: str) -> Dict[str, str]:
        """å†’é ­ï¼ˆæœ€åˆã®1æ–‡ï¼‰ã‚’åˆ†æ"""
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        if not lines:
            return {"first_sentence": "", "pattern": "ä¸æ˜", "role": "ä¸æ˜"}

        first_line = lines[0]

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š
        pattern = "ãã®ä»–"
        for name, regex in self.opening_patterns.items():
            if re.search(regex, first_line):
                pattern = name
                break

        # å½¹å‰²ã‚’æ¨å®š
        role_map = {
            "ç–‘å•å½¢": "ç–‘å•ã‚’æŠ•ã’ã‹ã‘ã¦èª­è€…ã®æ€è€ƒã‚’å¼•ãå‡ºã™",
            "æ„Ÿå˜†å½¢": "å¼·ã„æ„Ÿæƒ…ã§æ³¨æ„ã‚’å¼•ã",
            "å‘¼ã³ã‹ã‘å½¢": "èª­è€…ã«ç›´æ¥èªã‚Šã‹ã‘ã¦å¼•ãè¾¼ã‚€",
            "ä»®å®šæ³•": "æƒ³åƒã•ã›ã¦è‡ªåˆ†äº‹ã«ã•ã›ã‚‹",
            "å¦å®šå½¢": "å¸¸è­˜ã‚’å¦å®šã—ã¦èˆˆå‘³ã‚’å¼•ã",
            "æ•°å­—å¼·èª¿": "å…·ä½“æ€§ã§ä¿¡é ¼ã‚’å¾—ã‚‹",
            "ç§˜åŒ¿æƒ…å ±": "ç§˜å¯†ã®å…±æœ‰ã§å¥½å¥‡å¿ƒã‚’åˆºæ¿€",
            "è‡ªå·±é–‹ç¤º": "è¦ªè¿‘æ„Ÿã‚’ä½œã‚Šå…±æ„Ÿã‚’å¾—ã‚‹",
            "ãã®ä»–": "è‡ªç„¶ãªèªã‚Šå£ã§å…¥ã‚Šã‚„ã™ãã™ã‚‹"
        }

        return {
            "first_sentence": first_line,
            "pattern": pattern,
            "role": role_map.get(pattern, "èª­è€…ã®æ³¨æ„ã‚’å¼•ã")
        }

    def analyze_structure(self, text: str) -> Dict[str, any]:
        """æ–‡ç« ã®å±•é–‹æ§‹é€ ã‚’åˆ†æ"""
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        sentences = re.split(r'[ã€‚\.ï¼!ï¼Ÿ?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        # åŸºæœ¬çµ±è¨ˆ
        stats = {
            "total_chars": len(text),
            "line_count": len(lines),
            "sentence_count": len(sentences),
            "avg_sentence_length": len(text) / max(len(sentences), 1)
        }

        # æ§‹æˆè¦ç´ ã®æ¤œå‡º
        has_list = bool(re.search(r'^[ãƒ»\-â–¶â–¸âœ…â˜‘âœ“â—†â– â—â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\d+[\.\)ï¼‰]]', text, re.MULTILINE))
        has_url = bool(re.search(r'https?://', text))
        has_quote = bool(re.search(r'ã€Œ.*?ã€', text))
        has_numbers = len(re.findall(r'\d+', text))

        # å±•é–‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ¤å®š
        if has_list:
            structure_type = "ãƒªã‚¹ãƒˆå‹ï¼ˆç®‡æ¡æ›¸ãã§æ•´ç†ï¼‰"
        elif len(sentences) <= 3:
            structure_type = "çŸ­æ–‡å®Œçµå‹ï¼ˆã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–ï¼‰"
        elif has_quote:
            structure_type = "å¼•ç”¨å‹ï¼ˆä¼šè©±ã‚„å¼•ç”¨ã§è‡¨å ´æ„Ÿï¼‰"
        else:
            structure_type = "èª¬æ˜å‹ï¼ˆè«–ç†çš„ã«å±•é–‹ï¼‰"

        return {
            "stats": stats,
            "has_list": has_list,
            "has_url": has_url,
            "has_quote": has_quote,
            "number_count": has_numbers,
            "structure_type": structure_type
        }

    def analyze_emotions(self, text: str) -> List[Tuple[str, List[str]]]:
        """æ„Ÿæƒ…ã®å‹•ãã‚’åˆ†æ"""
        # æ–‡ã‚’3ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ï¼ˆå°å…¥ãƒ»å±•é–‹ãƒ»ç· ã‚ï¼‰
        sentences = re.split(r'[ã€‚\.ï¼!ï¼Ÿ?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) == 0:
            return []

        # å„ãƒ–ãƒ­ãƒƒã‚¯ã®æ„Ÿæƒ…ã‚’æ¤œå‡º
        blocks = []
        if len(sentences) <= 3:
            blocks = [("å…¨ä½“", text)]
        else:
            third = len(sentences) // 3
            blocks = [
                ("å°å…¥", 'ã€‚'.join(sentences[:third])),
                ("å±•é–‹", 'ã€‚'.join(sentences[third:third*2])),
                ("ç· ã‚", 'ã€‚'.join(sentences[third*2:]))
            ]

        emotion_flow = []
        for block_name, block_text in blocks:
            detected_emotions = []
            for emotion, keywords in self.emotion_patterns.items():
                if any(kw in block_text for kw in keywords):
                    detected_emotions.append(emotion)

            if detected_emotions:
                emotion_flow.append((block_name, detected_emotions))

        return emotion_flow

    def analyze_closing(self, text: str) -> Dict[str, str]:
        """ç· ã‚æ–¹ã‚’åˆ†æ"""
        # æœ€å¾Œã®1-2æ–‡ã‚’å–å¾—
        sentences = re.split(r'[ã€‚\.ï¼!ï¼Ÿ?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            return {"last_sentence": "", "pattern": "ä¸æ˜", "effect": "ä¸æ˜"}

        last_sentence = sentences[-1]

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š
        pattern = "ãã®ä»–"
        for name, regex in self.closing_patterns.items():
            if re.search(regex, text[-50:]):  # æœ€å¾Œ50æ–‡å­—ã§åˆ¤å®š
                pattern = name
                break

        # åŠ¹æœã‚’æ¨å®š
        effect_map = {
            "è¡Œå‹•å–šèµ·": "èª­è€…ã«å…·ä½“çš„ãªè¡Œå‹•ã‚’ä¿ƒã™",
            "ç–‘å•æŠ•ã’ã‹ã‘": "è€ƒãˆã•ã›ã¦è­°è«–ã‚’èª˜ç™ºã™ã‚‹",
            "ä½™éŸ»ãƒ»çœç•¥": "æƒ³åƒã®ä½™åœ°ã‚’æ®‹ã—ã¦ä½™éŸ»ã‚’ä½œã‚‹",
            "çµµæ–‡å­—ç· ã‚": "æ„Ÿæƒ…ã‚’è¦–è¦šåŒ–ã—ã¦å°è±¡ã¥ã‘ã‚‹",
            "æ±ºæ„è¡¨æ˜": "æ±ºæ„ã‚’ç¤ºã—ã¦èª­è€…ã‚’é¼“èˆã™ã‚‹",
            "æœŸå¾…ç…½ã‚Š": "æ¬¡ã¸ã®æœŸå¾…ã‚’é«˜ã‚ã¦ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’ç¶­æŒ",
            "ãã®ä»–": "è‡ªç„¶ã«ç· ã‚ã¦ä½™éŸ»ã‚’æ®‹ã™"
        }

        return {
            "last_sentence": last_sentence,
            "pattern": pattern,
            "effect": effect_map.get(pattern, "èª­è€…ã®å°è±¡ã«æ®‹ã™")
        }

    def analyze_rhythm(self, text: str) -> Dict[str, any]:
        """æ–‡ç« ã®ãƒªã‚ºãƒ ã‚’åˆ†æ"""
        sentences = re.split(r'[ã€‚\.ï¼!ï¼Ÿ?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            return {"rhythm": "ä¸æ˜", "variation": 0, "punctuation_style": "ä¸æ˜"}

        # æ–‡ã®é•·ã•ã®ãƒãƒ©ã¤ã
        lengths = [len(s) for s in sentences]
        avg_length = sum(lengths) / len(lengths)
        variation = sum(abs(l - avg_length) for l in lengths) / len(lengths)

        # ãƒªã‚ºãƒ ã®è©•ä¾¡
        if variation < 10:
            rhythm = "å˜èª¿ï¼ˆæ–‡ã®é•·ã•ãŒå‡ä¸€ï¼‰"
        elif variation < 30:
            rhythm = "ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆé©åº¦ãªé•·çŸ­ã®å¤‰åŒ–ï¼‰"
        else:
            rhythm = "ãƒ¡ãƒªãƒãƒªå‹ï¼ˆæ–‡ã®é•·çŸ­ãŒå¤§ããå¤‰åŒ–ï¼‰"

        # å¥èª­ç‚¹ã‚¹ã‚¿ã‚¤ãƒ«
        exclamation_count = text.count('ï¼') + text.count('!')
        question_count = text.count('ï¼Ÿ') + text.count('?')
        ellipsis_count = text.count('â€¦') + text.count('..')

        if exclamation_count >= 3:
            punctuation = "æ„Ÿå˜†å‹ï¼ˆï¼ã®å¤šç”¨ã§èˆˆå¥®ã‚’è¡¨ç¾ï¼‰"
        elif question_count >= 2:
            punctuation = "ç–‘å•å‹ï¼ˆï¼Ÿã§å¯¾è©±ã‚’èª˜ç™ºï¼‰"
        elif ellipsis_count >= 2:
            punctuation = "ä½™éŸ»å‹ï¼ˆâ€¦ã§é–“ã‚’ä½œã‚‹ï¼‰"
        else:
            punctuation = "æ¨™æº–å‹ï¼ˆãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå¥èª­ç‚¹ï¼‰"

        # å£èªè¡¨ç¾ã®æ¤œå‡º
        casual_markers = ['ã€œã ã‚ˆã­', 'ã€œã ã‚', 'ã€œã‹ãª', 'ã€œã‚“ã ã‘ã©', 'ã€œã£ã¦', 'w', 'www']
        casual_count = sum(text.count(m) for m in casual_markers)

        return {
            "rhythm": rhythm,
            "variation": round(variation, 1),
            "punctuation_style": punctuation,
            "casual_degree": "é«˜" if casual_count >= 3 else "ä¸­" if casual_count >= 1 else "ä½",
            "avg_sentence_length": round(avg_length, 1)
        }

    def estimate_success_factor(self, text: str, metrics: Dict[str, int]) -> str:
        """ãªãœä¼¸ã³ãŸã‹ã‚’æ¨å®šï¼ˆ1æ–‡ã§ï¼‰"""
        # å„è¦ç´ ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        factors = []

        opening = self.analyze_opening(text)
        if opening['pattern'] in ['ç–‘å•å½¢', 'ç§˜åŒ¿æƒ…å ±', 'æ•°å­—å¼·èª¿']:
            factors.append(f"å†’é ­ã®{opening['pattern']}ã§æ³¨æ„ã‚’å¼•ã„ãŸ")

        structure = self.analyze_structure(text)
        if structure['has_list']:
            factors.append("ãƒªã‚¹ãƒˆå½¢å¼ã§æƒ…å ±ã‚’æ•´ç†ã—ãŸ")
        if structure['has_url']:
            factors.append("URLã§è©³ç´°æƒ…å ±ã¸èª˜å°ã—ãŸ")
        if structure['number_count'] >= 3:
            factors.append("å…·ä½“çš„ãªæ•°å­—ã§ä¿¡é ¼æ€§ã‚’é«˜ã‚ãŸ")

        emotions = self.analyze_emotions(text)
        if len(emotions) >= 2:
            factors.append("æ„Ÿæƒ…ã®å¤‰åŒ–ã§èª­è€…ã‚’å¼•ãè¾¼ã‚“ã ")

        closing = self.analyze_closing(text)
        if closing['pattern'] == 'è¡Œå‹•å–šèµ·':
            factors.append("CTAã§æ‹¡æ•£ã‚’ä¿ƒã—ãŸ")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰åˆ¤æ–­
        if 'likes' in metrics and metrics['likes'] > 500:
            if structure['stats']['total_chars'] < 100:
                factors.append("çŸ­æ–‡ã§ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’æœ€å¤§åŒ–ã—ãŸ")

        # ç·åˆåˆ¤æ–­
        if factors:
            return "ã€".join(factors[:2]) + "ãŸã‚"
        else:
            return "èª­è€…ã®å…±æ„Ÿã¾ãŸã¯å½¹ç«‹ã¤æƒ…å ±ã‚’æä¾›ã—ãŸãŸã‚"

    def analyze_post(self, text: str, metrics: Dict[str, int] = None) -> Dict[str, any]:
        """1ã¤ã®æŠ•ç¨¿ã‚’å®Œå…¨åˆ†æ"""
        if metrics is None:
            metrics = {}

        opening = self.analyze_opening(text)
        structure = self.analyze_structure(text)
        emotions = self.analyze_emotions(text)
        closing = self.analyze_closing(text)
        rhythm = self.analyze_rhythm(text)
        success_factor = self.estimate_success_factor(text, metrics)

        return {
            "text": text,
            "metrics": metrics,
            "opening": opening,
            "structure": structure,
            "emotions": emotions,
            "closing": closing,
            "rhythm": rhythm,
            "success_factor": success_factor
        }

    def compare_writing_styles(self, posts_a: List[Dict], posts_b: List[Dict],
                               label_a: str = "ã‚°ãƒ«ãƒ¼ãƒ—A", label_b: str = "ã‚°ãƒ«ãƒ¼ãƒ—B") -> Dict[str, any]:
        """2ã¤ã®æŠ•ç¨¿ã‚°ãƒ«ãƒ¼ãƒ—ã®æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ¯”è¼ƒ"""

        def aggregate_analyses(posts):
            """è¤‡æ•°æŠ•ç¨¿ã®åˆ†æçµæœã‚’é›†è¨ˆ"""
            opening_patterns = []
            structure_types = []
            closing_patterns = []
            rhythm_types = []
            total_chars = []
            casual_degrees = []

            for post in posts:
                analysis = self.analyze_post(post['text'], post.get('metrics', {}))
                opening_patterns.append(analysis['opening']['pattern'])
                structure_types.append(analysis['structure']['structure_type'])
                closing_patterns.append(analysis['closing']['pattern'])
                rhythm_types.append(analysis['rhythm']['rhythm'])
                total_chars.append(analysis['structure']['stats']['total_chars'])
                casual_degrees.append(analysis['rhythm']['casual_degree'])

            return {
                "opening_distribution": pd.Series(opening_patterns).value_counts(normalize=True) * 100,
                "structure_distribution": pd.Series(structure_types).value_counts(normalize=True) * 100,
                "closing_distribution": pd.Series(closing_patterns).value_counts(normalize=True) * 100,
                "rhythm_distribution": pd.Series(rhythm_types).value_counts(normalize=True) * 100,
                "avg_chars": sum(total_chars) / len(total_chars) if total_chars else 0,
                "casual_high_rate": casual_degrees.count('é«˜') / len(casual_degrees) * 100 if casual_degrees else 0
            }

        stats_a = aggregate_analyses(posts_a)
        stats_b = aggregate_analyses(posts_b)

        return {
            label_a: stats_a,
            label_b: stats_b
        }


def generate_detailed_report(posts: List[Dict], output_path: str, title: str = "æŠ•ç¨¿æ–‡ç« è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"):
    """æŠ•ç¨¿ãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°ãªæ–‡ç« åˆ†è§£ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    analyzer = WritingAnalyzer()
    lines = []

    lines.append(f"# {title}")
    lines.append(f"\n**åˆ†ææ—¥**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append(f"**åˆ†æå¯¾è±¡**: {len(posts)}ä»¶ã®æŠ•ç¨¿")
    lines.append("")

    for i, post in enumerate(posts, 1):
        analysis = analyzer.analyze_post(post['text'], post.get('metrics', {}))

        lines.append(f"---")
        lines.append(f"## {i}ä½: {post.get('title', f'æŠ•ç¨¿{i}')}")
        lines.append("")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        if analysis['metrics']:
            lines.append("**ğŸ“Š ãƒ‡ãƒ¼ã‚¿**:")
            for key, value in analysis['metrics'].items():
                lines.append(f"- {key}: {value:,}")
            lines.append("")

        # å…¨æ–‡
        lines.append("**å…¨æ–‡**:")
        lines.append(f"> {analysis['text']}")
        lines.append("")

        # å†’é ­åˆ†æ
        lines.append(f"**å†’é ­ã®å½¹å‰²** ({analysis['opening']['pattern']})")
        lines.append(f"- æœ€åˆã®1æ–‡: ã€Œ{analysis['opening']['first_sentence']}ã€")
        lines.append(f"- å½¹å‰²: {analysis['opening']['role']}")
        lines.append("")

        # å±•é–‹éƒ¨
        lines.append(f"**å±•é–‹éƒ¨** ({analysis['structure']['structure_type']})")
        lines.append(f"- æ–‡å­—æ•°: {analysis['structure']['stats']['total_chars']}æ–‡å­—")
        lines.append(f"- æ–‡ã®æ•°: {analysis['structure']['stats']['sentence_count']}æ–‡")
        features = []
        if analysis['structure']['has_list']:
            features.append("ãƒªã‚¹ãƒˆå½¢å¼")
        if analysis['structure']['has_url']:
            features.append("URLå«ã‚€")
        if analysis['structure']['has_quote']:
            features.append("å¼•ç”¨ã‚ã‚Š")
        if analysis['structure']['number_count'] >= 3:
            features.append(f"æ•°å­—{analysis['structure']['number_count']}å€‹")
        if features:
            lines.append(f"- ç‰¹å¾´: {', '.join(features)}")
        lines.append("")

        # æ„Ÿæƒ…ã®å‹•ã
        lines.append("**æ„Ÿæƒ…ã®å‹•ã**:")
        if analysis['emotions']:
            emotion_strs = [f"{block}: {', '.join(emotions)}" for block, emotions in analysis['emotions']]
            lines.append(f"- {' â†’ '.join(emotion_strs)}")
        else:
            lines.append("- æ„Ÿæƒ…ã®å¤‰åŒ–: æ¤œå‡ºã•ã‚Œãšï¼ˆäº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®æŠ•ç¨¿ï¼‰")
        lines.append("")

        # ç· ã‚æ–¹
        lines.append(f"**ç· ã‚æ–¹** ({analysis['closing']['pattern']})")
        lines.append(f"- æœ€å¾Œã®1æ–‡: ã€Œ{analysis['closing']['last_sentence']}ã€")
        lines.append(f"- åŠ¹æœ: {analysis['closing']['effect']}")
        lines.append("")

        # ãƒªã‚ºãƒ 
        lines.append(f"**æ–‡ç« ã®ãƒªã‚ºãƒ **:")
        lines.append(f"- ã‚¿ã‚¤ãƒ—: {analysis['rhythm']['rhythm']}")
        lines.append(f"- å¹³å‡æ–‡é•·: {analysis['rhythm']['avg_sentence_length']:.0f}æ–‡å­—")
        lines.append(f"- å¥èª­ç‚¹: {analysis['rhythm']['punctuation_style']}")
        lines.append(f"- å£èªåº¦: {analysis['rhythm']['casual_degree']}")
        lines.append("")

        # ãªãœä¼¸ã³ãŸã‹
        lines.append(f"**ğŸ“Œ ã“ã®æŠ•ç¨¿ãŒãªãœä¼¸ã³ãŸã‹**:")
        lines.append(f"- {analysis['success_factor']}")
        lines.append("")

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_path}")
    return output_path


def generate_comparison_report(posts_a: List[Dict], posts_b: List[Dict],
                               label_a: str, label_b: str, output_path: str):
    """2ã‚°ãƒ«ãƒ¼ãƒ—ã®æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    analyzer = WritingAnalyzer()

    # å€‹åˆ¥ã®è©³ç´°åˆ†æ
    lines = []
    lines.append(f"# {label_a} vs {label_b} æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append(f"\n**åˆ†ææ—¥**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append(f"**{label_a}**: {len(posts_a)}ä»¶")
    lines.append(f"**{label_b}**: {len(posts_b)}ä»¶")
    lines.append("")

    # ã‚°ãƒ«ãƒ¼ãƒ—Aã®è©³ç´°
    lines.append(f"---")
    lines.append(f"## ç¬¬1éƒ¨ï¼š{label_a} è©³ç´°åˆ†è§£")
    lines.append("")

    for i, post in enumerate(posts_a, 1):
        analysis = analyzer.analyze_post(post['text'], post.get('metrics', {}))
        lines.append(f"### {i}ä½: {post.get('title', f'æŠ•ç¨¿{i}')}")
        lines.append("")
        lines.append(f"**å…¨æ–‡**: {analysis['text'][:100]}...")
        lines.append("")
        lines.append(f"- **å†’é ­**: {analysis['opening']['role']}")
        lines.append(f"- **å±•é–‹**: {analysis['structure']['structure_type']}")
        if analysis['emotions']:
            emotion_flow = ' â†’ '.join([', '.join(ems) for _, ems in analysis['emotions']])
            lines.append(f"- **æ„Ÿæƒ…**: {emotion_flow}")
        lines.append(f"- **ç· ã‚**: {analysis['closing']['effect']}")
        lines.append(f"- **ãƒªã‚ºãƒ **: {analysis['rhythm']['rhythm']}")
        lines.append(f"- **ãªãœä¼¸ã³ãŸã‹**: {analysis['success_factor']}")
        lines.append("")

    # ã‚°ãƒ«ãƒ¼ãƒ—Bã®è©³ç´°
    lines.append(f"---")
    lines.append(f"## ç¬¬2éƒ¨ï¼š{label_b} è©³ç´°åˆ†è§£")
    lines.append("")

    for i, post in enumerate(posts_b, 1):
        analysis = analyzer.analyze_post(post['text'], post.get('metrics', {}))
        lines.append(f"### {i}ä½: {post.get('title', f'æŠ•ç¨¿{i}')}")
        lines.append("")
        lines.append(f"**å…¨æ–‡**: {analysis['text'][:100]}...")
        lines.append("")
        lines.append(f"- **å†’é ­**: {analysis['opening']['role']}")
        lines.append(f"- **å±•é–‹**: {analysis['structure']['structure_type']}")
        if analysis['emotions']:
            emotion_flow = ' â†’ '.join([', '.join(ems) for _, ems in analysis['emotions']])
            lines.append(f"- **æ„Ÿæƒ…**: {emotion_flow}")
        lines.append(f"- **ç· ã‚**: {analysis['closing']['effect']}")
        lines.append(f"- **ãƒªã‚ºãƒ **: {analysis['rhythm']['rhythm']}")
        lines.append(f"- **ãªãœä¼¸ã³ãŸã‹**: {analysis['success_factor']}")
        lines.append("")

    # æ¯”è¼ƒçµ±è¨ˆ
    comparison = analyzer.compare_writing_styles(posts_a, posts_b, label_a, label_b)

    lines.append(f"---")
    lines.append(f"## ç¬¬3éƒ¨ï¼šæ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«ã®æ±ºå®šçš„ãªé•ã„")
    lines.append("")

    # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
    lines.append("### 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é•ã„")
    lines.append("")
    lines.append(f"| ãƒ‘ã‚¿ãƒ¼ãƒ³ | {label_a} | {label_b} |")
    lines.append("|:---|---:|---:|")
    all_patterns = set(comparison[label_a]['opening_distribution'].index) | set(comparison[label_b]['opening_distribution'].index)
    for pattern in sorted(all_patterns):
        pct_a = comparison[label_a]['opening_distribution'].get(pattern, 0)
        pct_b = comparison[label_b]['opening_distribution'].get(pattern, 0)
        lines.append(f"| {pattern} | {pct_a:.1f}% | {pct_b:.1f}% |")
    lines.append("")

    # æ–‡ç« æ§‹æˆæ¯”è¼ƒ
    lines.append("### 2. æ–‡ç« æ§‹æˆã®é•ã„")
    lines.append("")
    lines.append(f"| æ§‹æˆã‚¿ã‚¤ãƒ— | {label_a} | {label_b} |")
    lines.append("|:---|---:|---:|")
    all_structures = set(comparison[label_a]['structure_distribution'].index) | set(comparison[label_b]['structure_distribution'].index)
    for struct in sorted(all_structures):
        pct_a = comparison[label_a]['structure_distribution'].get(struct, 0)
        pct_b = comparison[label_b]['structure_distribution'].get(struct, 0)
        lines.append(f"| {struct} | {pct_a:.1f}% | {pct_b:.1f}% |")
    lines.append("")

    # å¹³å‡æ–‡å­—æ•°
    lines.append("### 3. æ–‡å­—æ•°ã®é•ã„")
    lines.append("")
    lines.append(f"- **{label_a}**: å¹³å‡ {comparison[label_a]['avg_chars']:.0f}æ–‡å­—")
    lines.append(f"- **{label_b}**: å¹³å‡ {comparison[label_b]['avg_chars']:.0f}æ–‡å­—")
    lines.append("")

    # å£èªåº¦
    lines.append("### 4. å£èªè¡¨ç¾ã®ä½¿ç”¨åº¦")
    lines.append("")
    lines.append(f"- **{label_a}**: å£èªåº¦ã€Œé«˜ã€ã®å‰²åˆ {comparison[label_a]['casual_high_rate']:.1f}%")
    lines.append(f"- **{label_b}**: å£èªåº¦ã€Œé«˜ã€ã®å‰²åˆ {comparison[label_b]['casual_high_rate']:.1f}%")
    lines.append("")

    # ã¾ã¨ã‚
    lines.append("### ğŸ“Œ æ±ºå®šçš„ãªé•ã„ã®ã¾ã¨ã‚")
    lines.append("")

    # è‡ªå‹•ã§é•ã„ã‚’æ¤œå‡º
    differences = []

    # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€å¤§ã®å·®
    opening_diffs = {}
    for pattern in all_patterns:
        pct_a = comparison[label_a]['opening_distribution'].get(pattern, 0)
        pct_b = comparison[label_b]['opening_distribution'].get(pattern, 0)
        opening_diffs[pattern] = abs(pct_a - pct_b)
    top_opening_diff = max(opening_diffs.items(), key=lambda x: x[1])
    if top_opening_diff[1] > 15:
        differences.append(f"å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€Œ{top_opening_diff[0]}ã€ã®ä½¿ç”¨ç‡ãŒå¤§ããç•°ãªã‚‹ï¼ˆå·®: {top_opening_diff[1]:.1f}%ï¼‰")

    # æ–‡å­—æ•°ã®å·®
    char_diff = abs(comparison[label_a]['avg_chars'] - comparison[label_b]['avg_chars'])
    if char_diff > 30:
        longer = label_a if comparison[label_a]['avg_chars'] > comparison[label_b]['avg_chars'] else label_b
        differences.append(f"æ–‡å­—æ•°ã¯{longer}ã®æ–¹ãŒå¹³å‡{char_diff:.0f}æ–‡å­—é•·ã„")

    # å£èªåº¦ã®å·®
    casual_diff = abs(comparison[label_a]['casual_high_rate'] - comparison[label_b]['casual_high_rate'])
    if casual_diff > 20:
        more_casual = label_a if comparison[label_a]['casual_high_rate'] > comparison[label_b]['casual_high_rate'] else label_b
        differences.append(f"å£èªè¡¨ç¾ã¯{more_casual}ã®æ–¹ãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼ˆå·®: {casual_diff:.1f}%ï¼‰")

    for i, diff in enumerate(differences, 1):
        lines.append(f"{i}. {diff}")

    if not differences:
        lines.append("ä¸¡ã‚°ãƒ«ãƒ¼ãƒ—ã®æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«ã¯æ¯”è¼ƒçš„ä¼¼ã¦ã„ã‚‹")

    lines.append("")

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_path}")
    return output_path


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿
    sample_posts = [
        {
            "text": "å…¨ãã‚ã‹ã‚‰ãªã„ãƒ‰ç´ äººã ã‘ã©ã€Claude Codeå§‹ã‚ã¦ã¿ãŸã€‚ã„ã¾èª¿ã¹ãªãŒã‚‰è§¦ã£ã¦ã‚‹æ®µéšãªã‚“ã ã‘ã©ã€ã“ã‚Œã¡ã‚‡ã£ã¨ã‚„ã°ã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚ç„¡æ–™ã§ä½¿ãˆã‚‹ã‚„ã¤ã§ã‚‚ã‚ã‚‹ç¨‹åº¦ã¯è‡ªå‹•åŒ–ã§ãã¦ãŸã‘ã©ã€ã“ã£ã¡ã®æ–¹ãŒã‚‚ã†1æ®µéšãˆããã†ãªã‚“ã ã‚ˆãªã€‚ã¾ã ä½•ã‚‚ã‚ã‹ã£ã¦ãªã„ã€‚ã§ã‚‚ã€Œã‚ã‹ã‚‰ãªã„ãªã‚Šã«è§¦ã‚‹ã€ãŒã„ã¡ã°ã‚“æ—©ã„æ°—ãŒã—ã¦ããŸã€‚ã“ã“ã‹ã‚‰è‡ªå‹•åŒ–ã€ã©ã‚“ã©ã‚“é€²ã‚ã¦ã„ãã‚ã€‚",
            "metrics": {"impressions": 160105, "likes": 981},
            "title": "160,105ã‚¤ãƒ³ãƒ— / 981ã„ã„ã­"
        }
    ]

    analyzer = WritingAnalyzer()
    for post in sample_posts:
        analysis = analyzer.analyze_post(post['text'], post['metrics'])
        print(f"å†’é ­: {analysis['opening']}")
        print(f"æ§‹æˆ: {analysis['structure']['structure_type']}")
        print(f"æ„Ÿæƒ…: {analysis['emotions']}")
        print(f"ç· ã‚: {analysis['closing']}")
        print(f"ãƒªã‚ºãƒ : {analysis['rhythm']}")
        print(f"æˆåŠŸè¦å› : {analysis['success_factor']}")
