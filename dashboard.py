"""Streamlit ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - ãƒã‚ºãƒã‚¹ãƒˆåˆ†æ"""

import os
import re
from collections import defaultdict

import pandas as pd
import streamlit as st

# ===== ãƒšãƒ¼ã‚¸è¨­å®š =====
st.set_page_config(
    page_title="ãƒã‚ºãƒã‚¹ãƒˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“Š",
    layout="wide",
)


# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====
def classify_category(text):
    """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
    if re.search(r'é”æˆ|åç›Š|ç¨¼ã’ãŸ|ç¨¼ã„ã |æˆåŠŸ|å®Ÿç¸¾|å„²ã‹ã£ãŸ|ã€œä¸‡å††|æœˆå|å¹´å|å£²ä¸Š|å ±é…¬|åˆ©ç›Š', text, re.IGNORECASE):
        return "å®Ÿç¸¾å ±å‘Šç³»"
    if re.search(r'æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—|ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯|æ”»ç•¥', text, re.IGNORECASE):
        return "ãƒã‚¦ãƒã‚¦ç³»"
    if re.search(r'ç§ãŒ|åƒ•ãŒ|è‡ªåˆ†ãŒ|å®Ÿéš›ã«|ã‚„ã£ã¦ã¿ãŸ|è©¦ã—ã¦ã¿ãŸ|ä½“é¨“|çµŒé¨“', text, re.IGNORECASE):
        return "ä½“é¨“è«‡ç³»"
    if re.search(r'ã¯ï¼Ÿ|å•é¡Œ|å±é™º|æ³¨æ„|è­¦å‘Š|ã€æ‚²å ±ã€‘|ã€œã™ãã‚‹|ãƒ¤ãƒã„', text, re.IGNORECASE):
        return "å•é¡Œæèµ·ç³»"
    if re.search(r'ãƒ„ãƒ¼ãƒ«|ã‚¢ãƒ—ãƒª|ã‚µãƒ¼ãƒ“ã‚¹|ãŠã™ã™ã‚|ç´¹ä»‹|AI|Claude|ChatGPT|GPT', text, re.IGNORECASE):
        return "ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»"
    if re.search(r'ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|é–‹å§‹|é€Ÿå ±|æœ€æ–°|ãƒ‹ãƒ¥ãƒ¼ã‚¹|å…¬é–‹', text, re.IGNORECASE):
        return "ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»"
    return "ãã®ä»–"


def find_data_files():
    """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    files = []
    for search_dir in [".", "output"]:
        if not os.path.isdir(search_dir):
            continue
        for f in os.listdir(search_dir):
            if re.match(r"buzz_posts_\d{8}\.(csv|xlsx)$", f):
                files.append(os.path.join(search_dir, f))
    return sorted(files)


def load_data(file_path):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    if file_path.endswith(".csv"):
        return pd.read_csv(file_path)
    else:
        return pd.read_excel(file_path)


# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ =====
st.sidebar.title("ğŸ“Š ãƒã‚ºãƒã‚¹ãƒˆåˆ†æ")
st.sidebar.markdown("---")

# ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
data_files = find_data_files()
if not data_files:
    st.error("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« buzz_analyzer.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

selected_file = st.sidebar.selectbox("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«", data_files)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = load_data(selected_file)

# ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’è¿½åŠ 
df["ã‚«ãƒ†ã‚´ãƒª"] = df["æœ¬æ–‡"].apply(lambda x: classify_category(str(x)))

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.markdown("### ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

min_likes = st.sidebar.slider(
    "æœ€å°ã„ã„ã­æ•°",
    min_value=0,
    max_value=int(df["ã„ã„ã­æ•°"].max()) if "ã„ã„ã­æ•°" in df.columns else 1000,
    value=0,
    step=50,
)

# ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¸¯ãƒ•ã‚£ãƒ«ã‚¿
df["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°_num"] = pd.to_numeric(df["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"], errors="coerce").fillna(0).astype(int)
has_follower_data = (df["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°_num"] > 0).sum() >= 5
if has_follower_data:
    max_followers = int(df["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°_num"].max())
    follower_range = st.sidebar.slider(
        "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã®ç¯„å›²",
        min_value=0,
        max_value=max_followers,
        value=(0, max_followers),
        step=100,
    )
else:
    follower_range = None

categories = st.sidebar.multiselect(
    "ã‚«ãƒ†ã‚´ãƒª",
    options=sorted(df["ã‚«ãƒ†ã‚´ãƒª"].unique()),
    default=sorted(df["ã‚«ãƒ†ã‚´ãƒª"].unique()),
)

keyword_filter = st.sidebar.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")

# ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
df_filtered = df[df["ã„ã„ã­æ•°"] >= min_likes]
df_filtered = df_filtered[df_filtered["ã‚«ãƒ†ã‚´ãƒª"].isin(categories)]
if keyword_filter:
    df_filtered = df_filtered[df_filtered["æœ¬æ–‡"].str.contains(keyword_filter, na=False)]
if follower_range is not None:
    df_filtered = df_filtered[
        (df_filtered["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°_num"] >= follower_range[0]) &
        (df_filtered["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°_num"] <= follower_range[1])
    ]

st.sidebar.markdown(f"**è¡¨ç¤ºä»¶æ•°:** {len(df_filtered)} / {len(df)}ä»¶")


# ===== ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ =====
st.title("ğŸ“Š ãƒã‚ºãƒã‚¹ãƒˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# ã‚¿ãƒ–
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“ˆ æ¦‚è¦", "ğŸ“‹ æŠ•ç¨¿ä¸€è¦§", "ğŸ” è©³ç´°åˆ†æ", "âœï¸ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ", "ğŸ”¬ é«˜åº¦ãªåˆ†æ"])


# ===== ã‚¿ãƒ–1: æ¦‚è¦ =====
with tab1:
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æŠ•ç¨¿æ•°", f"{len(df_filtered)}ä»¶")
    with col2:
        st.metric("å¹³å‡ã„ã„ã­", f"{df_filtered['ã„ã„ã­æ•°'].mean():.0f}")
    with col3:
        st.metric("æœ€å¤§ã„ã„ã­", f"{df_filtered['ã„ã„ã­æ•°'].max():,}")
    with col4:
        st.metric("ä¸­å¤®å€¤", f"{df_filtered['ã„ã„ã­æ•°'].median():.0f}")

    st.markdown("---")

    # ãƒãƒ£ãƒ¼ãƒˆ
    col_left, col_right = st.columns(2)

    with col_left:
        st.subheader("ã‚«ãƒ†ã‚´ãƒªåˆ¥ å¹³å‡ã„ã„ã­æ•°")
        cat_stats = df_filtered.groupby("ã‚«ãƒ†ã‚´ãƒª")["ã„ã„ã­æ•°"].agg(["mean", "count"]).reset_index()
        cat_stats.columns = ["ã‚«ãƒ†ã‚´ãƒª", "å¹³å‡ã„ã„ã­æ•°", "ä»¶æ•°"]
        cat_stats = cat_stats.sort_values("å¹³å‡ã„ã„ã­æ•°", ascending=True)
        st.bar_chart(cat_stats.set_index("ã‚«ãƒ†ã‚´ãƒª")["å¹³å‡ã„ã„ã­æ•°"])

    with col_right:
        st.subheader("ã‚«ãƒ†ã‚´ãƒªåˆ¥ æŠ•ç¨¿æ•°")
        cat_counts = df_filtered["ã‚«ãƒ†ã‚´ãƒª"].value_counts()
        st.bar_chart(cat_counts)

    # ã„ã„ã­æ•°åˆ†å¸ƒ
    st.subheader("ã„ã„ã­æ•°ã®åˆ†å¸ƒ")
    hist_data = df_filtered["ã„ã„ã­æ•°"].clip(upper=df_filtered["ã„ã„ã­æ•°"].quantile(0.95))
    st.bar_chart(hist_data.value_counts().sort_index())


# ===== ã‚¿ãƒ–2: æŠ•ç¨¿ä¸€è¦§ =====
with tab2:
    st.subheader("æŠ•ç¨¿ä¸€è¦§")

    sort_col = st.selectbox("ä¸¦ã³æ›¿ãˆ", ["ã„ã„ã­æ•°", "ãƒªãƒã‚¹ãƒˆæ•°", "ãƒªãƒ—ãƒ©ã‚¤æ•°"], index=0)
    df_sorted = df_filtered.sort_values(sort_col, ascending=False)

    for i, (_, row) in enumerate(df_sorted.head(50).iterrows(), 1):
        with st.expander(
            f"#{i} | â¤ï¸ {row['ã„ã„ã­æ•°']:,} | ğŸ”„ {row['ãƒªãƒã‚¹ãƒˆæ•°']:,} | "
            f"[{row['ã‚«ãƒ†ã‚´ãƒª']}] @{row['ãƒ¦ãƒ¼ã‚¶ãƒ¼å']}"
        ):
            st.markdown(f"**æœ¬æ–‡:**")
            st.text(str(row["æœ¬æ–‡"])[:500])
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ã„ã„ã­", f"{row['ã„ã„ã­æ•°']:,}")
            with col2:
                st.metric("ãƒªãƒã‚¹ãƒˆ", f"{row['ãƒªãƒã‚¹ãƒˆæ•°']:,}")
            with col3:
                st.metric("ãƒªãƒ—ãƒ©ã‚¤", f"{row['ãƒªãƒ—ãƒ©ã‚¤æ•°']:,}")
            if pd.notna(row.get("ãƒã‚¹ãƒˆURL")):
                st.markdown(f"[ãƒã‚¹ãƒˆã‚’è¦‹ã‚‹]({row['ãƒã‚¹ãƒˆURL']})")


# ===== ã‚¿ãƒ–3: è©³ç´°åˆ†æ =====
with tab3:
    st.subheader("è©³ç´°åˆ†æ")

    analysis_type = st.selectbox(
        "åˆ†æã‚¿ã‚¤ãƒ—",
        ["CTAåˆ†æ", "æ„Ÿæƒ…åˆ†æ", "æ–‡å­—æ•°åˆ†æ", "TOPæŠ•ç¨¿ã®å…±é€šç‚¹"],
    )

    if analysis_type == "CTAåˆ†æ":
        cta_patterns = {
            "ã„ã„ã­ç³»": r'ã„ã„ã­|ğŸ‘|ãƒãƒ¼ãƒˆ',
            "ä¿å­˜ç³»": r'ä¿å­˜|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
            "ãƒ•ã‚©ãƒ­ãƒ¼ç³»": r'ãƒ•ã‚©ãƒ­ãƒ¼|follow',
            "ã‚·ã‚§ã‚¢ç³»": r'ãƒªãƒã‚¹ãƒˆ|RT|ã‚·ã‚§ã‚¢|æ‹¡æ•£',
            "ã‚³ãƒ¡ãƒ³ãƒˆç³»": r'ã‚³ãƒ¡ãƒ³ãƒˆ|è¿”ä¿¡|æ•™ãˆã¦',
        }

        cta_results = []
        for cta_type, pattern in cta_patterns.items():
            matches = df_filtered[df_filtered["æœ¬æ–‡"].str.contains(pattern, na=False, flags=re.IGNORECASE)]
            cta_results.append({
                "CTAç¨®é¡": cta_type,
                "ä»¶æ•°": len(matches),
                "å¹³å‡ã„ã„ã­": matches["ã„ã„ã­æ•°"].mean() if len(matches) > 0 else 0,
            })

        no_cta = df_filtered.copy()
        for pattern in cta_patterns.values():
            no_cta = no_cta[~no_cta["æœ¬æ–‡"].str.contains(pattern, na=False, flags=re.IGNORECASE)]
        cta_results.append({
            "CTAç¨®é¡": "CTAãªã—",
            "ä»¶æ•°": len(no_cta),
            "å¹³å‡ã„ã„ã­": no_cta["ã„ã„ã­æ•°"].mean() if len(no_cta) > 0 else 0,
        })

        cta_df = pd.DataFrame(cta_results)
        st.dataframe(cta_df, use_container_width=True)
        st.bar_chart(cta_df.set_index("CTAç¨®é¡")["å¹³å‡ã„ã„ã­"])

    elif analysis_type == "æ„Ÿæƒ…åˆ†æ":
        emotion_patterns = {
            "æœŸå¾…": r'ãƒãƒ£ãƒ³ã‚¹|å¯èƒ½æ€§|ç¨¼ã’ã‚‹|å„²ã‹ã‚‹|æˆåŠŸ|é”æˆ|å®Ÿç¾|ã§ãã‚‹',
            "é©šã": r'ã¾ã•ã‹|ã³ã£ãã‚Š|é©šã|ã™ã”ã„|ã‚„ã°ã„',
            "å…±æ„Ÿ": r'ã‚ã‹ã‚‹|ãã†ãã†|ã‚ã‚‹ã‚ã‚‹|åŒã˜|ç§ã‚‚',
            "ææ€–": r'å±é™º|æ€–ã„|ãƒªã‚¹ã‚¯|å¤±æ•—|æ|ãƒ¤ãƒã„|æœ€æ‚ª',
        }

        emotion_results = []
        for emotion, pattern in emotion_patterns.items():
            matches = df_filtered[df_filtered["æœ¬æ–‡"].str.contains(pattern, na=False, flags=re.IGNORECASE)]
            emotion_results.append({
                "æ„Ÿæƒ…": emotion,
                "ä»¶æ•°": len(matches),
                "å¹³å‡ã„ã„ã­": matches["ã„ã„ã­æ•°"].mean() if len(matches) > 0 else 0,
            })

        em_df = pd.DataFrame(emotion_results)
        st.dataframe(em_df, use_container_width=True)
        st.bar_chart(em_df.set_index("æ„Ÿæƒ…")["å¹³å‡ã„ã„ã­"])

    elif analysis_type == "æ–‡å­—æ•°åˆ†æ":
        df_temp = df_filtered.copy()
        df_temp["æ–‡å­—æ•°"] = df_temp["æœ¬æ–‡"].apply(lambda x: len(str(x)))

        col1, col2 = st.columns(2)
        with col1:
            st.metric("å¹³å‡æ–‡å­—æ•°", f"{df_temp['æ–‡å­—æ•°'].mean():.0f}å­—")
        with col2:
            st.metric("ä¸­å¤®å€¤", f"{df_temp['æ–‡å­—æ•°'].median():.0f}å­—")

        # ä¸Šä½25%ã¨ä¸‹ä½25%ã®æ¯”è¼ƒ
        top_25 = df_temp.nlargest(len(df_temp) // 4, "ã„ã„ã­æ•°")
        bottom_25 = df_temp.nsmallest(len(df_temp) // 4, "ã„ã„ã­æ•°")

        st.markdown("### ã„ã„ã­æ•°ä¸Šä½25% vs ä¸‹ä½25%")
        comp_col1, comp_col2 = st.columns(2)
        with comp_col1:
            st.metric("ä¸Šä½25%ã®å¹³å‡æ–‡å­—æ•°", f"{top_25['æ–‡å­—æ•°'].mean():.0f}å­—")
        with comp_col2:
            st.metric("ä¸‹ä½25%ã®å¹³å‡æ–‡å­—æ•°", f"{bottom_25['æ–‡å­—æ•°'].mean():.0f}å­—")

    elif analysis_type == "TOPæŠ•ç¨¿ã®å…±é€šç‚¹":
        top10 = df_filtered.nlargest(10, "ã„ã„ã­æ•°")

        st.markdown("### TOP10æŠ•ç¨¿")
        for i, (_, row) in enumerate(top10.iterrows(), 1):
            st.markdown(f"**{i}ä½** ({row['ã„ã„ã­æ•°']:,}ã„ã„ã­) - [{row['ã‚«ãƒ†ã‚´ãƒª']}]")
            st.text(str(row["æœ¬æ–‡"])[:200])
            st.markdown("---")

        # å…±é€šç‚¹åˆ†æ
        st.markdown("### å…±é€šç‚¹")
        avg_len = top10["æœ¬æ–‡"].apply(lambda x: len(str(x))).mean()
        top_cats = top10["ã‚«ãƒ†ã‚´ãƒª"].value_counts()

        st.markdown(f"- **å¹³å‡æ–‡å­—æ•°:** {avg_len:.0f}å­—")
        st.markdown(f"- **æœ€å¤šã‚«ãƒ†ã‚´ãƒª:** {top_cats.index[0]}ï¼ˆ{top_cats.values[0]}ä»¶ï¼‰")

        has_url = top10["æœ¬æ–‡"].str.contains(r'https?://', na=False).sum()
        st.markdown(f"- **URLå«ã‚€:** {has_url}ä»¶ / 10ä»¶")

        has_emoji = top10["æœ¬æ–‡"].str.contains(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ¤€-ğŸ§¿ğŸ©°-ğŸ«¿]', na=False, regex=True).sum()
        st.markdown(f"- **çµµæ–‡å­—ã‚ã‚Š:** {has_emoji}ä»¶ / 10ä»¶")


# ===== ã‚¿ãƒ–4: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ =====
with tab4:
    st.subheader("âœï¸ ãƒã‚ºãƒã‚¹ãƒˆ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ")
    st.markdown("åˆ†æãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€ãƒã‚ºã‚Šã‚„ã™ã„æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

    if st.button("ğŸ² ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹", type="primary"):
        try:
            from generate_posts import generate_posts, extract_trending_topics, extract_effective_ctas

            posts, tools, works, ctas = generate_posts(df_filtered, n=5)

            st.markdown("### ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±")
            st.markdown(f"- **äººæ°—ãƒ„ãƒ¼ãƒ«:** {', '.join(tools)}")
            st.markdown(f"- **äººæ°—ã‚¸ãƒ£ãƒ³ãƒ«:** {', '.join(works)}")
            st.markdown(f"- **åŠ¹æœçš„ãªCTA:** {', '.join(ctas[:3])}")

            st.markdown("---")

            for i, post in enumerate(posts, 1):
                st.markdown(f"### ç”Ÿæˆæ¡ˆ{i}: {post['type']}")
                st.code(post["text"] + (f"\n\n{post['cta']}" if post.get("cta") else ""), language=None)
                st.info(f"ğŸ’¡ **Tips:** {post['tips']}")

        except Exception as e:
            st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    st.markdown("---")
    st.markdown("### ä½¿ã„æ–¹")
    st.markdown("""
    1. ã€Œãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™
    2. æ°—ã«å…¥ã£ãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸ã¶
    3. æ•°å­—ãƒ»ãƒ„ãƒ¼ãƒ«åã‚’è‡ªåˆ†ã®å®Ÿç¸¾ã«ç½®ãæ›ãˆ
    4. çµµæ–‡å­—ã‚’2ã€œ3å€‹è¿½åŠ 
    5. æœ7ã€œ9æ™‚ or å¤œ19ã€œ21æ™‚ã«æŠ•ç¨¿
    """)


# ===== ã‚¿ãƒ–5: é«˜åº¦ãªåˆ†æ =====
with tab5:
    st.subheader("ğŸ”¬ é«˜åº¦ãªåˆ†æ")

    advanced_type = st.selectbox(
        "åˆ†æã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
        ["ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢", "ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–", "ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ", "ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°", "ç«¶åˆãƒã‚¸ã‚·ãƒ§ãƒ³", "ãƒ•ãƒƒã‚¯å¼·åº¦", "è­°è«–èª˜ç™ºåº¦"],
        key="advanced_analysis",
    )

    if advanced_type == "ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢":
        st.markdown("### æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚ºäºˆæ¸¬")
        st.markdown("æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒã‚ºã‚Šã‚„ã™ã•ã‚’0-100ç‚¹ã§äºˆæ¸¬ã—ã¾ã™ã€‚")

        user_text = st.text_area("æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=150, key="buzz_score_input")
        if st.button("ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—", type="primary", key="calc_score"):
            if user_text.strip():
                try:
                    from analyze_posts import calculate_buzz_score
                    result = calculate_buzz_score(user_text)
                    score = result["total_score"]

                    col1, col2 = st.columns([1, 2])
                    with col1:
                        st.metric("ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢", f"{score}/100ç‚¹")
                        if score >= 70:
                            st.success("ãƒã‚ºã‚‹å¯èƒ½æ€§ãŒé«˜ã„ï¼")
                        elif score >= 50:
                            st.info("æ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
                        else:
                            st.warning("è¦æ”¹å–„")

                    with col2:
                        st.markdown("#### è¦ç´ åˆ¥ã‚¹ã‚³ã‚¢")
                        factor_df = pd.DataFrame([
                            {"è¦ç´ ": k, "ã‚¹ã‚³ã‚¢": v} for k, v in result["factors"].items()
                        ])
                        st.bar_chart(factor_df.set_index("è¦ç´ ")["ã‚¹ã‚³ã‚¢"])

                except Exception as e:
                    st.error(f"ã‚¹ã‚³ã‚¢è¨ˆç®—ã«å¤±æ•—: {e}")
            else:
                st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        # å…¨æŠ•ç¨¿ã®ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        st.markdown("---")
        st.markdown("### å…¨æŠ•ç¨¿ã®ã‚¹ã‚³ã‚¢åˆ†æ")
        try:
            from analyze_posts import analyze_buzz_scores
            buzz_data = analyze_buzz_scores(df_filtered)

            st.markdown(f"**äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¨å®Ÿã„ã„ã­æ•°ã®ç›¸é–¢:** r={buzz_data['correlation']:.2f}")

            score_df = pd.DataFrame([
                {"ã‚¹ã‚³ã‚¢å¸¯": k, "ä»¶æ•°": len(v), "å¹³å‡ã„ã„ã­": sum(v)/len(v) if v else 0}
                for k, v in buzz_data["score_buckets"].items()
            ])
            st.dataframe(score_df, use_container_width=True)

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")

    elif advanced_type == "ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–":
        try:
            from analyze_posts import analyze_text_length, analyze_emoji_usage, analyze_hashtag_usage

            st.markdown("### æ–‡å­—æ•°åˆ†æ")
            tl_data = analyze_text_length(df_filtered)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å¹³å‡æ–‡å­—æ•°", f"{tl_data['avg_length']:.0f}å­—")
            with col2:
                st.metric("æœ€é©æ–‡å­—æ•°å¸¯", tl_data["best_bucket"])
            with col3:
                st.metric("ç›¸é–¢ä¿‚æ•°", f"r={tl_data['correlation']:.2f}")

            bucket_df = pd.DataFrame([
                {"æ–‡å­—æ•°å¸¯": k, "ä»¶æ•°": len(v), "å¹³å‡ã„ã„ã­": sum(v)/len(v) if v else 0}
                for k, v in tl_data["bucket_data"].items()
            ])
            st.bar_chart(bucket_df.set_index("æ–‡å­—æ•°å¸¯")["å¹³å‡ã„ã„ã­"])

            st.markdown("---")
            st.markdown("### çµµæ–‡å­—åˆ†æ")
            emoji_data = analyze_emoji_usage(df_filtered)
            col1, col2 = st.columns(2)
            with col1:
                avg_with = sum(emoji_data["with_emoji"]) / len(emoji_data["with_emoji"]) if emoji_data["with_emoji"] else 0
                st.metric("çµµæ–‡å­—ã‚ã‚Š å¹³å‡ã„ã„ã­", f"{avg_with:.0f}")
            with col2:
                avg_without = sum(emoji_data["without_emoji"]) / len(emoji_data["without_emoji"]) if emoji_data["without_emoji"] else 0
                st.metric("çµµæ–‡å­—ãªã— å¹³å‡ã„ã„ã­", f"{avg_without:.0f}")

            emoji_count_df = pd.DataFrame([
                {"çµµæ–‡å­—æ•°": k, "ä»¶æ•°": len(v), "å¹³å‡ã„ã„ã­": sum(v)/len(v) if v else 0}
                for k, v in emoji_data["emoji_count_data"].items()
            ])
            st.dataframe(emoji_count_df, use_container_width=True)

            st.markdown("---")
            st.markdown("### ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°åˆ†æ")
            ht_data = analyze_hashtag_usage(df_filtered)
            col1, col2 = st.columns(2)
            with col1:
                avg_ht = sum(ht_data["with_hashtag"]) / len(ht_data["with_hashtag"]) if ht_data["with_hashtag"] else 0
                st.metric("ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚ã‚Š å¹³å‡ã„ã„ã­", f"{avg_ht:.0f}")
            with col2:
                avg_no_ht = sum(ht_data["without_hashtag"]) / len(ht_data["without_hashtag"]) if ht_data["without_hashtag"] else 0
                st.metric("ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãªã— å¹³å‡ã„ã„ã­", f"{avg_no_ht:.0f}")

            if ht_data["top_hashtags"]:
                st.markdown("**äººæ°—ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°:**")
                for tag, count in ht_data["top_hashtags"][:10]:
                    st.markdown(f"- {tag} ({count}ä»¶)")

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")

    elif advanced_type == "ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ":
        try:
            from analyze_posts import analyze_users, filter_keywords

            # é‡è¤‡é™¤å»å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            df_raw = filter_keywords(df)

            user_data = analyze_users(df_raw, df_filtered)

            st.markdown("### ãƒªãƒ”ãƒ¼ãƒˆãƒã‚ºãƒ¦ãƒ¼ã‚¶ãƒ¼")
            if user_data["repeat_buzzers"]:
                repeat_df = pd.DataFrame(user_data["repeat_buzzers"][:20])
                repeat_df.columns = ["ãƒ¦ãƒ¼ã‚¶ãƒ¼", "æŠ•ç¨¿æ•°", "å¹³å‡ã„ã„ã­", "æœ€å¤§ã„ã„ã­", "åˆè¨ˆã„ã„ã­", "æ¨™æº–åå·®"]
                st.dataframe(repeat_df, use_container_width=True)
            else:
                st.info("ãƒªãƒ”ãƒ¼ãƒˆãƒã‚ºãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            st.markdown("---")
            st.markdown("### æŠ•ç¨¿é »åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ")
            freq_df = pd.DataFrame([
                {"æŠ•ç¨¿æ•°": k, "ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°": len(v), "å¹³å‡ã„ã„ã­": sum(v)/len(v) if v else 0}
                for k, v in user_data["freq_data"].items()
            ])
            st.dataframe(freq_df, use_container_width=True)

            traits = user_data["common_traits"]
            if traits["total"] > 0:
                st.markdown("---")
                st.markdown("### å¸¸é€£ãƒã‚ºã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ç‰¹å¾´")
                avg_len = sum(traits["text_lengths"]) / len(traits["text_lengths"]) if traits["text_lengths"] else 0
                st.markdown(f"- **å¹³å‡æŠ•ç¨¿æ–‡å­—æ•°:** {avg_len:.0f}å­—")
                if traits["categories"]:
                    top_cat = traits["categories"].most_common(1)[0]
                    st.markdown(f"- **æœ€å¤šã‚«ãƒ†ã‚´ãƒª:** {top_cat[0]}ï¼ˆ{top_cat[1]}ä»¶ï¼‰")
                cta_rate = (traits["cta_count"] / traits["total"]) * 100
                st.markdown(f"- **CTAä½¿ç”¨ç‡:** {cta_rate:.0f}%")

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")

    elif advanced_type == "ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°":
        try:
            from analyze_posts import analyze_viral_coefficient
            viral = analyze_viral_coefficient(df_filtered)

            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°", f"{viral['avg_coeff']:.3f}")
            with col2:
                st.metric("ä¸­å¤®å€¤", f"{viral['median_coeff']:.3f}")

            st.markdown("### ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°")
            cat_df = pd.DataFrame([
                {"ã‚«ãƒ†ã‚´ãƒª": k, "ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°": v} for k, v in
                sorted(viral["cat_viral"].items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(cat_df, use_container_width=True)

            st.markdown("### æ‹¡æ•£åŠ›TOP10")
            for i, item in enumerate(viral["top10_viral"][:10], 1):
                st.markdown(f"{i}. **{item['likes']:,}ã„ã„ã­ / {item['retweets']:,}RT** (ä¿‚æ•°: {item['viral_coeff']:.3f}) - {item['text']}...")

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")

    elif advanced_type == "ç«¶åˆãƒã‚¸ã‚·ãƒ§ãƒ³":
        try:
            from analyze_posts import analyze_competitive_position
            comp = analyze_competitive_position(df_filtered)

            st.markdown("### å››è±¡é™ãƒãƒˆãƒªã‚¯ã‚¹")
            pos_df = pd.DataFrame(comp["positions"])
            pos_df.columns = ["ã‚«ãƒ†ã‚´ãƒª", "æŠ•ç¨¿æ•°", "å¹³å‡ã„ã„ã­", "ãƒã‚¸ã‚·ãƒ§ãƒ³"]
            st.dataframe(pos_df, use_container_width=True)

            blue = [p for p in comp["positions"] if "ãƒ–ãƒ«ãƒ¼ã‚ªãƒ¼ã‚·ãƒ£ãƒ³" in p["quadrant"]]
            if blue:
                st.success("**ç‹™ã„ç›®:** " + ", ".join(p["category"] for p in blue))
            red = [p for p in comp["positions"] if "ãƒ¬ãƒƒãƒ‰ã‚ªãƒ¼ã‚·ãƒ£ãƒ³" in p["quadrant"]]
            if red:
                st.warning("**é£½å’Œ:** " + ", ".join(p["category"] for p in red))

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")

    elif advanced_type == "ãƒ•ãƒƒã‚¯å¼·åº¦":
        try:
            from analyze_posts import analyze_hook_strength
            hook = analyze_hook_strength(df_filtered)

            st.markdown("### ãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¨®é¡åˆ¥ã®åŠ¹æœ")
            pw_df = pd.DataFrame([
                {"ãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰": k, "ä»¶æ•°": len(v), "å¹³å‡ã„ã„ã­": sum(v)/len(v) if v else 0}
                for k, v in sorted(hook["pw_type_data"].items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
            ])
            st.dataframe(pw_df, use_container_width=True)
            if pw_df is not None and len(pw_df) > 0:
                st.bar_chart(pw_df.set_index("ãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰")["å¹³å‡ã„ã„ã­"])

            st.markdown("### TOP10é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ•ãƒƒã‚¯")
            for i, h in enumerate(hook["top_hooks"][:10], 1):
                pw_str = ", ".join(h["power_words"]) if h["power_words"] else "ãªã—"
                st.markdown(f"{i}. **{h['likes']:,}ã„ã„ã­** [{pw_str}] - {h['first_line']}")

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")

    elif advanced_type == "è­°è«–èª˜ç™ºåº¦":
        try:
            from analyze_posts import analyze_discussion_inducement
            disc = analyze_discussion_inducement(df_filtered)

            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡è­°è«–èª˜ç™ºåº¦", f"{disc['avg_rate']:.3f}")
            with col2:
                st.metric("ä¸­å¤®å€¤", f"{disc['median_rate']:.3f}")

            st.markdown("### ã‚«ãƒ†ã‚´ãƒªåˆ¥è­°è«–èª˜ç™ºåº¦")
            cat_df = pd.DataFrame([
                {"ã‚«ãƒ†ã‚´ãƒª": k, "è­°è«–èª˜ç™ºåº¦": v} for k, v in
                sorted(disc["cat_discussion"].items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(cat_df, use_container_width=True)

            st.markdown("### è­°è«–ã‚’å‘¼ã¶ãƒã‚¹ãƒˆTOP10")
            for i, item in enumerate(disc["top10_discussion"][:10], 1):
                st.markdown(f"{i}. **{item['likes']:,}ã„ã„ã­ / {item['replies']}ãƒªãƒ—** (èª˜ç™ºåº¦: {item['discussion_rate']:.3f}) - {item['text']}...")

        except Exception as e:
            st.error(f"åˆ†æã«å¤±æ•—: {e}")
