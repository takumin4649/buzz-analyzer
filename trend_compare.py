"""è¤‡æ•°æ—¥ã®ãƒã‚ºãƒã‚¹ãƒˆ ãƒˆãƒ¬ãƒ³ãƒ‰æ¯”è¼ƒåˆ†æ"""

import os
import re
from collections import defaultdict
from datetime import datetime

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

from visualize import setup_japanese_font, COLORS

FONT = setup_japanese_font()


def find_data_files(data_dir="."):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSV/Excelï¼‰ã‚’æ—¥ä»˜é †ã«æ¤œç´¢"""
    files = []
    patterns = [
        (r"buzz_posts_(\d{8})\.csv", "csv"),
        (r"buzz_posts_(\d{8})\.xlsx", "xlsx"),
    ]

    for search_dir in [data_dir, os.path.join(data_dir, "output")]:
        if not os.path.isdir(search_dir):
            continue
        for filename in os.listdir(search_dir):
            for pattern, fmt in patterns:
                match = re.match(pattern, filename)
                if match:
                    date_str = match.group(1)
                    files.append({
                        "path": os.path.join(search_dir, filename),
                        "date": date_str,
                        "format": fmt,
                    })

    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆã€åŒæ—¥ã¯Excelå„ªå…ˆ
    files.sort(key=lambda x: (x["date"], 0 if x["format"] == "xlsx" else 1))

    # åŒæ—¥ã®é‡è¤‡é™¤å»ï¼ˆExcelå„ªå…ˆï¼‰
    seen = set()
    unique = []
    for f in files:
        if f["date"] not in seen:
            seen.add(f["date"])
            unique.append(f)

    return unique


def load_data_file(file_info):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§DataFrameã‚’è¿”ã™"""
    if file_info["format"] == "csv":
        return pd.read_csv(file_info["path"])
    else:
        return pd.read_excel(file_info["path"])


def classify_category(text):
    """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆanalyze_posts.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    if re.search(r'é”æˆ|åç›Š|ç¨¼ã’ãŸ|ç¨¼ã„ã |æˆåŠŸ|å®Ÿç¸¾|å„²ã‹ã£ãŸ|ã€œä¸‡å††|æœˆå|å¹´å|å£²ä¸Š|å ±é…¬|åˆ©ç›Š', text, re.IGNORECASE):
        return "å®Ÿç¸¾å ±å‘Šç³»"
    if re.search(r'æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—|ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯|æ”»ç•¥|ãƒãƒ‹ãƒ¥ã‚¢ãƒ«|ã‚¬ã‚¤ãƒ‰|ã€œã™ã‚‹æ–¹æ³•|ã€œã®ã‚„ã‚Šæ–¹', text, re.IGNORECASE):
        return "ãƒã‚¦ãƒã‚¦ç³»"
    if re.search(r'ç§ãŒ|åƒ•ãŒ|è‡ªåˆ†ãŒ|å®Ÿéš›ã«|ã‚„ã£ã¦ã¿ãŸ|è©¦ã—ã¦ã¿ãŸ|ä½“é¨“|çµŒé¨“|ã€œã—ãŸã‚‰|ã€œã—ã¦ã¿ãŸ', text, re.IGNORECASE):
        return "ä½“é¨“è«‡ç³»"
    if re.search(r'ã¯ï¼Ÿ|å•é¡Œ|å±é™º|æ³¨æ„|è­¦å‘Š|ã€æ‚²å ±ã€‘|ã€œã™ãã‚‹|ãƒ¤ãƒã„|ãŠã‹ã—ã„', text, re.IGNORECASE):
        return "å•é¡Œæèµ·ç³»"
    if re.search(r'ãƒ„ãƒ¼ãƒ«|ã‚¢ãƒ—ãƒª|ã‚µãƒ¼ãƒ“ã‚¹|ãƒ—ãƒ©ã‚°ã‚¤ãƒ³|æ‹¡å¼µæ©Ÿèƒ½|ãŠã™ã™ã‚|ç´¹ä»‹|AI|Claude|ChatGPT|GPT', text, re.IGNORECASE):
        return "ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»"
    if re.search(r'ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|é–‹å§‹|é–‹å‚¬|é€Ÿå ±|æœ€æ–°|ãƒ‹ãƒ¥ãƒ¼ã‚¹|å…¬é–‹', text, re.IGNORECASE):
        return "ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»"
    return "ãã®ä»–"


def extract_keywords(df, top_n=10):
    """é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    keyword_pattern = re.compile(
        r'(ChatGPT|Claude|Claude Code|Gemini|Copilot|Midjourney|'
        r'Stable Diffusion|DALL-E|Canva|Notion AI|Cursor|v0|Bolt|'
        r'å‰¯æ¥­|ç¨¼ã|åç›Š|è‡ªå‹•åŒ–|AI|ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°|ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°|'
        r'ãƒ‡ã‚¶ã‚¤ãƒ³|å‹•ç”»|YouTube|SNS|ãƒ–ãƒ­ã‚°|ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ)',
        re.IGNORECASE
    )

    counts = defaultdict(int)
    for _, row in df.iterrows():
        text = str(row.get("æœ¬æ–‡", ""))
        found = set()
        for match in keyword_pattern.finditer(text):
            word = match.group()
            if word not in found:
                counts[word] += 1
                found.add(word)

    return sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_n]


def compare_trends(data_dir="."):
    """è¤‡æ•°æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒåˆ†æ"""
    files = find_data_files(data_dir)

    if len(files) < 2:
        print("æ¯”è¼ƒã«ã¯2æ—¥åˆ†ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
        print(f"ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}ä»¶")
        return None

    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« {len(files)}æ—¥åˆ†ã‚’æ¤œå‡º:")
    for f in files:
        print(f"  - {f['date']}: {f['path']}")

    daily_stats = []

    for file_info in files:
        df = load_data_file(file_info)
        date = file_info["date"]

        # åŸºæœ¬çµ±è¨ˆ
        stats = {
            "date": date,
            "post_count": len(df),
            "avg_likes": df["ã„ã„ã­æ•°"].mean() if "ã„ã„ã­æ•°" in df.columns else 0,
            "max_likes": df["ã„ã„ã­æ•°"].max() if "ã„ã„ã­æ•°" in df.columns else 0,
            "median_likes": df["ã„ã„ã­æ•°"].median() if "ã„ã„ã­æ•°" in df.columns else 0,
            "total_likes": df["ã„ã„ã­æ•°"].sum() if "ã„ã„ã­æ•°" in df.columns else 0,
        }

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        cat_counts = defaultdict(int)
        for _, row in df.iterrows():
            text = str(row.get("æœ¬æ–‡", ""))
            cat = classify_category(text)
            cat_counts[cat] += 1
        stats["categories"] = dict(cat_counts)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        stats["keywords"] = extract_keywords(df)

        daily_stats.append(stats)

    return daily_stats, files


def generate_trend_charts(daily_stats, output_dir="output/charts"):
    """ãƒˆãƒ¬ãƒ³ãƒ‰æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    os.makedirs(output_dir, exist_ok=True)

    dates = [s["date"] for s in daily_stats]
    date_labels = [f"{d[4:6]}/{d[6:8]}" for d in dates]

    # æŠ•ç¨¿æ•°ã¨å¹³å‡ã„ã„ã­æ•°ã®æ¨ç§»
    fig, ax1 = plt.subplots(figsize=(10, 6))

    counts = [s["post_count"] for s in daily_stats]
    avg_likes = [s["avg_likes"] for s in daily_stats]

    ax1.bar(date_labels, counts, color="#4472C4", alpha=0.7, label="æŠ•ç¨¿æ•°")
    ax1.set_xlabel("æ—¥ä»˜" if FONT else "Date")
    ax1.set_ylabel("æŠ•ç¨¿æ•°" if FONT else "Post Count", color="#4472C4")

    ax2 = ax1.twinx()
    ax2.plot(date_labels, avg_likes, "o-", color="#ED7D31", linewidth=2, markersize=8, label="å¹³å‡ã„ã„ã­æ•°")
    ax2.set_ylabel("å¹³å‡ã„ã„ã­æ•°" if FONT else "Avg Likes", color="#ED7D31")

    ax1.set_title("æ—¥åˆ¥ æŠ•ç¨¿æ•°ã¨å¹³å‡ã„ã„ã­æ•°ã®æ¨ç§»" if FONT else "Daily Post Count & Avg Likes")

    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc="upper left")

    plt.tight_layout()
    path = os.path.join(output_dir, "trend_daily.png")
    fig.savefig(path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¨ç§»ï¼ˆç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ï¼‰
    all_categories = set()
    for s in daily_stats:
        all_categories.update(s["categories"].keys())
    all_categories = sorted(all_categories)

    fig, ax = plt.subplots(figsize=(12, 6))
    bottom = [0] * len(daily_stats)

    for i, cat in enumerate(all_categories):
        values = [s["categories"].get(cat, 0) for s in daily_stats]
        ax.bar(date_labels, values, bottom=bottom, label=cat,
               color=COLORS[i % len(COLORS)])
        bottom = [b + v for b, v in zip(bottom, values)]

    ax.set_xlabel("æ—¥ä»˜" if FONT else "Date")
    ax.set_ylabel("æŠ•ç¨¿æ•°" if FONT else "Post Count")
    ax.set_title("ã‚«ãƒ†ã‚´ãƒªåˆ¥æŠ•ç¨¿æ•°ã®æ¨ç§»" if FONT else "Category Distribution Over Time")
    ax.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()

    path2 = os.path.join(output_dir, "trend_categories.png")
    fig.savefig(path2, dpi=150, bbox_inches="tight")
    plt.close(fig)

    return {"æ—¥åˆ¥æ¨ç§»": path, "ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¨ç§»": path2}


def generate_trend_report(daily_stats, output_file):
    """ãƒˆãƒ¬ãƒ³ãƒ‰æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("# ãƒã‚ºãƒã‚¹ãƒˆ ãƒˆãƒ¬ãƒ³ãƒ‰æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n\n")
        f.write(f"**æ¯”è¼ƒæœŸé–“:** {daily_stats[0]['date']} ã€œ {daily_stats[-1]['date']}\n\n")
        f.write("---\n\n")

        # æ—¥åˆ¥ã‚µãƒãƒªãƒ¼
        f.write("## 1. æ—¥åˆ¥ã‚µãƒãƒªãƒ¼\n\n")
        f.write("| æ—¥ä»˜ | æŠ•ç¨¿æ•° | å¹³å‡ã„ã„ã­ | æœ€å¤§ã„ã„ã­ | ä¸­å¤®å€¤ |\n")
        f.write("|------|--------|-----------|-----------|--------|\n")
        for s in daily_stats:
            d = s["date"]
            f.write(f"| {d[4:6]}/{d[6:8]} | {s['post_count']}ä»¶ | "
                    f"{s['avg_likes']:.0f} | {s['max_likes']:,} | {s['median_likes']:.0f} |\n")
        f.write("\n")

        # ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–
        if len(daily_stats) >= 2:
            prev = daily_stats[-2]
            curr = daily_stats[-1]

            f.write("## 2. å‰æ—¥æ¯”ã®å¤‰åŒ–\n\n")

            count_diff = curr["post_count"] - prev["post_count"]
            likes_diff = curr["avg_likes"] - prev["avg_likes"]
            count_emoji = "ğŸ“ˆ" if count_diff > 0 else "ğŸ“‰" if count_diff < 0 else "â†’"
            likes_emoji = "ğŸ“ˆ" if likes_diff > 0 else "ğŸ“‰" if likes_diff < 0 else "â†’"

            f.write(f"- **æŠ•ç¨¿æ•°:** {prev['post_count']}ä»¶ â†’ {curr['post_count']}ä»¶ "
                    f"({count_emoji} {count_diff:+d}ä»¶)\n")
            f.write(f"- **å¹³å‡ã„ã„ã­:** {prev['avg_likes']:.0f} â†’ {curr['avg_likes']:.0f} "
                    f"({likes_emoji} {likes_diff:+.0f})\n\n")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¨ç§»
        f.write("## 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¨ç§»\n\n")
        all_categories = set()
        for s in daily_stats:
            all_categories.update(s["categories"].keys())

        header = "| ã‚«ãƒ†ã‚´ãƒª | " + " | ".join(f"{s['date'][4:6]}/{s['date'][6:8]}" for s in daily_stats) + " |\n"
        separator = "|---------|" + "|".join(["------" for _ in daily_stats]) + "|\n"
        f.write(header)
        f.write(separator)

        for cat in sorted(all_categories):
            row = f"| {cat} | "
            row += " | ".join(f"{s['categories'].get(cat, 0)}ä»¶" for s in daily_stats)
            row += " |\n"
            f.write(row)
        f.write("\n")

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯”è¼ƒ
        f.write("## 4. ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯”è¼ƒ\n\n")
        for s in daily_stats:
            f.write(f"### {s['date'][4:6]}/{s['date'][6:8]}ã®ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n\n")
            for word, count in s["keywords"][:10]:
                f.write(f"- **{word}**: {count}ä»¶\n")
            f.write("\n")

        # æ–°ç™»å ´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
        if len(daily_stats) >= 2:
            prev_words = {w for w, _ in daily_stats[-2]["keywords"]}
            curr_words = {w for w, _ in daily_stats[-1]["keywords"]}
            new_words = curr_words - prev_words

            if new_words:
                f.write("### æ–°ãŸã«ãƒˆãƒ¬ãƒ³ãƒ‰å…¥ã‚Šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n\n")
                for word in new_words:
                    f.write(f"- **{word}** (NEW)\n")
                f.write("\n")

        f.write("---\n\n")
        f.write("*ã‚°ãƒ©ãƒ•ã¯ `output/charts/` ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™*\n")

    print(f"ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    result = compare_trends(".")

    if result is None:
        return

    daily_stats, files = result

    os.makedirs("output", exist_ok=True)
    today = datetime.now().strftime("%Y%m%d")
    output_file = f"output/trend_report_{today}.md"

    # ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ
    print("\nãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    chart_paths = generate_trend_charts(daily_stats)
    for name, path in chart_paths.items():
        print(f"  âœ“ {name}: {path}")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    generate_trend_report(daily_stats, output_file)


if __name__ == "__main__":
    main()
