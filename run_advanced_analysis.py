"""é«˜åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°ãƒ»ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ãƒ»ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢æ¤œè¨¼"""

import os
import re
from collections import defaultdict
from datetime import datetime

import pandas as pd

from analyze_posts import (
    POWER_WORDS,
    analyze_buzz_scores,
    analyze_follower_normalized,
    analyze_viral_coefficient,
    calculate_buzz_score,
    classify_category,
    classify_opening_pattern,
    filter_data,
    has_story,
    load_excel,
    safe_get,
)

# === ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===

BUZZ_FILE = "output/buzz_posts_20260215.xlsx"
SELF_FILE = "output/TwExport_20260217_191942.csv"
OUTPUT_FILE = "output/advanced_analysis_20260217.md"


def load_self_posts(filepath):
    """è‡ªåˆ†ã®æŠ•ç¨¿CSVã‚’èª­ã¿è¾¼ã‚€"""
    df = pd.read_csv(filepath, encoding="utf-8-sig")
    df["ã„ã„ã­æ•°"] = pd.to_numeric(df["ã„ã„ã­æ•°"], errors="coerce").fillna(0).astype(int)
    df["ãƒªãƒã‚¹ãƒˆæ•°"] = pd.to_numeric(df["ãƒªãƒã‚¹ãƒˆæ•°"], errors="coerce").fillna(0).astype(int)
    df["ãƒªãƒ—ãƒ©ã‚¤æ•°"] = pd.to_numeric(df["ãƒªãƒ—ãƒ©ã‚¤æ•°"], errors="coerce").fillna(0).astype(int)
    df["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"] = pd.to_numeric(df["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"], errors="coerce").fillna(0).astype(int)
    print(f"è‡ªåˆ†ã®æŠ•ç¨¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    return df


def detect_cta(text):
    """CTAæœ‰ç„¡ã‚’åˆ¤å®š"""
    cta_patterns = [
        r"ã„ã„ã­|ğŸ‘", r"ä¿å­˜|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", r"ãƒ•ã‚©ãƒ­ãƒ¼",
        r"ãƒªãƒã‚¹ãƒˆ|RT|ã‚·ã‚§ã‚¢|æ‹¡æ•£", r"ã‚³ãƒ¡ãƒ³ãƒˆ|è¿”ä¿¡|æ•™ãˆã¦",
    ]
    return any(re.search(p, text, re.IGNORECASE) for p in cta_patterns)


def count_power_words(text):
    """ãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¨®é¡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    count = 0
    types = []
    for pw_type, pattern in POWER_WORDS.items():
        if pattern.search(text):
            count += 1
            types.append(pw_type)
    return count, types


# === åˆ†æ1: ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°åˆ†æ ===

def generate_viral_section(df_buzz, df_self):
    """ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
    viral = analyze_viral_coefficient(df_buzz)
    lines = []

    lines.append("## 1. ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°åˆ†æ")
    lines.append("")
    lines.append("> ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° = ãƒªãƒã‚¹ãƒˆæ•° / ã„ã„ã­æ•°ï¼ˆã‚·ã‚§ã‚¢ã•ã‚Œã‚„ã™ã•ã®æŒ‡æ¨™ï¼‰")
    lines.append("")

    # 1.1 å…¨ä½“æ¦‚è¦
    lines.append("### 1.1 å…¨ä½“æ¦‚è¦")
    lines.append("")
    lines.append(f"- **å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°:** {viral['avg_coeff']:.3f}")
    lines.append(f"- **ä¸­å¤®å€¤:** {viral['median_coeff']:.3f}")
    lines.append(f"- **é«˜ãƒã‚¤ãƒ©ãƒ«ç¾¤:** {viral['high_viral_traits']['count']}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {viral['high_viral_traits']['avg_likes']:.0f}ï¼‰")
    lines.append(f"- **ä½ãƒã‚¤ãƒ©ãƒ«ç¾¤:** {viral['low_viral_traits']['count']}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {viral['low_viral_traits']['avg_likes']:.0f}ï¼‰")
    lines.append("")

    # 1.2 ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°TOP10
    lines.append("### 1.2 ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° TOP10ï¼ˆã‚·ã‚§ã‚¢ã•ã‚Œã‚„ã™ã„æŠ•ç¨¿ï¼‰")
    lines.append("")
    lines.append("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | RT | ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
    lines.append("|------|---------|--------|-----|------------|------------|")
    for i, d in enumerate(viral["top10_viral"][:10], 1):
        text_preview = d["text"].replace("|", "ï½œ").replace("\n", " ")[:40]
        lines.append(f"| {i} | @{d['user']} | {d['likes']:,} | {d['retweets']:,} | {d['viral_coeff']:.3f} | {text_preview} |")
    lines.append("")

    # 1.3 é«˜ã„ã„ã­Ã—ä½RT vs é«˜RT ã®ç‰¹å¾´æ¯”è¼ƒ
    lines.append("### 1.3 é«˜ã„ã„ã­Ã—ä½RT vs é«˜RTæŠ•ç¨¿ã®ç‰¹å¾´æ¯”è¼ƒ")
    lines.append("")

    # ãƒã‚¤ãƒ©ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å†è¨ˆç®—ï¼ˆæŠ•ç¨¿å˜ä½ã®è©³ç´°ãŒå¿…è¦ï¼‰
    all_viral = []
    for _, row in df_buzz.iterrows():
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        rts = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        text = safe_get(row, "æœ¬æ–‡", "")
        if likes > 0:
            vc = rts / likes
            pw_count, pw_types = count_power_words(text)
            all_viral.append({
                "likes": likes, "rts": rts, "viral_coeff": vc,
                "text": text, "category": classify_category(text),
                "has_cta": detect_cta(text), "has_story": has_story(text),
                "pw_count": pw_count, "pw_types": pw_types,
                "length": len(text),
            })

    all_viral.sort(key=lambda x: x["viral_coeff"], reverse=True)
    median_vc = viral["median_coeff"]

    # é«˜ã„ã„ã­Ã—ä½RT: ã„ã„ã­ä¸Šä½25%ã§ã€ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°ãŒä¸­å¤®å€¤ä»¥ä¸‹
    likes_75 = sorted([d["likes"] for d in all_viral])[int(len(all_viral) * 0.75)] if all_viral else 0
    high_like_low_rt = [d for d in all_viral if d["likes"] >= likes_75 and d["viral_coeff"] <= median_vc]
    high_rt = [d for d in all_viral if d["viral_coeff"] > median_vc and d["likes"] >= likes_75]

    def group_stats(group, label):
        if not group:
            return f"**{label}:** ãƒ‡ãƒ¼ã‚¿ãªã—\n"
        avg_likes = sum(d["likes"] for d in group) / len(group)
        avg_rts = sum(d["rts"] for d in group) / len(group)
        avg_vc = sum(d["viral_coeff"] for d in group) / len(group)
        avg_len = sum(d["length"] for d in group) / len(group)
        cta_rate = sum(1 for d in group if d["has_cta"]) / len(group) * 100
        story_rate = sum(1 for d in group if d["has_story"]) / len(group) * 100
        avg_pw = sum(d["pw_count"] for d in group) / len(group)

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        cats = defaultdict(int)
        for d in group:
            cats[d["category"]] += 1
        top_cat = sorted(cats.items(), key=lambda x: x[1], reverse=True)[:3]
        cat_str = ", ".join(f"{c}({n}ä»¶)" for c, n in top_cat)

        # ãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¨®é¡
        pw_freq = defaultdict(int)
        for d in group:
            for t in d["pw_types"]:
                pw_freq[t] += 1
        top_pw = sorted(pw_freq.items(), key=lambda x: x[1], reverse=True)[:3]
        pw_str = ", ".join(f"{p}({n})" for p, n in top_pw)

        result = f"**{label}**ï¼ˆ{len(group)}ä»¶ï¼‰\n"
        result += f"- å¹³å‡ã„ã„ã­: {avg_likes:.0f} / å¹³å‡RT: {avg_rts:.0f} / å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°: {avg_vc:.3f}\n"
        result += f"- å¹³å‡æ–‡å­—æ•°: {avg_len:.0f}å­—\n"
        result += f"- CTAä½¿ç”¨ç‡: {cta_rate:.0f}% / ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§: {story_rate:.0f}%\n"
        result += f"- å¹³å‡ãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¨®é¡æ•°: {avg_pw:.1f}\n"
        result += f"- ä¸»ãªã‚«ãƒ†ã‚´ãƒª: {cat_str}\n"
        result += f"- ä¸»ãªãƒ‘ãƒ¯ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {pw_str}\n"
        return result

    lines.append(group_stats(high_like_low_rt, "é«˜ã„ã„ã­Ã—ä½RTç¾¤ï¼ˆã„ã„ã­ã¯å¤šã„ãŒã‚·ã‚§ã‚¢ã•ã‚Œã«ãã„ï¼‰"))
    lines.append("")
    lines.append(group_stats(high_rt, "é«˜ã„ã„ã­Ã—é«˜RTç¾¤ï¼ˆã„ã„ã­ã‚‚RTã‚‚å¤šã„ï¼‰"))
    lines.append("")

    # 1.4 ä½•ãŒã‚·ã‚§ã‚¢ã‚’ç”Ÿã‚€ã®ã‹
    lines.append("### 1.4 ä½•ãŒã‚·ã‚§ã‚¢ã‚’ç”Ÿã‚€ã®ã‹ï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰")
    lines.append("")

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°
    lines.append("**ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°:**")
    lines.append("")
    lines.append("| ã‚«ãƒ†ã‚´ãƒª | å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° |")
    lines.append("|---------|----------------|")
    for cat, avg in sorted(viral["cat_viral"].items(), key=lambda x: x[1], reverse=True):
        lines.append(f"| {cat} | {avg:.3f} |")
    lines.append("")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°
    lines.append("**å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°:**")
    lines.append("")
    lines.append("| ãƒ‘ã‚¿ãƒ¼ãƒ³ | å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° |")
    lines.append("|---------|----------------|")
    for pat, avg in sorted(viral["pattern_viral"].items(), key=lambda x: x[1], reverse=True):
        lines.append(f"| {pat} | {avg:.3f} |")
    lines.append("")

    # ã‚·ã‚§ã‚¢ã‚’ç”Ÿã‚€è¦å› ã¾ã¨ã‚
    top_cat_viral = max(viral["cat_viral"].items(), key=lambda x: x[1]) if viral["cat_viral"] else ("N/A", 0)
    top_pat_viral = max(viral["pattern_viral"].items(), key=lambda x: x[1]) if viral["pattern_viral"] else ("N/A", 0)

    lines.append("**ã‚·ã‚§ã‚¢ã‚’ç”Ÿã‚€ä¸»ãªè¦å› :**")
    lines.append(f"- ã‚«ãƒ†ã‚´ãƒªã€Œ{top_cat_viral[0]}ã€ãŒãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°æœ€é«˜ï¼ˆ{top_cat_viral[1]:.3f}ï¼‰")
    lines.append(f"- å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã€Œ{top_pat_viral[0]}ã€ãŒã‚·ã‚§ã‚¢ã•ã‚Œã‚„ã™ã„ï¼ˆ{top_pat_viral[1]:.3f}ï¼‰")

    # é«˜ãƒã‚¤ãƒ©ãƒ«ç¾¤ã®CTAãƒ»ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç‡
    high_viral_posts = [d for d in all_viral if d["viral_coeff"] > median_vc]
    low_viral_posts = [d for d in all_viral if d["viral_coeff"] <= median_vc]
    if high_viral_posts and low_viral_posts:
        hv_cta = sum(1 for d in high_viral_posts if d["has_cta"]) / len(high_viral_posts) * 100
        lv_cta = sum(1 for d in low_viral_posts if d["has_cta"]) / len(low_viral_posts) * 100
        hv_story = sum(1 for d in high_viral_posts if d["has_story"]) / len(high_viral_posts) * 100
        lv_story = sum(1 for d in low_viral_posts if d["has_story"]) / len(low_viral_posts) * 100
        lines.append(f"- CTAä½¿ç”¨ç‡: é«˜ãƒã‚¤ãƒ©ãƒ«ç¾¤ {hv_cta:.0f}% vs ä½ãƒã‚¤ãƒ©ãƒ«ç¾¤ {lv_cta:.0f}%")
        lines.append(f"- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§: é«˜ãƒã‚¤ãƒ©ãƒ«ç¾¤ {hv_story:.0f}% vs ä½ãƒã‚¤ãƒ©ãƒ«ç¾¤ {lv_story:.0f}%")
    lines.append("")

    # 1.5 @Mr_botenã®ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°
    lines.append("### 1.5 @Mr_botenã®ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°")
    lines.append("")
    self_viral = []
    for _, row in df_self.iterrows():
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        rts = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        text = safe_get(row, "æœ¬æ–‡", "")
        if likes > 0:
            vc = rts / likes
            self_viral.append({"likes": likes, "rts": rts, "viral_coeff": vc, "text": text[:50]})

    if self_viral:
        avg_self_vc = sum(d["viral_coeff"] for d in self_viral) / len(self_viral)
        lines.append(f"- **è‡ªåˆ†ã®å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°:** {avg_self_vc:.3f}ï¼ˆãƒã‚ºæŠ•ç¨¿å¹³å‡: {viral['avg_coeff']:.3f}ï¼‰")
        diff_pct = ((avg_self_vc - viral["avg_coeff"]) / viral["avg_coeff"] * 100) if viral["avg_coeff"] > 0 else 0
        if diff_pct > 0:
            lines.append(f"- ãƒã‚ºæŠ•ç¨¿ã‚ˆã‚Š **{diff_pct:.0f}%é«˜ã„** â†’ ã‚·ã‚§ã‚¢ã•ã‚Œã‚„ã™ã„æŠ•ç¨¿å‚¾å‘")
        else:
            lines.append(f"- ãƒã‚ºæŠ•ç¨¿ã‚ˆã‚Š **{abs(diff_pct):.0f}%ä½ã„** â†’ ã„ã„ã­ã¯ä»˜ããŒã‚·ã‚§ã‚¢ã•ã‚Œã«ãã„å‚¾å‘")
        lines.append("")

        # è‡ªåˆ†ã®ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°TOP5
        self_viral.sort(key=lambda x: x["viral_coeff"], reverse=True)
        lines.append("**è‡ªåˆ†ã®ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° TOP5:**")
        lines.append("")
        lines.append("| é †ä½ | ã„ã„ã­ | RT | ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•° | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
        lines.append("|------|--------|-----|------------|------------|")
        for i, d in enumerate(self_viral[:5], 1):
            text_preview = d["text"].replace("|", "ï½œ").replace("\n", " ")[:40]
            lines.append(f"| {i} | {d['likes']:,} | {d['rts']:,} | {d['viral_coeff']:.3f} | {text_preview} |")
        lines.append("")
    else:
        lines.append("è‡ªåˆ†ã®æŠ•ç¨¿ã§ã„ã„ã­æ•°>0ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        lines.append("")

    return "\n".join(lines)


# === åˆ†æ2: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ– ===

def generate_follower_section(df_buzz, df_self):
    """ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
    fnorm = analyze_follower_normalized(df_buzz)
    lines = []

    lines.append("## 2. ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ")
    lines.append("")

    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹åˆ¤å®š
    has_follower = fnorm["has_follower_data"]

    if has_follower:
        lines.append("> ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ = (ã„ã„ã­æ•° / ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°) Ã— 100")
        lines.append("")

        # 2.1 ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        lines.append("### 2.1 ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP10")
        lines.append("")
        lines.append("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ | ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
        lines.append("|------|---------|--------|-----------|-----------------|------------|")
        for i, d in enumerate(fnorm["top10"], 1):
            text_preview = d["text"].replace("|", "ï½œ").replace("\n", " ")[:35]
            lines.append(f"| {i} | @{d['user']} | {d['likes']:,} | {d['followers']:,} | {d['rate']:.2f}% | {text_preview} |")
        lines.append("")

        # 2.2 Hidden Gems
        lines.append("### 2.2 Hidden Gemsï¼ˆå°‘ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ Ã— é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼‰")
        lines.append("")
        if fnorm["hidden_gems"]:
            lines.append("> ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãŒä¸­å¤®å€¤ä»¥ä¸‹ & ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãŒ75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Šã®æŠ•ç¨¿")
            lines.append("")
            lines.append("| ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ | ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
            lines.append("|---------|--------|-----------|-----------------|------------|")
            for d in fnorm["hidden_gems"]:
                text_preview = d["text"].replace("|", "ï½œ").replace("\n", " ")[:35]
                lines.append(f"| @{d['user']} | {d['likes']:,} | {d['followers']:,} | {d['rate']:.2f}% | {text_preview} |")
        else:
            lines.append("Hidden Gemsã«è©²å½“ã™ã‚‹æŠ•ç¨¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        lines.append("")

        # 2.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥
        lines.append("### 2.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡")
        lines.append("")

        if fnorm["category_by_rate"]:
            lines.append("**ã‚«ãƒ†ã‚´ãƒªåˆ¥:**")
            lines.append("")
            lines.append("| ã‚«ãƒ†ã‚´ãƒª | å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ | ä»¶æ•° |")
            lines.append("|---------|---------------------|------|")
            for cat, rates in sorted(fnorm["category_by_rate"].items(),
                                      key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0, reverse=True):
                avg_rate = sum(rates) / len(rates) if rates else 0
                lines.append(f"| {cat} | {avg_rate:.2f}% | {len(rates)} |")
            lines.append("")

        if fnorm["pattern_by_rate"]:
            lines.append("**å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥:**")
            lines.append("")
            lines.append("| ãƒ‘ã‚¿ãƒ¼ãƒ³ | å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ | ä»¶æ•° |")
            lines.append("|---------|---------------------|------|")
            for pat, rates in sorted(fnorm["pattern_by_rate"].items(),
                                      key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0, reverse=True):
                avg_rate = sum(rates) / len(rates) if rates else 0
                lines.append(f"| {pat} | {avg_rate:.2f}% | {len(rates)} |")
            lines.append("")

    else:
        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãƒ‡ãƒ¼ã‚¿ãªã— â†’ ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã§ä»£æ›¿
        lines.append("> ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ‡ãƒ¼ã‚¿ãŒæœªå–å¾—ã®ãŸã‚ã€**ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢**ã§ä»£æ›¿åˆ†æ")
        lines.append("> ç·åˆã‚¹ã‚³ã‚¢ = ã„ã„ã­ + ãƒªãƒã‚¹ãƒˆÃ—2 + ãƒªãƒ—ãƒ©ã‚¤Ã—3")
        lines.append("")

        # ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        buzz_engagement = []
        for _, row in df_buzz.iterrows():
            likes = safe_get(row, "ã„ã„ã­æ•°", 0)
            rts = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
            replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)
            text = safe_get(row, "æœ¬æ–‡", "")
            user = safe_get(row, "ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "")
            total = likes + rts * 2 + replies * 3
            buzz_engagement.append({
                "user": user, "likes": likes, "rts": rts, "replies": replies,
                "total": total, "text": text,
            })
        buzz_engagement.sort(key=lambda x: x["total"], reverse=True)

        # 2.1 ç·åˆã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        lines.append("### 2.1 ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP10")
        lines.append("")
        lines.append("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | RT | ãƒªãƒ—ãƒ©ã‚¤ | ç·åˆã‚¹ã‚³ã‚¢ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
        lines.append("|------|---------|--------|-----|---------|----------|------------|")
        for i, d in enumerate(buzz_engagement[:10], 1):
            text_preview = d["text"][:35].replace("|", "ï½œ").replace("\n", " ")
            lines.append(f"| {i} | @{d['user']} | {d['likes']:,} | {d['rts']:,} | {d['replies']:,} | {d['total']:,} | {text_preview} |")
        lines.append("")

        # 2.2 Hidden Gems: ã„ã„ã­æ•°ã¯ä¸­å¤®å€¤ä»¥ä¸‹ã ãŒç·åˆã‚¹ã‚³ã‚¢ãŒé«˜ã„
        lines.append("### 2.2 Hidden Gemsï¼ˆã„ã„ã­æ•°ä»¥ä¸Šã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãŒé«˜ã„æŠ•ç¨¿ï¼‰")
        lines.append("")
        lines.append("> ã„ã„ã­æ•°ã¯ä¸­å¤®å€¤ä»¥ä¸‹ã ãŒã€RTãƒ»ãƒªãƒ—ãƒ©ã‚¤ã‚’å«ã‚€ç·åˆã‚¹ã‚³ã‚¢ãŒ75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Š")
        lines.append("")

        if buzz_engagement:
            likes_list = sorted([d["likes"] for d in buzz_engagement])
            total_list = sorted([d["total"] for d in buzz_engagement])
            median_likes = likes_list[len(likes_list) // 2]
            total_75 = total_list[int(len(total_list) * 0.75)]

            gems = [d for d in buzz_engagement if d["likes"] <= median_likes and d["total"] >= total_75]
            gems.sort(key=lambda x: x["total"], reverse=True)

            if gems:
                lines.append("| ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­ | RT | ãƒªãƒ—ãƒ©ã‚¤ | ç·åˆã‚¹ã‚³ã‚¢ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
                lines.append("|---------|--------|-----|---------|----------|------------|")
                for d in gems[:5]:
                    text_preview = d["text"][:35].replace("|", "ï½œ").replace("\n", " ")
                    lines.append(f"| @{d['user']} | {d['likes']:,} | {d['rts']:,} | {d['replies']:,} | {d['total']:,} | {text_preview} |")
                lines.append("")
                lines.append(f"Hidden Gemsç‰¹å¾´: ã„ã„ã­æ•°ãŒ{median_likes}ä»¥ä¸‹ã§ã‚‚ã€RTãƒ»ãƒªãƒ—ãƒ©ã‚¤ã§ç·åˆã‚¹ã‚³ã‚¢{total_75}ä»¥ä¸Šã‚’é”æˆ")
            else:
                lines.append("è©²å½“ã™ã‚‹æŠ•ç¨¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        lines.append("")

        # 2.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ ç·åˆã‚¹ã‚³ã‚¢
        lines.append("### 2.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢")
        lines.append("")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥
        cat_scores = defaultdict(list)
        pat_scores = defaultdict(list)
        for d in buzz_engagement:
            cat = classify_category(d["text"])
            cat_scores[cat].append(d["total"])
            first_line = d["text"].split("\n")[0] if d["text"] else ""
            pat = classify_opening_pattern(first_line)
            pat_scores[pat].append(d["total"])

        lines.append("**ã‚«ãƒ†ã‚´ãƒªåˆ¥:**")
        lines.append("")
        lines.append("| ã‚«ãƒ†ã‚´ãƒª | å¹³å‡ç·åˆã‚¹ã‚³ã‚¢ | ä»¶æ•° |")
        lines.append("|---------|-------------|------|")
        for cat, scores in sorted(cat_scores.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0, reverse=True):
            avg = sum(scores) / len(scores) if scores else 0
            lines.append(f"| {cat} | {avg:.0f} | {len(scores)} |")
        lines.append("")

        lines.append("**å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥:**")
        lines.append("")
        lines.append("| ãƒ‘ã‚¿ãƒ¼ãƒ³ | å¹³å‡ç·åˆã‚¹ã‚³ã‚¢ | ä»¶æ•° |")
        lines.append("|---------|-------------|------|")
        for pat, scores in sorted(pat_scores.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0, reverse=True):
            avg = sum(scores) / len(scores) if scores else 0
            lines.append(f"| {pat} | {avg:.0f} | {len(scores)} |")
        lines.append("")

    # 2.4 @Mr_botenã®ãƒã‚¸ã‚·ãƒ§ãƒ³
    lines.append("### 2.4 @Mr_botenã®ãƒã‚¸ã‚·ãƒ§ãƒ³")
    lines.append("")

    if len(df_self) == 0:
        lines.append("è‡ªåˆ†ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        lines.append("")
        lines.append("> `output/TwExport_20260217_191942.csv` ã‚’é…ç½®ã—ã¦å†å®Ÿè¡Œã™ã‚‹ã¨ã€è‡ªåˆ†ã®æŠ•ç¨¿ã¨ã®æ¯”è¼ƒãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        lines.append("")
    else:
        self_followers = df_self["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"].max() if len(df_self) > 0 else 0

        if self_followers > 0 and has_follower:
            self_rates = []
            for _, row in df_self.iterrows():
                likes = safe_get(row, "ã„ã„ã­æ•°", 0)
                text = safe_get(row, "æœ¬æ–‡", "")
                rate = (likes / self_followers) * 100
                self_rates.append({"likes": likes, "rate": rate, "text": text[:50]})

            avg_self_rate = sum(d["rate"] for d in self_rates) / len(self_rates) if self_rates else 0

            buzz_rates_all = []
            for rates in fnorm["category_by_rate"].values():
                buzz_rates_all.extend(rates)
            avg_buzz_rate = sum(buzz_rates_all) / len(buzz_rates_all) if buzz_rates_all else 0

            lines.append(f"- **ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°:** {self_followers:,}")
            lines.append(f"- **å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡:** {avg_self_rate:.2f}%ï¼ˆãƒã‚ºæŠ•ç¨¿å¹³å‡: {avg_buzz_rate:.2f}%ï¼‰")
            lines.append("")

            self_rates.sort(key=lambda x: x["rate"], reverse=True)
            lines.append("**è‡ªåˆ†ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ TOP5:**")
            lines.append("")
            lines.append("| é †ä½ | ã„ã„ã­ | ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
            lines.append("|------|--------|-----------------|------------|")
            for i, d in enumerate(self_rates[:5], 1):
                text_preview = d["text"].replace("|", "ï½œ").replace("\n", " ")[:40]
                lines.append(f"| {i} | {d['likes']:,} | {d['rate']:.2f}% | {text_preview} |")
            lines.append("")
        else:
            # ç·åˆã‚¹ã‚³ã‚¢ã§æ¯”è¼ƒ
            lines.append("**ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã§ã®æ¯”è¼ƒ:**")
            lines.append("")
            self_eng = []
            for _, row in df_self.iterrows():
                likes = safe_get(row, "ã„ã„ã­æ•°", 0)
                rts = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
                replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)
                text = safe_get(row, "æœ¬æ–‡", "")
                total = likes + rts * 2 + replies * 3
                self_eng.append({"total": total, "likes": likes, "text": text[:50]})
            self_eng.sort(key=lambda x: x["total"], reverse=True)

            avg_self = sum(d["total"] for d in self_eng) / len(self_eng) if self_eng else 0
            avg_buzz = sum(d["total"] for d in buzz_engagement) / len(buzz_engagement) if buzz_engagement else 0

            lines.append(f"- **è‡ªåˆ†ã®å¹³å‡ç·åˆã‚¹ã‚³ã‚¢:** {avg_self:.0f}ï¼ˆãƒã‚ºæŠ•ç¨¿å¹³å‡: {avg_buzz:.0f}ï¼‰")
            lines.append("")

            if self_eng:
                lines.append("**è‡ªåˆ†ã®ç·åˆã‚¹ã‚³ã‚¢ TOP5:**")
                lines.append("")
                lines.append("| é †ä½ | ã„ã„ã­ | ç·åˆã‚¹ã‚³ã‚¢ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
                lines.append("|------|--------|-----------|------------|")
                for i, d in enumerate(self_eng[:5], 1):
                    text_preview = d["text"].replace("|", "ï½œ").replace("\n", " ")[:40]
                    lines.append(f"| {i} | {d['likes']:,} | {d['total']:,} | {text_preview} |")
                lines.append("")

    return "\n".join(lines)


# === åˆ†æ3: ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢æ¤œè¨¼ ===

def generate_buzz_score_section(df_buzz, df_self):
    """ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢æ¤œè¨¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
    buzz_result = analyze_buzz_scores(df_buzz)
    lines = []

    lines.append("## 3. ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢æ¤œè¨¼")
    lines.append("")
    lines.append("> ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆ0-100ç‚¹ï¼‰: å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³(20) + ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–(15) + ã‚«ãƒ†ã‚´ãƒª(15) + æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼(10) + CTA(10) + ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§(10) + çµµæ–‡å­—(10) + èª­ã¿ã‚„ã™ã•(10)")
    lines.append("")

    # 3.1 ã‚¹ã‚³ã‚¢åˆ†å¸ƒã¨ç›¸é–¢
    lines.append("### 3.1 ã‚¹ã‚³ã‚¢åˆ†å¸ƒã¨å®Ÿã„ã„ã­æ•°ã®ç›¸é–¢")
    lines.append("")
    lines.append(f"- **ç›¸é–¢ä¿‚æ•°:** {buzz_result['correlation']:.3f}")
    if buzz_result["correlation"] > 0.5:
        lines.append("  â†’ å¼·ã„æ­£ã®ç›¸é–¢ã€‚ã‚¹ã‚³ã‚¢ã¯ãƒã‚ºäºˆæ¸¬ã«ã‹ãªã‚Šæœ‰åŠ¹")
    elif buzz_result["correlation"] > 0.3:
        lines.append("  â†’ ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢ã€‚ã‚¹ã‚³ã‚¢ã¯å‚è€ƒã«ãªã‚‹ãŒæ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
    elif buzz_result["correlation"] > 0.1:
        lines.append("  â†’ å¼±ã„æ­£ã®ç›¸é–¢ã€‚ã‚¹ã‚³ã‚¢ã®äºˆæ¸¬åŠ›ã¯é™å®šçš„")
    elif buzz_result["correlation"] > -0.1:
        lines.append("  â†’ ã»ã¼ç„¡ç›¸é–¢ã€‚ã‚¹ã‚³ã‚¢ã¨ã„ã„ã­æ•°ã«é–¢é€£æ€§ãŒè–„ã„")
    else:
        lines.append("  â†’ è² ã®ç›¸é–¢ã€‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©ã„ã„ã­ãŒå°‘ãªã„é€†è»¢ç¾è±¡")
    lines.append("")

    scores = buzz_result["scores"]
    if scores:
        avg_score = sum(s["score"] for s in scores) / len(scores)
        max_score = max(s["score"] for s in scores)
        min_score = min(s["score"] for s in scores)
        lines.append(f"- **å¹³å‡ã‚¹ã‚³ã‚¢:** {avg_score:.1f}ç‚¹ / **æœ€é«˜:** {max_score}ç‚¹ / **æœ€ä½:** {min_score}ç‚¹")
        lines.append("")

    # 3.2 ã‚¹ã‚³ã‚¢å¸¯åˆ¥å¹³å‡ã„ã„ã­æ•°
    lines.append("### 3.2 ã‚¹ã‚³ã‚¢å¸¯åˆ¥ã®å¹³å‡ã„ã„ã­æ•°")
    lines.append("")
    lines.append("| ã‚¹ã‚³ã‚¢å¸¯ | ä»¶æ•° | å¹³å‡ã„ã„ã­ | ä¸­å¤®å€¤ã„ã„ã­ |")
    lines.append("|---------|------|----------|------------|")
    for bucket_name in ["80-100ç‚¹", "60-79ç‚¹", "40-59ç‚¹", "0-39ç‚¹"]:
        likes_list = buzz_result["score_buckets"].get(bucket_name, [])
        if likes_list:
            avg_l = sum(likes_list) / len(likes_list)
            med_l = sorted(likes_list)[len(likes_list) // 2]
            lines.append(f"| {bucket_name} | {len(likes_list)} | {avg_l:.0f} | {med_l:,} |")
        else:
            lines.append(f"| {bucket_name} | 0 | - | - |")
    lines.append("")

    # 3.3 ä¹–é›¢æŠ•ç¨¿åˆ†æ
    lines.append("### 3.3 ä¹–é›¢æŠ•ç¨¿åˆ†æ")
    lines.append("")

    if scores:
        # ã‚¹ã‚³ã‚¢ã¨ã„ã„ã­æ•°ã®ä¸­å¤®å€¤ã‚’åŸºæº–ã«ä¹–é›¢ã‚’è¨ˆç®—
        median_likes = sorted([s["likes"] for s in scores])[len(scores) // 2]
        median_score_val = sorted([s["score"] for s in scores])[len(scores) // 2]

        # é«˜ã‚¹ã‚³ã‚¢Ã—ä½ã„ã„ã­ï¼ˆã‚¹ã‚³ã‚¢ãŒä¸Šä½25%ãªã®ã«ã€ã„ã„ã­ãŒä¸‹ä½50%ï¼‰
        score_75 = sorted([s["score"] for s in scores])[int(len(scores) * 0.75)]
        high_score_low_likes = [s for s in scores if s["score"] >= score_75 and s["likes"] <= median_likes]
        high_score_low_likes.sort(key=lambda x: x["likes"])

        lines.append("**é«˜ã‚¹ã‚³ã‚¢ãªã®ã«ãƒã‚ºã£ã¦ãªã„æŠ•ç¨¿:**")
        lines.append(f"ï¼ˆã‚¹ã‚³ã‚¢{score_75}ç‚¹ä»¥ä¸Š & ã„ã„ã­{median_likes}ä»¥ä¸‹ï¼‰")
        lines.append("")
        if high_score_low_likes:
            lines.append("| ã‚¹ã‚³ã‚¢ | ã„ã„ã­ | è¦å› å†…è¨³ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
            lines.append("|--------|--------|---------|------------|")
            for s in high_score_low_likes[:5]:
                factors_str = " / ".join(f"{k}:{v}" for k, v in s["factors"].items() if v > 0)
                text_preview = s["text"].replace("|", "ï½œ").replace("\n", " ")[:35]
                lines.append(f"| {s['score']} | {s['likes']:,} | {factors_str} | {text_preview} |")
            lines.append("")
            lines.append("**ãªãœãƒã‚ºã‚‰ãªã‹ã£ãŸï¼Ÿè€ƒãˆã‚‰ã‚Œã‚‹è¦å› :**")
            for s in high_score_low_likes[:3]:
                text_preview = s["text"].replace("\n", " ")[:40]
                weak_factors = [k for k, v in s["factors"].items() if v <= 3]
                lines.append(f"- ã€Œ{text_preview}...ã€â†’ å¼±ã„è¦ç´ : {', '.join(weak_factors) if weak_factors else 'ã‚¹ã‚³ã‚¢ä¸Šã¯å•é¡Œãªã—ï¼ˆã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚„ç«¶åˆã®å½±éŸ¿ã‹ï¼‰'}")
            lines.append("")
        else:
            lines.append("è©²å½“ã™ã‚‹æŠ•ç¨¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            lines.append("")

        # ä½ã‚¹ã‚³ã‚¢Ã—é«˜ã„ã„ã­
        score_25 = sorted([s["score"] for s in scores])[int(len(scores) * 0.25)]
        low_score_high_likes = [s for s in scores if s["score"] <= score_25 and s["likes"] >= median_likes]
        low_score_high_likes.sort(key=lambda x: x["likes"], reverse=True)

        lines.append("**ä½ã‚¹ã‚³ã‚¢ãªã®ã«ãƒã‚ºã£ãŸæŠ•ç¨¿:**")
        lines.append(f"ï¼ˆã‚¹ã‚³ã‚¢{score_25}ç‚¹ä»¥ä¸‹ & ã„ã„ã­{median_likes}ä»¥ä¸Šï¼‰")
        lines.append("")
        if low_score_high_likes:
            lines.append("| ã‚¹ã‚³ã‚¢ | ã„ã„ã­ | è¦å› å†…è¨³ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
            lines.append("|--------|--------|---------|------------|")
            for s in low_score_high_likes[:5]:
                factors_str = " / ".join(f"{k}:{v}" for k, v in s["factors"].items() if v > 0)
                text_preview = s["text"].replace("|", "ï½œ").replace("\n", " ")[:35]
                lines.append(f"| {s['score']} | {s['likes']:,} | {factors_str} | {text_preview} |")
            lines.append("")
            lines.append("**ãªãœãƒã‚ºã£ãŸï¼Ÿã‚¹ã‚³ã‚¢ãŒæ‰ãˆã‚‰ã‚Œã¦ã„ãªã„è¦å› :**")
            for s in low_score_high_likes[:3]:
                text_preview = s["text"].replace("\n", " ")[:40]
                lines.append(f"- ã€Œ{text_preview}...ã€â†’ ã‚¹ã‚³ã‚¢ã«å«ã¾ã‚Œãªã„è¦å› ï¼ˆè©±é¡Œæ€§ã€ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åŠ¹æœã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°ç­‰ï¼‰ãŒå¯„ä¸")
            lines.append("")
        else:
            lines.append("è©²å½“ã™ã‚‹æŠ•ç¨¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            lines.append("")

    # 3.4 è¦ç´ åˆ¥å¯„ä¸åº¦
    lines.append("### 3.4 è¦ç´ åˆ¥å¯„ä¸åº¦")
    lines.append("")
    lines.append("| è¦ç´  | é…ç‚¹ | å¹³å‡ã‚¹ã‚³ã‚¢ | å……è¶³ç‡ |")
    lines.append("|------|------|----------|--------|")
    max_points = {
        "å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³": 20, "ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–": 15, "ã‚«ãƒ†ã‚´ãƒª": 15,
        "æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼": 10, "CTA": 10, "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§": 10,
        "çµµæ–‡å­—ãƒ»æ›¸å¼": 10, "èª­ã¿ã‚„ã™ã•": 10,
    }
    for factor, avg_val in sorted(buzz_result["factor_avg"].items(),
                                   key=lambda x: x[1] / max_points.get(x[0], 10), reverse=True):
        max_pt = max_points.get(factor, 10)
        fill_rate = (avg_val / max_pt) * 100 if max_pt > 0 else 0
        lines.append(f"| {factor} | {max_pt}ç‚¹ | {avg_val:.1f} | {fill_rate:.0f}% |")
    lines.append("")

    # 3.5 @Mr_botenã®æŠ•ç¨¿ã‚¹ã‚³ã‚¢
    lines.append("### 3.5 @Mr_botenã®æŠ•ç¨¿ã‚¹ã‚³ã‚¢")
    lines.append("")

    self_scores = []
    for _, row in df_self.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        result = calculate_buzz_score(text)
        self_scores.append({
            "text": text[:50], "score": result["total_score"],
            "likes": likes, "factors": result["factors"],
        })
    self_scores.sort(key=lambda x: x["score"], reverse=True)

    if self_scores:
        avg_self_score = sum(s["score"] for s in self_scores) / len(self_scores)
        avg_buzz_score_val = sum(s["score"] for s in scores) / len(scores) if scores else 0
        lines.append(f"- **è‡ªåˆ†ã®å¹³å‡ã‚¹ã‚³ã‚¢:** {avg_self_score:.1f}ç‚¹ï¼ˆãƒã‚ºæŠ•ç¨¿å¹³å‡: {avg_buzz_score_val:.1f}ç‚¹ï¼‰")
        diff = avg_self_score - avg_buzz_score_val
        lines.append(f"- **å·®åˆ†:** {diff:+.1f}ç‚¹")
        lines.append("")

        # è‡ªåˆ†ã®TOP5
        lines.append("**è‡ªåˆ†ã®æŠ•ç¨¿ã‚¹ã‚³ã‚¢ TOP5:**")
        lines.append("")
        lines.append("| é †ä½ | ã‚¹ã‚³ã‚¢ | ã„ã„ã­ | è¦å› å†…è¨³ | æœ¬æ–‡ï¼ˆå†’é ­ï¼‰ |")
        lines.append("|------|--------|--------|---------|------------|")
        for i, s in enumerate(self_scores[:5], 1):
            factors_str = " / ".join(f"{k}:{v}" for k, v in s["factors"].items() if v > 0)
            text_preview = s["text"].replace("|", "ï½œ").replace("\n", " ")[:30]
            lines.append(f"| {i} | {s['score']} | {s['likes']:,} | {factors_str} | {text_preview} |")
        lines.append("")

        # è¦ç´ åˆ¥ã®è‡ªåˆ† vs ãƒã‚ºæŠ•ç¨¿æ¯”è¼ƒ
        lines.append("**è¦ç´ åˆ¥æ¯”è¼ƒï¼ˆè‡ªåˆ† vs ãƒã‚ºæŠ•ç¨¿ï¼‰:**")
        lines.append("")
        lines.append("| è¦ç´  | è‡ªåˆ†ã®å¹³å‡ | ãƒã‚ºæŠ•ç¨¿å¹³å‡ | å·®åˆ† |")
        lines.append("|------|----------|------------|------|")
        self_factor_avg = defaultdict(list)
        for s in self_scores:
            for k, v in s["factors"].items():
                self_factor_avg[k].append(v)
        for factor in max_points:
            self_avg = sum(self_factor_avg[factor]) / len(self_factor_avg[factor]) if self_factor_avg[factor] else 0
            buzz_avg = buzz_result["factor_avg"].get(factor, 0)
            diff_val = self_avg - buzz_avg
            lines.append(f"| {factor} | {self_avg:.1f} | {buzz_avg:.1f} | {diff_val:+.1f} |")
        lines.append("")
    else:
        lines.append("è‡ªåˆ†ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        lines.append("")

    # 3.6 ã‚¹ã‚³ã‚¢ã®ç²¾åº¦ã¨æ”¹å–„ææ¡ˆ
    lines.append("### 3.6 ã‚¹ã‚³ã‚¢ã®ç²¾åº¦ã¨æ”¹å–„ææ¡ˆ")
    lines.append("")

    corr = buzz_result["correlation"]
    lines.append(f"**ç¾åœ¨ã®ç²¾åº¦:** ç›¸é–¢ä¿‚æ•° {corr:.3f}")
    lines.append("")

    lines.append("**ã‚¹ã‚³ã‚¢ãŒæ‰ãˆã‚‰ã‚Œã¦ã„ãªã„è¦å› ï¼ˆæ”¹å–„ææ¡ˆï¼‰:**")
    lines.append("")
    lines.append("1. **æŠ•ç¨¿ã‚¿ã‚¤ãƒŸãƒ³ã‚°**: 18-21æ™‚ã®ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¿ã‚¤ãƒ æŠ•ç¨¿ã¯ãƒã‚ºã‚Šã‚„ã™ã„ãŒã€ç¾åœ¨ã®ã‚¹ã‚³ã‚¢ã«æ™‚é–“å¸¯ã¯æœªåæ˜ ")
    lines.append("2. **ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå½±éŸ¿åŠ›**: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ¨©å¨æ€§ãŒãƒã‚ºã«å¤§ããå½±éŸ¿ã™ã‚‹ãŒæœªè€ƒæ…®")
    lines.append("3. **è©±é¡Œæ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰**: æ—¬ã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆAIæ–°ãƒ„ãƒ¼ãƒ«ç™ºè¡¨ç›´å¾Œãªã©ï¼‰ã«ä¹—ã£ãŸæŠ•ç¨¿ã¯ã‚¹ã‚³ã‚¢ä»¥ä¸Šã«ãƒã‚ºã‚‹")
    lines.append("4. **ç§˜åŒ¿æ„Ÿãƒ»å‘Šç™½ãƒ•ãƒ¬ãƒ¼ã‚º**: ã€Œæ­£ç›´ã«è¨€ã†ã¨ã€ã€Œã¶ã£ã¡ã‚ƒã‘ã€ç­‰ã®è‡ªå·±é–‹ç¤ºãƒ•ãƒ¬ãƒ¼ã‚ºã®é‡ã¿ãŒè»½ã„å¯èƒ½æ€§")

    # ä¹–é›¢æŠ•ç¨¿ã®åˆ†æã‹ã‚‰è¿½åŠ ã®æ”¹å–„ç‚¹ã‚’æŠ½å‡º
    if scores:
        high_score_low_likes_posts = [s for s in scores if s["score"] >= score_75 and s["likes"] <= median_likes]
        if high_score_low_likes_posts:
            # é«˜ã‚¹ã‚³ã‚¢ä½ã„ã„ã­ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            weak_in_high = defaultdict(int)
            for s in high_score_low_likes_posts:
                for k, v in s["factors"].items():
                    if v <= max_points.get(k, 10) * 0.3:
                        weak_in_high[k] += 1
            if weak_in_high:
                most_common_weak = max(weak_in_high.items(), key=lambda x: x[1])
                lines.append(f"5. **{most_common_weak[0]}ã®é‡ã¿èª¿æ•´**: é«˜ã‚¹ã‚³ã‚¢ä½ãƒã‚ºæŠ•ç¨¿ã®{most_common_weak[1]}ä»¶ä¸­ã§å¼±ã„è¦ç´  â†’ é…ç‚¹ãŒéå¤§ãªå¯èƒ½æ€§")

    lines.append("")

    return "\n".join(lines)


# === ç·åˆã¾ã¨ã‚ ===

def generate_summary(df_buzz, df_self, viral_result, fnorm_result, buzz_result):
    """ç·åˆã¾ã¨ã‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
    lines = []
    lines.append("## 4. ç·åˆã¾ã¨ã‚ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ")
    lines.append("")

    lines.append("### ç™ºè¦‹ã•ã‚ŒãŸä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
    lines.append("")

    # ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°ã‹ã‚‰ã®ç™ºè¦‹
    lines.append("**ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°åˆ†æã‹ã‚‰:**")
    top_cat = max(viral_result["cat_viral"].items(), key=lambda x: x[1]) if viral_result["cat_viral"] else ("N/A", 0)
    lines.append(f"- ã‚·ã‚§ã‚¢ã•ã‚Œã‚„ã™ã„ã‚«ãƒ†ã‚´ãƒªã¯ã€Œ{top_cat[0]}ã€ï¼ˆãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°: {top_cat[1]:.3f}ï¼‰")
    lines.append(f"- ãƒã‚ºæŠ•ç¨¿ã®å¹³å‡ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°: {viral_result['avg_coeff']:.3f}")
    lines.append("")

    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‹ã‚‰ã®ç™ºè¦‹
    lines.append("**ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‹ã‚‰:**")
    if fnorm_result["hidden_gems"]:
        gems = fnorm_result["hidden_gems"]
        lines.append(f"- Hidden Gemsç™ºè¦‹: {len(gems)}ä»¶ï¼ˆå°‘ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã§ã‚‚é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’å®Ÿç¾ï¼‰")
        lines.append(f"  - TOP: @{gems[0]['user']}ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼{gems[0]['followers']:,} â†’ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡{gems[0]['rate']:.2f}%ï¼‰")
    lines.append("")

    # ãƒã‚ºã‚¹ã‚³ã‚¢ã‹ã‚‰ã®ç™ºè¦‹
    lines.append("**ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ã‹ã‚‰:**")
    lines.append(f"- ã‚¹ã‚³ã‚¢ã¨å®Ÿã„ã„ã­æ•°ã®ç›¸é–¢: {buzz_result['correlation']:.3f}")
    if buzz_result["factor_avg"]:
        weakest = min(buzz_result["factor_avg"].items(),
                      key=lambda x: x[1] / max(1, {"å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³": 20, "ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–": 15, "ã‚«ãƒ†ã‚´ãƒª": 15}.get(x[0], 10)))
        lines.append(f"- å…¨ä½“çš„ã«å¼±ã„è¦ç´ : {weakest[0]}ï¼ˆå¹³å‡{weakest[1]:.1f}ç‚¹ï¼‰")
    lines.append("")

    # @Mr_botenã¸ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ
    lines.append("### @Mr_botenã¸ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ")
    lines.append("")

    # è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ”¹å–„ç‚¹ã‚’ç‰¹å®š
    self_scores = []
    for _, row in df_self.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        result = calculate_buzz_score(text)
        self_scores.append(result)

    if self_scores:
        self_factor_avg = defaultdict(list)
        for s in self_scores:
            for k, v in s["factors"].items():
                self_factor_avg[k].append(v)

        max_points = {
            "å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³": 20, "ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–": 15, "ã‚«ãƒ†ã‚´ãƒª": 15,
            "æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼": 10, "CTA": 10, "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§": 10,
            "çµµæ–‡å­—ãƒ»æ›¸å¼": 10, "èª­ã¿ã‚„ã™ã•": 10,
        }

        # æ”¹å–„ä½™åœ°ãŒå¤§ãã„è¦ç´ ã‚’ç‰¹å®š
        improvements = []
        for factor, max_pt in max_points.items():
            vals = self_factor_avg.get(factor, [0])
            avg_val = sum(vals) / len(vals) if vals else 0
            gap = max_pt - avg_val
            fill = avg_val / max_pt * 100
            if fill < 50:
                improvements.append((factor, avg_val, max_pt, gap))

        improvements.sort(key=lambda x: x[3], reverse=True)

        if improvements:
            lines.append("**æ”¹å–„ã™ã¹ãè¦ç´ ï¼ˆå……è¶³ç‡50%æœªæº€ï¼‰:**")
            for factor, avg_val, max_pt, gap in improvements:
                fill = avg_val / max_pt * 100
                lines.append(f"- **{factor}**: ç¾åœ¨{avg_val:.1f}/{max_pt}ç‚¹ï¼ˆå……è¶³ç‡{fill:.0f}%ï¼‰â†’ +{gap:.0f}ç‚¹ã®ä¼¸ã³ã—ã‚")
            lines.append("")

    lines.append("**å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:**")
    lines.append("1. ã‚·ã‚§ã‚¢ã•ã‚Œã‚„ã™ã„æŠ•ç¨¿ã‚’ç‹™ã†ãªã‚‰ã€ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ„è­˜")
    lines.append("2. Hidden Gemsã®æŠ•ç¨¿ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å‚è€ƒã«ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼è¦æ¨¡ã«è¦‹åˆã£ãŸé«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸æŠ•ç¨¿ã‚’ç›®æŒ‡ã™")
    lines.append("3. ãƒã‚ºã‚¹ã‚³ã‚¢ã®å¼±ã„è¦ç´ ã‚’1ã¤ãšã¤æ”¹å–„ï¼ˆå†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ãŒåŠ¹æœå¤§ï¼‰")
    lines.append("4. æŠ•ç¨¿ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆ18-21æ™‚ï¼‰Ã— ç­‰èº«å¤§ã®å‘Šç™½ã‚¹ã‚¿ã‚¤ãƒ«ã¯å¼•ãç¶šãæœ€å¼·ã®æ­¦å™¨")
    lines.append("")

    return "\n".join(lines)


# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===

def main():
    print("=" * 60)
    print("é«˜åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if not os.path.exists(BUZZ_FILE):
        print(f"ã‚¨ãƒ©ãƒ¼: {BUZZ_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    df_buzz_raw = load_excel(BUZZ_FILE)
    if df_buzz_raw is None:
        return

    df_buzz, original_count, excluded_count = filter_data(df_buzz_raw)

    if os.path.exists(SELF_FILE):
        df_self = load_self_posts(SELF_FILE)
    else:
        print(f"æ³¨æ„: {SELF_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‡ªåˆ†ã®æŠ•ç¨¿ã¨ã®æ¯”è¼ƒã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        df_self = pd.DataFrame(columns=["æœ¬æ–‡", "ã„ã„ã­æ•°", "ãƒªãƒã‚¹ãƒˆæ•°", "ãƒªãƒ—ãƒ©ã‚¤æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ¦ãƒ¼ã‚¶ãƒ¼å"])

    print(f"\nãƒã‚ºæŠ•ç¨¿: {len(df_buzz)}ä»¶ / è‡ªåˆ†ã®æŠ•ç¨¿: {len(df_self)}ä»¶")
    print()

    # åˆ†æå®Ÿè¡Œ
    print("åˆ†æ1: ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°åˆ†æ...")
    viral_result = analyze_viral_coefficient(df_buzz)

    print("åˆ†æ2: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–...")
    fnorm_result = analyze_follower_normalized(df_buzz)

    print("åˆ†æ3: ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢æ¤œè¨¼...")
    buzz_result = analyze_buzz_scores(df_buzz)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

    report = []
    report.append("# é«˜åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: ãƒã‚¤ãƒ©ãƒ«ä¿‚æ•°ãƒ»ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ãƒ»ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢æ¤œè¨¼")
    report.append("")
    report.append(f"**åˆ†ææ—¥æ™‚:** {now}")
    report.append(f"**ãƒã‚ºæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿:** {len(df_buzz)}ä»¶ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿{original_count}ä»¶ã‹ã‚‰{excluded_count}ä»¶é™¤å¤–ï¼‰")
    report.append(f"**è‡ªåˆ†ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿:** {len(df_self)}ä»¶ï¼ˆ@Mr_botenï¼‰")
    report.append("")
    report.append("---")
    report.append("")

    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
    report.append(generate_viral_section(df_buzz, df_self))
    report.append("")
    report.append("---")
    report.append("")
    report.append(generate_follower_section(df_buzz, df_self))
    report.append("")
    report.append("---")
    report.append("")
    report.append(generate_buzz_score_section(df_buzz, df_self))
    report.append("")
    report.append("---")
    report.append("")
    report.append(generate_summary(df_buzz, df_self, viral_result, fnorm_result, buzz_result))

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    output_text = "\n".join(report)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(output_text)

    print(f"\nãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {OUTPUT_FILE}")
    print(f"æ–‡å­—æ•°: {len(output_text):,}æ–‡å­—")


if __name__ == "__main__":
    main()
