"""ãƒã‚ºãƒã‚¹ãƒˆã®è©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆv2 - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime

import pandas as pd


def load_excel(filename):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        df = pd.read_excel(filename)
        print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒã‚¹ãƒˆ")
        return df
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def filter_keywords(df):
    """ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã®ã¿é™¤å¤–ï¼ˆé‡è¤‡é™¤å»ãªã—ï¼‰"""
    exclude_keywords = [
        "è‘—ä½œæ¨©", "ç‰ˆæ¨©", "æµ·è³Šç‰ˆ", "åç›ŠåŒ–åœæ­¢", "åç›ŠåŒ–ãŒåœæ­¢",
        "å‰¥å¥ª", "ä¾µå®³", "ã‚¤ãƒ³ãƒ—ãƒ¬ã‚¾ãƒ³ãƒ“"
    ]

    def should_exclude(text):
        for keyword in exclude_keywords:
            if keyword in text:
                return True
        return False

    return df[~df["æœ¬æ–‡"].apply(should_exclude)].copy()


def filter_data(df):
    """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    original_count = len(df)

    # 1. ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã‚’é™¤å¤–
    df_filtered = filter_keywords(df)
    excluded_by_keyword = original_count - len(df_filtered)
    print(f"ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã‚’é™¤å¤–: {excluded_by_keyword}ä»¶")

    # 2. åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã¯æœ€ã‚‚ã„ã„ã­æ•°ãŒé«˜ã„1ä»¶ã®ã¿æ®‹ã™
    df_filtered = df_filtered.sort_values("ã„ã„ã­æ•°", ascending=False)
    df_filtered = df_filtered.drop_duplicates(subset=["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"], keep="first")
    excluded_by_user = len(df) - excluded_by_keyword - len(df_filtered)
    print(f"åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é‡è¤‡æŠ•ç¨¿ã‚’é™¤å¤–: {excluded_by_user}ä»¶")

    total_excluded = original_count - len(df_filtered)
    print(f"æœ€çµ‚åˆ†æå¯¾è±¡: {len(df_filtered)}ä»¶ï¼ˆ{total_excluded}ä»¶é™¤å¤–ï¼‰")

    return df_filtered, original_count, total_excluded


def safe_get(row, column, default=""):
    """å®‰å…¨ã«ã‚«ãƒ©ãƒ ã®å€¤ã‚’å–å¾—"""
    try:
        value = row.get(column, default)
        return value if pd.notna(value) else default
    except:
        return default


def analyze_line_breaks(df):
    """æ”¹è¡Œã®åˆ†æ"""
    results = []
    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        line_count = text.count("\n")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        results.append({"line_count": line_count, "likes": likes})

    df_lines = pd.DataFrame(results)
    avg_lines = df_lines["line_count"].mean()

    # ä¸Šä½25%ã¨ä¸‹ä½25%ã§æ¯”è¼ƒ
    top_25 = df.nlargest(len(df) // 4, "ã„ã„ã­æ•°")
    bottom_25 = df.nsmallest(len(df) // 4, "ã„ã„ã­æ•°")

    top_avg_lines = sum(safe_get(row, "æœ¬æ–‡", "").count("\n") for _, row in top_25.iterrows()) / len(top_25) if len(top_25) > 0 else 0
    bottom_avg_lines = sum(safe_get(row, "æœ¬æ–‡", "").count("\n") for _, row in bottom_25.iterrows()) / len(bottom_25) if len(bottom_25) > 0 else 0

    return avg_lines, top_avg_lines, bottom_avg_lines


def analyze_bullet_points(df):
    """ç®‡æ¡æ›¸ãã®åˆ†æ"""
    bullet_pattern = re.compile(r'^[ãƒ»\-\*â‘ -â“1-9]\s', re.MULTILINE)

    with_bullets = []
    without_bullets = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        if bullet_pattern.search(text):
            with_bullets.append(likes)
        else:
            without_bullets.append(likes)

    return len(with_bullets), len(without_bullets), with_bullets, without_bullets


def analyze_symbols(df):
    """è¨˜å·ã®ä½¿ç”¨åˆ†æ"""
    symbol_patterns = {
        "â†’": r"â†’",
        "ï¼": r"[ï¼=]{2,}",
        "ï½œ": r"ï½œ",
        "ã€ã€‘": r"ã€.*?ã€‘",
    }

    symbol_usage = defaultdict(int)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        for symbol, pattern in symbol_patterns.items():
            if re.search(pattern, text):
                symbol_usage[symbol] += 1

    return symbol_usage


def analyze_urls(df):
    """URLæœ‰ç„¡ã®åˆ†æ"""
    url_pattern = re.compile(r'https?://\S+')

    with_url = []
    without_url = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        if url_pattern.search(text):
            with_url.append(likes)
        else:
            without_url.append(likes)

    return len(with_url), len(without_url), with_url, without_url


def classify_opening_pattern(first_line):
    """å†’é ­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡"""
    if not first_line:
        return "ãã®ä»–"

    if re.search(r'[ï¼Ÿ?]', first_line):
        return "ç–‘å•å½¢"
    elif re.search(r'^[0-9â‘ -â“]|[0-9]+ã¤|[0-9]+å€‹|[0-9]+é¸', first_line):
        return "æ•°å­—æç¤º"
    elif re.search(r'ã¯ï¼Ÿ|ã¾ã˜ã§|ã‚„ã°ã„|æœ€æ‚ª|ã‚ã‚Šãˆãªã„', first_line, re.IGNORECASE):
        return "ç…½ã‚Š"
    elif re.search(r'ã‚ã‹ã‚‹|å…±æ„Ÿ|åŒã˜|ã‚ã‚‹ã‚ã‚‹', first_line, re.IGNORECASE):
        return "å…±æ„Ÿ"
    elif re.search(r'ã§ã™|ã¾ã™|ã§ã‚ã‚‹|ã ã€‚', first_line):
        return "æ–­å®šå½¢"
    elif re.search(r'ã¿ãªã•ã‚“|ã‚ãªãŸ|çš†ã•ã‚“', first_line, re.IGNORECASE):
        return "å‘¼ã³ã‹ã‘"
    else:
        return "ãã®ä»–"


def analyze_opening_patterns(df):
    """å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
    pattern_data = defaultdict(list)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        first_line = text.split("\n")[0] if text else ""

        pattern = classify_opening_pattern(first_line)
        pattern_data[pattern].append(likes)

    return pattern_data


def analyze_cta(df):
    """CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã®åˆ†æ"""
    cta_patterns = {
        "ã„ã„ã­ç³»": r'ã„ã„ã­|ğŸ‘|ãƒãƒ¼ãƒˆ',
        "ä¿å­˜ç³»": r'ä¿å­˜|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
        "ãƒ•ã‚©ãƒ­ãƒ¼ç³»": r'ãƒ•ã‚©ãƒ­ãƒ¼|follow',
        "ã‚·ã‚§ã‚¢ç³»": r'ãƒªãƒã‚¹ãƒˆ|RT|ã‚·ã‚§ã‚¢|æ‹¡æ•£',
        "ã‚³ãƒ¡ãƒ³ãƒˆç³»": r'ã‚³ãƒ¡ãƒ³ãƒˆ|è¿”ä¿¡|æ•™ãˆã¦',
    }

    cta_data = defaultdict(list)
    no_cta = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        has_cta = False
        for cta_type, pattern in cta_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                cta_data[cta_type].append(likes)
                has_cta = True

        if not has_cta:
            no_cta.append(likes)

    return cta_data, no_cta


def analyze_emotion(df):
    """æ„Ÿæƒ…åˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰- æ€’ã‚Šã‚«ãƒ†ã‚´ãƒªé™¤å¤–"""
    emotion_patterns = {
        "æœŸå¾…": r'ãƒãƒ£ãƒ³ã‚¹|å¯èƒ½æ€§|ç¨¼ã’ã‚‹|å„²ã‹ã‚‹|æˆåŠŸ|é”æˆ|å®Ÿç¾|ã§ãã‚‹',
        "é©šã": r'ã¾ã•ã‹|ã³ã£ãã‚Š|é©šã|ã™ã”ã„|ã‚„ã°ã„',
        "å…±æ„Ÿ": r'ã‚ã‹ã‚‹|ãã†ãã†|ã‚ã‚‹ã‚ã‚‹|åŒã˜|ç§ã‚‚',
        "ææ€–": r'å±é™º|æ€–ã„|ãƒªã‚¹ã‚¯|å¤±æ•—|æ|ãƒ¤ãƒã„|æœ€æ‚ª',
    }

    emotion_data = defaultdict(list)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        for emotion, pattern in emotion_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                emotion_data[emotion].append(likes)

    return emotion_data


def has_story(text):
    """ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®åˆ¤å®š"""
    story_keywords = [
        r'ã¾ãš|æ¬¡ã«|ãã—ã¦|æœ€å¾Œã«',
        r'before|after|â†’',
        r'æ˜”|ä»¥å‰|æœ€åˆ|ä»Šã§ã¯|ç¾åœ¨',
        r'ç§|åƒ•|è‡ªåˆ†|å®Ÿéš›ã«|ã‚„ã£ã¦ã¿ãŸ',
    ]

    for pattern in story_keywords:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


def analyze_story(df):
    """ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®åˆ†æ"""
    with_story = []
    without_story = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        if has_story(text):
            with_story.append(likes)
        else:
            without_story.append(likes)

    return len(with_story), len(without_story), with_story, without_story


def analyze_engagement_ratio(df):
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡ã®åˆ†æ"""
    high_reply_ratio = []  # ãƒªãƒ—ãƒ©ã‚¤ãŒå¤šã„
    high_retweet_ratio = []  # ãƒªãƒã‚¹ãƒˆãŒå¤šã„

    for _, row in df.iterrows():
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        text = safe_get(row, "æœ¬æ–‡", "")

        if likes > 0:
            reply_ratio = replies / likes
            retweet_ratio = retweets / likes

            if reply_ratio > 0.05:  # ãƒªãƒ—ãƒ©ã‚¤ç‡5%ä»¥ä¸Š
                high_reply_ratio.append({"text": text[:50], "likes": likes, "replies": replies})

            if retweet_ratio > 0.2:  # ãƒªãƒã‚¹ãƒˆç‡20%ä»¥ä¸Š
                high_retweet_ratio.append({"text": text[:50], "likes": likes, "retweets": retweets})

    return high_reply_ratio, high_retweet_ratio


def classify_category(text):
    """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰"""
    # å„ªå…ˆé †ä½ã‚’æŒãŸã›ã¦åˆ¤å®š

    # å®Ÿç¸¾å ±å‘Šç³»ï¼ˆæœ€å„ªå…ˆï¼‰
    if re.search(r'é”æˆ|åç›Š|ç¨¼ã’ãŸ|ç¨¼ã„ã |æˆåŠŸ|å®Ÿç¸¾|å„²ã‹ã£ãŸ|ã€œä¸‡å††|æœˆå|å¹´å|å£²ä¸Š|å ±é…¬|åˆ©ç›Š', text, re.IGNORECASE):
        return "å®Ÿç¸¾å ±å‘Šç³»"

    # ãƒã‚¦ãƒã‚¦ç³»
    if re.search(r'æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—|ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯|æ”»ç•¥|ãƒãƒ‹ãƒ¥ã‚¢ãƒ«|ã‚¬ã‚¤ãƒ‰|ã€œã™ã‚‹æ–¹æ³•|ã€œã®ã‚„ã‚Šæ–¹', text, re.IGNORECASE):
        return "ãƒã‚¦ãƒã‚¦ç³»"

    # ä½“é¨“è«‡ç³»
    if re.search(r'ç§ãŒ|åƒ•ãŒ|è‡ªåˆ†ãŒ|å®Ÿéš›ã«|ã‚„ã£ã¦ã¿ãŸ|è©¦ã—ã¦ã¿ãŸ|ä½“é¨“|çµŒé¨“|ã€œã—ãŸã‚‰|ã€œã—ã¦ã¿ãŸ', text, re.IGNORECASE):
        return "ä½“é¨“è«‡ç³»"

    # å•é¡Œæèµ·ç³»
    if re.search(r'ã¯ï¼Ÿ|å•é¡Œ|å±é™º|æ³¨æ„|è­¦å‘Š|ã€æ‚²å ±ã€‘|ã€œã™ãã‚‹|ãƒ¤ãƒã„|ãŠã‹ã—ã„', text, re.IGNORECASE):
        return "å•é¡Œæèµ·ç³»"

    # ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»
    if re.search(r'ãƒ„ãƒ¼ãƒ«|ã‚¢ãƒ—ãƒª|ã‚µãƒ¼ãƒ“ã‚¹|ãƒ—ãƒ©ã‚°ã‚¤ãƒ³|æ‹¡å¼µæ©Ÿèƒ½|ãŠã™ã™ã‚|ç´¹ä»‹|AI|Claude|ChatGPT|GPT', text, re.IGNORECASE):
        return "ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»"

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»
    if re.search(r'ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|é–‹å§‹|é–‹å‚¬|é€Ÿå ±|æœ€æ–°|ãƒ‹ãƒ¥ãƒ¼ã‚¹|å…¬é–‹', text, re.IGNORECASE):
        return "ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»"

    return "ãã®ä»–"


def analyze_categories(df):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ"""
    category_data = defaultdict(lambda: {"likes": [], "retweets": []})

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)

        category = classify_category(text)
        category_data[category]["likes"].append(likes)
        category_data[category]["retweets"].append(retweets)

    return category_data


def parse_datetime(dt_str):
    """æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹"""
    try:
        return pd.to_datetime(dt_str)
    except:
        return None


def analyze_time(df):
    """æ™‚é–“å¸¯åˆ†æ"""
    time_slots = {
        "æœ(6-9æ™‚)": [],
        "æ˜¼(9-12æ™‚)": [],
        "åˆå¾Œ(12-18æ™‚)": [],
        "å¤œ(18-22æ™‚)": [],
        "æ·±å¤œ(22-6æ™‚)": [],
    }

    weekday_data = defaultdict(list)

    for _, row in df.iterrows():
        dt_str = safe_get(row, "æŠ•ç¨¿æ—¥æ™‚", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        dt = parse_datetime(dt_str)
        if dt:
            # JSTå¤‰æ›ï¼ˆ+9æ™‚é–“ï¼‰
            hour = (dt.hour + 9) % 24
            weekday = dt.weekday()

            if 6 <= hour < 9:
                time_slots["æœ(6-9æ™‚)"].append(likes)
            elif 9 <= hour < 12:
                time_slots["æ˜¼(9-12æ™‚)"].append(likes)
            elif 12 <= hour < 18:
                time_slots["åˆå¾Œ(12-18æ™‚)"].append(likes)
            elif 18 <= hour < 22:
                time_slots["å¤œ(18-22æ™‚)"].append(likes)
            else:
                time_slots["æ·±å¤œ(22-6æ™‚)"].append(likes)

            weekday_names = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
            weekday_data[weekday_names[weekday]].append(likes)

    return time_slots, weekday_data


def analyze_time_category_cross(df):
    """æ™‚é–“å¸¯Ã—ã‚«ãƒ†ã‚´ãƒªã®ã‚¯ãƒ­ã‚¹åˆ†æ"""
    cross_data = defaultdict(lambda: defaultdict(list))

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        dt_str = safe_get(row, "æŠ•ç¨¿æ—¥æ™‚", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        category = classify_category(text)
        dt = parse_datetime(dt_str)

        if dt:
            hour = (dt.hour + 9) % 24
            if 6 <= hour < 12:
                time_slot = "æœã€œæ˜¼"
            elif 12 <= hour < 18:
                time_slot = "åˆå¾Œ"
            elif 18 <= hour < 22:
                time_slot = "å¤œ"
            else:
                time_slot = "æ·±å¤œ"

            cross_data[time_slot][category].append(likes)

    return cross_data


# === æ–°è¦åˆ†æé–¢æ•°: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ– ===

def analyze_follower_normalized(df):
    """ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ"""
    results = {
        "has_follower_data": False,
        "top10": [],
        "hidden_gems": [],
        "pattern_by_rate": {},
        "category_by_rate": {},
    }

    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’æ•°å€¤ã«å¤‰æ›
    df_work = df.copy()
    df_work["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"] = pd.to_numeric(df_work["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"], errors="coerce").fillna(0).astype(int)

    valid_followers = df_work[df_work["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"] > 0]

    if len(valid_followers) >= 5:
        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
        results["has_follower_data"] = True
        valid_followers = valid_followers.copy()
        valid_followers["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"] = (valid_followers["ã„ã„ã­æ•°"] / valid_followers["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"]) * 100

        # TOP10
        top10 = valid_followers.nlargest(10, "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡")
        for _, row in top10.iterrows():
            results["top10"].append({
                "user": row["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"],
                "likes": int(row["ã„ã„ã­æ•°"]),
                "followers": int(row["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"]),
                "rate": float(row["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"]),
                "text": str(row["æœ¬æ–‡"])[:60],
            })

        # Hidden Gems: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãŒä¸­å¤®å€¤ä»¥ä¸‹ & ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãŒ75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Š
        median_followers = valid_followers["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"].median()
        rate_75 = valid_followers["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"].quantile(0.75)
        gems = valid_followers[
            (valid_followers["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"] <= median_followers) &
            (valid_followers["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"] >= rate_75)
        ].nlargest(5, "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡")
        for _, row in gems.iterrows():
            results["hidden_gems"].append({
                "user": row["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"],
                "likes": int(row["ã„ã„ã­æ•°"]),
                "followers": int(row["ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"]),
                "rate": float(row["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"]),
                "text": str(row["æœ¬æ–‡"])[:60],
            })

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡
        for _, row in valid_followers.iterrows():
            text = safe_get(row, "æœ¬æ–‡", "")
            first_line = text.split("\n")[0] if text else ""
            pattern = classify_opening_pattern(first_line)
            if pattern not in results["pattern_by_rate"]:
                results["pattern_by_rate"][pattern] = []
            results["pattern_by_rate"][pattern].append(float(row["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"]))

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡
        for _, row in valid_followers.iterrows():
            text = safe_get(row, "æœ¬æ–‡", "")
            cat = classify_category(text)
            if cat not in results["category_by_rate"]:
                results["category_by_rate"][cat] = []
            results["category_by_rate"][cat].append(float(row["ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡"]))
    else:
        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ: ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã§ä»£æ›¿
        df_work["ç·åˆã‚¹ã‚³ã‚¢"] = df_work["ã„ã„ã­æ•°"] + df_work["ãƒªãƒã‚¹ãƒˆæ•°"] * 2 + df_work["ãƒªãƒ—ãƒ©ã‚¤æ•°"] * 3
        top10 = df_work.nlargest(10, "ç·åˆã‚¹ã‚³ã‚¢")
        for _, row in top10.iterrows():
            results["top10"].append({
                "user": row["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"],
                "likes": int(row["ã„ã„ã­æ•°"]),
                "followers": 0,
                "rate": float(row["ç·åˆã‚¹ã‚³ã‚¢"]),
                "text": str(row["æœ¬æ–‡"])[:60],
            })

    return results


# === æ–°è¦åˆ†æé–¢æ•°: ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ– ===

EMOJI_PATTERN = re.compile(
    "[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF"
    "\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U0001F900-\U0001F9FF"
    "\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002600-\U000026FF"
    "\U0000FE00-\U0000FE0F\U0000200D]+",
    flags=re.UNICODE,
)


def analyze_text_length(df):
    """æ–‡å­—æ•°Ã—ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ"""
    buckets = {
        "0-50å­—": (0, 50), "51-100å­—": (51, 100), "101-150å­—": (101, 150),
        "151-200å­—": (151, 200), "201-300å­—": (201, 300),
        "301-500å­—": (301, 500), "500å­—ä»¥ä¸Š": (501, 99999),
    }

    bucket_data = {k: [] for k in buckets}
    lengths_likes = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        length = len(text)
        lengths_likes.append((length, likes))

        for bucket_name, (lo, hi) in buckets.items():
            if lo <= length <= hi:
                bucket_data[bucket_name].append(likes)
                break

    # æœ€é©ãƒ¬ãƒ³ã‚¸: å¹³å‡ã„ã„ã­ãŒæœ€ã‚‚é«˜ã„ãƒã‚±ãƒƒãƒˆ
    best_bucket = max(bucket_data.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0)

    # ç›¸é–¢ä¿‚æ•°
    if len(lengths_likes) > 2:
        df_corr = pd.DataFrame(lengths_likes, columns=["length", "likes"])
        correlation = float(df_corr["length"].corr(df_corr["likes"]))
    else:
        correlation = 0.0

    all_lengths = [x[0] for x in lengths_likes]

    return {
        "avg_length": sum(all_lengths) / len(all_lengths) if all_lengths else 0,
        "bucket_data": bucket_data,
        "best_bucket": best_bucket[0],
        "correlation": correlation,
        "lengths_likes": lengths_likes,
    }


def analyze_emoji_usage(df):
    """çµµæ–‡å­—ä½¿ç”¨åˆ†æ"""
    with_emoji = []
    without_emoji = []
    emoji_count_data = defaultdict(list)  # count -> [likes]
    emoji_counter = Counter()

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        emojis = EMOJI_PATTERN.findall(text)
        count = len(emojis)

        if count > 0:
            with_emoji.append(likes)
            for e in emojis:
                emoji_counter[e] += 1
        else:
            without_emoji.append(likes)

        # å€‹æ•°å¸¯
        if count == 0:
            emoji_count_data["0å€‹"].append(likes)
        elif count <= 2:
            emoji_count_data["1-2å€‹"].append(likes)
        elif count <= 5:
            emoji_count_data["3-5å€‹"].append(likes)
        else:
            emoji_count_data["6å€‹ä»¥ä¸Š"].append(likes)

    # äººæ°—çµµæ–‡å­— TOP5
    top_emoji = emoji_counter.most_common(5)

    return {
        "with_emoji": with_emoji,
        "without_emoji": without_emoji,
        "emoji_count_data": emoji_count_data,
        "top_emoji": top_emoji,
    }


def analyze_hashtag_usage(df):
    """ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ä½¿ç”¨åˆ†æ"""
    hashtag_pattern = re.compile(r'[#ï¼ƒ]\S+')
    with_hashtag = []
    without_hashtag = []
    hashtag_counter = Counter()
    count_data = defaultdict(list)  # tag_count -> [likes]

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        tags = hashtag_pattern.findall(text)
        tag_count = len(tags)

        if tag_count > 0:
            with_hashtag.append(likes)
            for tag in tags:
                hashtag_counter[tag] += 1
        else:
            without_hashtag.append(likes)

        if tag_count == 0:
            count_data["0å€‹"].append(likes)
        elif tag_count <= 2:
            count_data["1-2å€‹"].append(likes)
        else:
            count_data["3å€‹ä»¥ä¸Š"].append(likes)

    return {
        "with_hashtag": with_hashtag,
        "without_hashtag": without_hashtag,
        "top_hashtags": hashtag_counter.most_common(10),
        "count_data": count_data,
    }


# === æ–°è¦åˆ†æé–¢æ•°: ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ ===

def calculate_buzz_score(text, score_params=None):
    """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-100ç‚¹ï¼‰"""
    if score_params is None:
        score_params = {}

    factors = {}
    total = 0

    # 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ (20ç‚¹)
    first_line = text.split("\n")[0] if text else ""
    pattern = classify_opening_pattern(first_line)
    pattern_scores = {"æ•°å­—æç¤º": 20, "ç–‘å•å½¢": 16, "ç…½ã‚Š": 14, "å…±æ„Ÿ": 14, "å‘¼ã³ã‹ã‘": 12, "æ–­å®šå½¢": 8, "ãã®ä»–": 5}
    s = pattern_scores.get(pattern, 5)
    factors["å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³"] = s
    total += s

    # 2. ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ– (15ç‚¹)
    length = len(text)
    optimal_min = score_params.get("optimal_min", 100)
    optimal_max = score_params.get("optimal_max", 300)
    if optimal_min <= length <= optimal_max:
        s = 15
    elif length < optimal_min:
        s = max(3, int(15 * length / optimal_min))
    else:
        s = max(3, int(15 * optimal_max / length))
    factors["ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–"] = s
    total += s

    # 3. ã‚«ãƒ†ã‚´ãƒª (15ç‚¹)
    category = classify_category(text)
    cat_scores = score_params.get("cat_scores", {
        "å®Ÿç¸¾å ±å‘Šç³»": 15, "ãƒã‚¦ãƒã‚¦ç³»": 13, "å•é¡Œæèµ·ç³»": 12,
        "ä½“é¨“è«‡ç³»": 11, "ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»": 10, "ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»": 8, "ãã®ä»–": 5,
    })
    s = cat_scores.get(category, 5)
    factors["ã‚«ãƒ†ã‚´ãƒª"] = s
    total += s

    # 4. æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ (10ç‚¹)
    emotion_patterns = {
        "æœŸå¾…": r'ãƒãƒ£ãƒ³ã‚¹|å¯èƒ½æ€§|ç¨¼ã’ã‚‹|å„²ã‹ã‚‹|æˆåŠŸ|é”æˆ|å®Ÿç¾|ã§ãã‚‹',
        "é©šã": r'ã¾ã•ã‹|ã³ã£ãã‚Š|é©šã|ã™ã”ã„|ã‚„ã°ã„',
        "å…±æ„Ÿ": r'ã‚ã‹ã‚‹|ãã†ãã†|ã‚ã‚‹ã‚ã‚‹|åŒã˜|ç§ã‚‚',
        "ææ€–": r'å±é™º|æ€–ã„|ãƒªã‚¹ã‚¯|å¤±æ•—|æ|ãƒ¤ãƒã„|æœ€æ‚ª',
    }
    emotion_count = sum(1 for pat in emotion_patterns.values() if re.search(pat, text, re.IGNORECASE))
    s = min(10, emotion_count * 4)
    factors["æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼"] = s
    total += s

    # 5. CTA (10ç‚¹)
    cta_patterns = [r'ã„ã„ã­|ğŸ‘', r'ä¿å­˜|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯', r'ãƒ•ã‚©ãƒ­ãƒ¼', r'ãƒªãƒã‚¹ãƒˆ|RT|ã‚·ã‚§ã‚¢|æ‹¡æ•£', r'ã‚³ãƒ¡ãƒ³ãƒˆ|è¿”ä¿¡|æ•™ãˆã¦']
    has_cta = any(re.search(p, text, re.IGNORECASE) for p in cta_patterns)
    s = 10 if has_cta else 0
    factors["CTA"] = s
    total += s

    # 6. ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ (10ç‚¹)
    s = 10 if has_story(text) else 0
    factors["ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§"] = s
    total += s

    # 7. çµµæ–‡å­—ãƒ»æ›¸å¼ (10ç‚¹)
    emojis = EMOJI_PATTERN.findall(text)
    emoji_count = len(emojis)
    if 1 <= emoji_count <= 3:
        s = 10
    elif emoji_count == 0:
        s = 4
    else:
        s = 6
    factors["çµµæ–‡å­—ãƒ»æ›¸å¼"] = s
    total += s

    # 8. èª­ã¿ã‚„ã™ã• (10ç‚¹)
    line_breaks = text.count("\n")
    bullet_pattern = re.compile(r'^[ãƒ»\-\*â‘ -â“1-9]\s', re.MULTILINE)
    has_bullets = bool(bullet_pattern.search(text))
    s = 0
    if 3 <= line_breaks <= 10:
        s += 5
    elif line_breaks > 0:
        s += 3
    if has_bullets:
        s += 5
    factors["èª­ã¿ã‚„ã™ã•"] = s
    total += s

    return {"total_score": total, "factors": factors}


def analyze_buzz_scores(df, score_params=None):
    """å…¨æŠ•ç¨¿ã®ãƒã‚ºã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—åˆ†æ"""
    scores = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        result = calculate_buzz_score(text, score_params)
        scores.append({
            "text": text[:60],
            "score": result["total_score"],
            "likes": likes,
            "factors": result["factors"],
        })

    # ã‚¹ã‚³ã‚¢ã¨å®Ÿã„ã„ã­æ•°ã®ç›¸é–¢
    if len(scores) > 2:
        df_scores = pd.DataFrame(scores)
        correlation = float(df_scores["score"].corr(df_scores["likes"]))
    else:
        correlation = 0.0

    # ã‚¹ã‚³ã‚¢å¸¯åˆ¥ã®å¹³å‡ã„ã„ã­æ•°
    score_buckets = {"80-100ç‚¹": [], "60-79ç‚¹": [], "40-59ç‚¹": [], "0-39ç‚¹": []}
    for s in scores:
        sc = s["score"]
        if sc >= 80:
            score_buckets["80-100ç‚¹"].append(s["likes"])
        elif sc >= 60:
            score_buckets["60-79ç‚¹"].append(s["likes"])
        elif sc >= 40:
            score_buckets["40-59ç‚¹"].append(s["likes"])
        else:
            score_buckets["0-39ç‚¹"].append(s["likes"])

    # è¦ç´ åˆ¥ã®å¹³å‡å¯„ä¸åº¦
    factor_totals = defaultdict(list)
    for s in scores:
        for factor, value in s["factors"].items():
            factor_totals[factor].append(value)

    factor_avg = {k: sum(v) / len(v) if v else 0 for k, v in factor_totals.items()}

    return {
        "scores": sorted(scores, key=lambda x: x["score"], reverse=True),
        "correlation": correlation,
        "score_buckets": score_buckets,
        "factor_avg": factor_avg,
    }


# === æ–°è¦åˆ†æé–¢æ•°: ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ ===

def analyze_users(df_raw, df_filtered):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æï¼ˆé‡è¤‡é™¤å»å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰"""
    user_stats = []
    grouped = df_raw.groupby("ãƒ¦ãƒ¼ã‚¶ãƒ¼å")

    for user, group in grouped:
        likes_list = group["ã„ã„ã­æ•°"].tolist()
        user_stats.append({
            "user": user,
            "post_count": len(group),
            "avg_likes": sum(likes_list) / len(likes_list) if likes_list else 0,
            "max_likes": max(likes_list) if likes_list else 0,
            "total_likes": sum(likes_list),
            "std_likes": pd.Series(likes_list).std() if len(likes_list) > 1 else 0,
        })

    user_stats.sort(key=lambda x: x["total_likes"], reverse=True)

    # ãƒªãƒ”ãƒ¼ãƒˆãƒã‚ºãƒ¦ãƒ¼ã‚¶ãƒ¼
    repeat_buzzers = [u for u in user_stats if u["post_count"] >= 2]

    # æŠ•ç¨¿é »åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚
    freq_data = defaultdict(list)
    for u in user_stats:
        count = u["post_count"]
        if count == 1:
            freq_data["1ä»¶"].append(u["avg_likes"])
        elif count == 2:
            freq_data["2ä»¶"].append(u["avg_likes"])
        else:
            freq_data["3ä»¶ä»¥ä¸Š"].append(u["avg_likes"])

    # å¸¸é€£ãƒã‚ºã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å…±é€šç‰¹å¾´
    common_traits = {"categories": Counter(), "openings": Counter(), "cta_count": 0, "total": 0, "text_lengths": []}
    for user in repeat_buzzers[:10]:
        user_posts = df_raw[df_raw["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"] == user["user"]]
        for _, row in user_posts.iterrows():
            text = safe_get(row, "æœ¬æ–‡", "")
            common_traits["categories"][classify_category(text)] += 1
            first_line = text.split("\n")[0] if text else ""
            common_traits["openings"][classify_opening_pattern(first_line)] += 1
            common_traits["text_lengths"].append(len(text))
            common_traits["total"] += 1
            cta_pats = [r'ã„ã„ã­|ğŸ‘', r'ä¿å­˜|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯', r'ãƒ•ã‚©ãƒ­ãƒ¼', r'ãƒªãƒã‚¹ãƒˆ|RT|ã‚·ã‚§ã‚¢|æ‹¡æ•£', r'ã‚³ãƒ¡ãƒ³ãƒˆ|è¿”ä¿¡|æ•™ãˆã¦']
            if any(re.search(p, text, re.IGNORECASE) for p in cta_pats):
                common_traits["cta_count"] += 1

    return {
        "user_stats": user_stats,
        "repeat_buzzers": repeat_buzzers,
        "freq_data": freq_data,
        "common_traits": common_traits,
    }


def generate_report(df, output_filename, original_count, excluded_count, df_raw=None):
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print("\nåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")

    with open(output_filename, "w", encoding="utf-8") as f:
        f.write("# AIå‰¯æ¥­ç³»ãƒã‚ºãƒã‚¹ãƒˆ è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ v2\n\n")
        f.write(f"**åˆ†ææ—¥æ™‚:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n\n")
        f.write(f"**å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°:** {original_count}ä»¶\n\n")
        f.write(f"**é™¤å¤–ä»¶æ•°:** {excluded_count}ä»¶\n")
        f.write(f"- ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã®æŠ•ç¨¿ã‚’é™¤å¤–\n")
        f.write(f"- åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é‡è¤‡æŠ•ç¨¿ã‚’é™¤å¤–ï¼ˆæœ€ã‚‚ã„ã„ã­æ•°ãŒé«˜ã„1ä»¶ã®ã¿æ®‹ã™ï¼‰\n\n")
        f.write(f"**æœ€çµ‚åˆ†æå¯¾è±¡:** {len(df)}ä»¶ã®ãƒã‚¹ãƒˆ\n\n")
        f.write("---\n\n")

        # åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        print("åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’åˆ†æä¸­...")
        f.write("## 1. åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n")

        f.write("### ã„ã„ã­æ•° TOP10\n\n")
        top10_likes = df.nlargest(10, "ã„ã„ã­æ•°")
        for i, (_, row) in enumerate(top10_likes.iterrows(), 1):
            f.write(f"#### {i}ä½: {row['ã„ã„ã­æ•°']:,}ã„ã„ã­\n\n")
            f.write(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼:** @{row['ãƒ¦ãƒ¼ã‚¶ãƒ¼å']}\n\n")
            f.write(f"**æœ¬æ–‡:**\n```\n{row['æœ¬æ–‡'][:200]}{'...' if len(str(row['æœ¬æ–‡'])) > 200 else ''}\n```\n\n")
            f.write(f"- ãƒªãƒã‚¹ãƒˆ: {row['ãƒªãƒã‚¹ãƒˆæ•°']:,}ä»¶\n")
            f.write(f"- URL: {row['ãƒã‚¹ãƒˆURL']}\n\n")

        f.write("### ãƒªãƒã‚¹ãƒˆæ•° TOP10\n\n")
        top10_retweets = df.nlargest(10, "ãƒªãƒã‚¹ãƒˆæ•°")
        for i, (_, row) in enumerate(top10_retweets.iterrows(), 1):
            f.write(f"{i}. **{row['ãƒªãƒã‚¹ãƒˆæ•°']:,}RT** - ã„ã„ã­{row['ã„ã„ã­æ•°']:,}ä»¶ - {row['æœ¬æ–‡'][:60]}...\n")
        f.write("\n")

        # æ§‹æˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æ
        print("æ§‹æˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’åˆ†æä¸­...")
        f.write("## 2. æ§‹æˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æ\n\n")

        avg_lines, top_avg_lines, bottom_avg_lines = analyze_line_breaks(df)
        f.write("### æ”¹è¡Œã®ä½¿ç”¨å‚¾å‘\n\n")
        f.write(f"- **å…¨ä½“ã®å¹³å‡æ”¹è¡Œæ•°:** {avg_lines:.1f}å›\n")
        f.write(f"- **ã„ã„ã­æ•°ä¸Šä½25%ã®å¹³å‡æ”¹è¡Œæ•°:** {top_avg_lines:.1f}å›\n")
        f.write(f"- **ã„ã„ã­æ•°ä¸‹ä½25%ã®å¹³å‡æ”¹è¡Œæ•°:** {bottom_avg_lines:.1f}å›\n\n")
        if top_avg_lines > bottom_avg_lines:
            f.write(f"**å‚¾å‘:** ãƒã‚ºã‚‹ãƒã‚¹ãƒˆã¯å¹³å‡{top_avg_lines - bottom_avg_lines:.1f}å›å¤šãæ”¹è¡Œã—ã¦ã„ã¾ã™ã€‚èª­ã¿ã‚„ã™ã•ãŒé‡è¦ã€‚\n\n")
        else:
            f.write(f"**å‚¾å‘:** ãƒã‚ºã‚‹ãƒã‚¹ãƒˆã¯æ”¹è¡ŒãŒå°‘ãªã‚ã€‚ç°¡æ½”ã•ãŒå¥½ã¾ã‚Œã‚‹å‚¾å‘ã€‚\n\n")

        bullet_count, non_bullet_count, bullet_likes, non_bullet_likes = analyze_bullet_points(df)
        f.write("### ç®‡æ¡æ›¸ãã®ä½¿ç”¨\n\n")
        f.write(f"- **ç®‡æ¡æ›¸ãã‚ã‚Š:** {bullet_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(bullet_likes)/len(bullet_likes) if bullet_likes else 0:.0f}ä»¶ï¼‰\n")
        f.write(f"- **ç®‡æ¡æ›¸ããªã—:** {non_bullet_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(non_bullet_likes)/len(non_bullet_likes) if non_bullet_likes else 0:.0f}ä»¶ï¼‰\n\n")

        symbol_usage = analyze_symbols(df)
        f.write("### è¨˜å·ã®ä½¿ç”¨å‚¾å‘\n\n")
        for symbol, count in symbol_usage.items():
            f.write(f"- **{symbol}** ã‚’ä½¿ç”¨: {count}ä»¶\n")
        f.write("\n")

        url_count, non_url_count, url_likes, non_url_likes = analyze_urls(df)
        f.write("### URL/ãƒªãƒ³ã‚¯ã®å½±éŸ¿\n\n")
        f.write(f"- **URLã‚ã‚Š:** {url_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(url_likes)/len(url_likes) if url_likes else 0:.0f}ä»¶ï¼‰\n")
        f.write(f"- **URLãªã—:** {non_url_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(non_url_likes)/len(non_url_likes) if non_url_likes else 0:.0f}ä»¶ï¼‰\n\n")

        # å¿ƒç†ãƒ»ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°åˆ†æ
        print("å¿ƒç†ãƒ»ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã‚’åˆ†æä¸­...")
        f.write("## 3. å¿ƒç†ãƒ»ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°åˆ†æ\n\n")

        pattern_data = analyze_opening_patterns(df)
        f.write("### å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®ã„ã„ã­æ•°\n\n")
        f.write("| ãƒ‘ã‚¿ãƒ¼ãƒ³ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        pattern_sorted = sorted(pattern_data.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
        for pattern, likes in pattern_sorted:
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {pattern} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")
        if pattern_sorted:
            best_pattern = pattern_sorted[0][0]
            f.write(f"**æœ€ã‚‚åŠ¹æœçš„:** {best_pattern}å‹ã®å†’é ­ãŒæœ€ã‚‚é«˜ã„ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ\n\n")

        cta_data, no_cta = analyze_cta(df)
        f.write("### CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã®åŠ¹æœ\n\n")
        f.write("| CTAç¨®é¡ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        for cta_type, likes in cta_data.items():
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {cta_type} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        no_cta_avg = sum(no_cta)/len(no_cta) if no_cta else 0
        f.write(f"| CTAãªã— | {len(no_cta)}ä»¶ | {no_cta_avg:.0f}ä»¶ |\n")
        f.write("\n")

        emotion_data = analyze_emotion(df)
        f.write("### æ„Ÿæƒ…åˆ¥ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ\n\n")
        f.write("| æ„Ÿæƒ… | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|------|------|-------------|\n")
        emotion_sorted = sorted(emotion_data.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
        for emotion, likes in emotion_sorted:
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {emotion} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        story_count, non_story_count, story_likes, non_story_likes = analyze_story(df)
        f.write("### ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®æœ‰ç„¡\n\n")
        f.write(f"- **ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚ã‚Š:** {story_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(story_likes)/len(story_likes) if story_likes else 0:.0f}ä»¶ï¼‰\n")
        f.write(f"- **ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãªã—:** {non_story_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(non_story_likes)/len(non_story_likes) if non_story_likes else 0:.0f}ä»¶ï¼‰\n\n")

        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡åˆ†æ
        print("ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡ã‚’åˆ†æä¸­...")
        f.write("## 4. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡åˆ†æ\n\n")

        high_reply, high_retweet = analyze_engagement_ratio(df)
        f.write("### ãƒªãƒ—ãƒ©ã‚¤ç‡ãŒé«˜ã„æŠ•ç¨¿ï¼ˆè­°è«–ãƒ»å…±æ„Ÿå‹ï¼‰\n\n")
        for item in high_reply[:5]:
            f.write(f"- **{item['likes']:,}ã„ã„ã­ / {item['replies']}ãƒªãƒ—ãƒ©ã‚¤** - {item['text']}...\n")
        f.write("\n")

        f.write("### ãƒªãƒã‚¹ãƒˆç‡ãŒé«˜ã„æŠ•ç¨¿ï¼ˆæ‹¡æ•£å‹ï¼‰\n\n")
        for item in high_retweet[:5]:
            f.write(f"- **{item['likes']:,}ã„ã„ã­ / {item['retweets']}RT** - {item['text']}...\n")
        f.write("\n")

        # ã‚«ãƒ†ã‚´ãƒªåˆ†æ
        print("ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æä¸­...")
        f.write("## 5. ãƒ†ãƒ¼ãƒãƒ»ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æ\n\n")

        category_data = analyze_categories(df)
        f.write("### ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ\n\n")
        f.write("| ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° | å¹³å‡ã„ã„ã­ | å¹³å‡RT |\n")
        f.write("|---------|------|-----------|--------|\n")
        category_sorted = sorted(category_data.items(), key=lambda x: sum(x[1]["likes"])/len(x[1]["likes"]) if x[1]["likes"] else 0, reverse=True)
        for category, data in category_sorted:
            avg_likes = sum(data["likes"])/len(data["likes"]) if data["likes"] else 0
            avg_rt = sum(data["retweets"])/len(data["retweets"]) if data["retweets"] else 0
            f.write(f"| {category} | {len(data['likes'])}ä»¶ | {avg_likes:.0f}ä»¶ | {avg_rt:.0f}ä»¶ |\n")
        f.write("\n")

        # æ™‚é–“åˆ†æ
        print("æ™‚é–“å¸¯ã‚’åˆ†æä¸­...")
        f.write("## 6. æ™‚é–“ãƒ»ã‚¿ã‚¤ãƒŸãƒ³ã‚°åˆ†æ\n\n")

        time_slots, weekday_data = analyze_time(df)
        f.write("### æŠ•ç¨¿æ™‚é–“å¸¯åˆ¥ã®å¹³å‡ã„ã„ã­æ•°\n\n")
        f.write("| æ™‚é–“å¸¯ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|--------|------|-------------|\n")
        for slot, likes in time_slots.items():
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {slot} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        f.write("### æ›œæ—¥åˆ¥ã®å¹³å‡ã„ã„ã­æ•°\n\n")
        f.write("| æ›œæ—¥ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|------|------|-------------|\n")
        for day in ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]:
            likes = weekday_data.get(day, [])
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {day}æ›œæ—¥ | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        cross_data = analyze_time_category_cross(df)
        f.write("### æ™‚é–“å¸¯Ã—ã‚«ãƒ†ã‚´ãƒªã®ã‚¯ãƒ­ã‚¹åˆ†æ\n\n")
        for time_slot, categories in cross_data.items():
            f.write(f"#### {time_slot}\n\n")
            cat_sorted = sorted(categories.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
            for cat, likes in cat_sorted[:3]:
                avg = sum(likes)/len(likes) if likes else 0
                f.write(f"- **{cat}:** å¹³å‡{avg:.0f}ã„ã„ã­ ({len(likes)}ä»¶)\n")
            f.write("\n")

        # ç·åˆã¾ã¨ã‚
        print("ç·åˆã¾ã¨ã‚ã‚’ç”Ÿæˆä¸­...")
        f.write("## 7. AIå‰¯æ¥­ç³»ã§ãƒã‚ºã‚‹ãƒã‚¹ãƒˆã®é»„é‡‘ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n")
        f.write("å…¨åˆ†æçµæœã‚’è¸ã¾ãˆãŸã€å†ç¾æ€§ã®é«˜ã„ãƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³5é¸:\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ•°å­—æç¤ºÃ—å®Ÿç¸¾å ±å‘Š
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ•°å­—æç¤ºÃ—å®Ÿç¸¾å ±å‘Šå‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- å†’é ­ã«å…·ä½“çš„ãªæ•°å­—ã‚’æç¤º\n")
        f.write("- å®Ÿç¸¾ã‚„æˆæœã‚’æ˜ç¢ºã«ç¤ºã™\n")
        f.write("- ç®‡æ¡æ›¸ãã§æƒ…å ±ã‚’æ•´ç†\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("AIå‰¯æ¥­ã§æœˆå30ä¸‡å††é”æˆã—ã¾ã—ãŸğŸ‰\n\n")
        f.write("å®Ÿè·µã—ãŸ3ã¤ã®ã“ã¨:\n")
        f.write("â‘  ChatGPTã§è¨˜äº‹ä½œæˆä»£è¡Œ\n")
        f.write("â‘¡ Midjourneyã§ãƒ­ã‚´ãƒ‡ã‚¶ã‚¤ãƒ³\n")
        f.write("â‘¢ Claude Codeã§è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«è²©å£²\n\n")
        f.write("åˆæœˆã¯5ä¸‡å††â†’3ãƒ¶æœˆã§30ä¸‡å††ã«ã€‚\n")
        f.write("å‰¯æ¥­ã§ã‚‚ååˆ†ç¨¼ã’ã¾ã™ğŸ’ª\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å•é¡Œæèµ·Ã—å…±æ„Ÿ
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³2: å•é¡Œæèµ·Ã—å…±æ„Ÿå‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- ã€Œã¯ï¼Ÿã€ã€Œã‚„ã°ã„ã€ãªã©æ„Ÿæƒ…çš„ãªå†’é ­\n")
        f.write("- èª­è€…ã®æ‚©ã¿ã«å…±æ„Ÿ\n")
        f.write("- è§£æ±ºç­–ã‚’æç¤º\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("ã¯ï¼ŸAIã§å‰¯æ¥­ã¨ã‹æ€ªã—ã„ã£ã¦æ€ã£ã¦ã¾ã—ãŸã€‚\n\n")
        f.write("ã§ã‚‚å®Ÿéš›ã‚„ã£ã¦ã¿ãŸã‚‰...\n")
        f.write("â†’ 1æ—¥2æ™‚é–“ã§æœˆ10ä¸‡å††ç¨¼ã’ãŸ\n")
        f.write("â†’ ã‚¹ã‚­ãƒ«ä¸è¦ã§åˆå¿ƒè€…ã§ã‚‚OK\n")
        f.write("â†’ åœ¨å®…ã§å®Œçµ\n\n")
        f.write("ãƒã‚¤ãƒˆã‚ˆã‚Šå…¨ç„¶åŠ¹ç‡ã„ã„ã€‚\n")
        f.write("ã‚‚ã£ã¨æ—©ãå§‹ã‚ã‚Œã°ã‚ˆã‹ã£ãŸğŸ˜­\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¦ãƒã‚¦Ã—ç®‡æ¡æ›¸ã
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¦ãƒã‚¦Ã—ç®‡æ¡æ›¸ãå‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- ã€Œã€œã™ã‚‹æ–¹æ³•ã€ãªã©ä¾¡å€¤æç¤º\n")
        f.write("- ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¢ºã«\n")
        f.write("- å†ç¾æ€§ã‚’å¼·èª¿\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("åˆå¿ƒè€…ãŒAIå‰¯æ¥­ã§æœˆ5ä¸‡å††ç¨¼ãæ–¹æ³•\n\n")
        f.write("ã€ã‚¹ãƒ†ãƒƒãƒ—ã€‘\n")
        f.write("1. ChatGPTã«ç„¡æ–™ç™»éŒ²\n")
        f.write("2. ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ã§ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ¡ˆä»¶æ¢ã™\n")
        f.write("3. AIã§ä¸‹æ›¸ãâ†’è‡ªåˆ†ã§ä»•ä¸Šã’\n")
        f.write("4. ç´å“ã—ã¦å ±é…¬ã‚²ãƒƒãƒˆ\n\n")
        f.write("ã“ã‚Œã ã‘ã€‚\n")
        f.write("ã‚¹ã‚­ãƒ«ã‚¼ãƒ­ã‹ã‚‰å§‹ã‚ã¦2é€±é–“ã§åˆåç›Šå‡ºã¾ã—ãŸâœ¨\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä½“é¨“è«‡Ã—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä½“é¨“è«‡Ã—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- è‡ªåˆ†ã®çµŒé¨“ã‚’æ™‚ç³»åˆ—ã§èªã‚‹\n")
        f.write("- Beforeâ†’Afterã‚’æ˜ç¢ºã«\n")
        f.write("- ãƒªã‚¢ãƒ«ãªæ•°å­—ã‚’å«ã‚ã‚‹\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("3ãƒ¶æœˆå‰: ãƒã‚¤ãƒˆæœˆ8ä¸‡å††ã§æ¶ˆè€—\n")
        f.write("2ãƒ¶æœˆå‰: AIå‰¯æ¥­é–‹å§‹â†’åˆæœˆ3ä¸‡å††\n")
        f.write("1ãƒ¶æœˆå‰: ã‚³ãƒ„æ´ã‚“ã§æœˆ12ä¸‡å††\n")
        f.write("ä»Š: ãƒã‚¤ãƒˆè¾ã‚ã¦AIå‰¯æ¥­ã®ã¿ã§æœˆ18ä¸‡å††ğŸš€\n\n")
        f.write("ä½¿ã£ã¦ã‚‹ã®ã¯ChatGPTã¨Canvaã ã‘ã€‚\n")
        f.write("äººç”Ÿå¤‰ã‚ã‚Šã¾ã—ãŸã€‚\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒ„ãƒ¼ãƒ«ç´¹ä»‹Ã—ç·Šæ€¥æ€§
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒ„ãƒ¼ãƒ«ç´¹ä»‹Ã—ç·Šæ€¥æ€§å‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- ã€Œä»Šã™ãã€ã€Œã¾ã é–“ã«åˆã†ã€ãªã©ç·Šæ€¥æ€§\n")
        f.write("- å…·ä½“çš„ãªãƒ„ãƒ¼ãƒ«å\n")
        f.write("- ç°¡æ½”ã«ãƒ¡ãƒªãƒƒãƒˆæç¤º\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("Claude Codeã€ã¾ã ä½¿ã£ã¦ãªã„äººã¯æã—ã¦ã¾ã™ã€‚\n\n")
        f.write("ã“ã‚Œ1ã¤ã§:\n")
        f.write("ãƒ»ã‚³ãƒ¼ãƒ‰è‡ªå‹•ç”Ÿæˆ\n")
        f.write("ãƒ»ãƒã‚°ä¿®æ­£ã‚‚ç§’é€Ÿ\n")
        f.write("ãƒ»ãƒ„ãƒ¼ãƒ«é–‹ç™ºãŒçˆ†é€ŸåŒ–\n\n")
        f.write("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã§ã‚‚\n")
        f.write("Webã‚¢ãƒ—ãƒªä½œã‚Œã‚‹ãƒ¬ãƒ™ãƒ«ã€‚\n\n")
        f.write("ã¿ã‚“ãªãŒæ°—ã¥ãå‰ã«ä½¿ã„å€’ã™ã¹ãğŸ”¥\n")
        f.write("```\n\n")

        # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³8: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ ===
        print("ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–åˆ†æä¸­...")
        f.write("## 8. ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ­£è¦åŒ–ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ\n\n")

        follower_data = analyze_follower_normalized(df)

        if follower_data["has_follower_data"]:
            f.write("### ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ TOP10\n\n")
            f.write("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­æ•° | ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•° | ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ |\n")
            f.write("|------|---------|---------|------------|------------------|\n")
            for i, item in enumerate(follower_data["top10"], 1):
                f.write(f"| {i} | @{item['user']} | {item['likes']:,} | {item['followers']:,} | {item['rate']:.1f}% |\n")
            f.write("\n")

            if follower_data["hidden_gems"]:
                f.write("### Hidden Gemsï¼ˆéš ã‚ŒãŸåæŠ•ç¨¿ï¼‰\n\n")
                f.write("ä½ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã§ã‚‚é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’é”æˆã—ãŸãƒã‚¹ãƒˆï¼š\n\n")
                for item in follower_data["hidden_gems"]:
                    f.write(f"- **@{item['user']}** (ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼: {item['followers']:,}äºº) - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {item['rate']:.1f}% - {item['text']}...\n")
                f.write("\n")

            if follower_data["pattern_by_rate"]:
                f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡\n\n")
                f.write("| ãƒ‘ã‚¿ãƒ¼ãƒ³ | ä»¶æ•° | å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ |\n")
                f.write("|---------|------|---------------------|\n")
                pat_sorted = sorted(follower_data["pattern_by_rate"].items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
                for pat, rates in pat_sorted:
                    avg_rate = sum(rates) / len(rates) if rates else 0
                    f.write(f"| {pat} | {len(rates)}ä»¶ | {avg_rate:.1f}% |\n")
                f.write("\n")

            if follower_data["category_by_rate"]:
                f.write("### ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡\n\n")
                f.write("| ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° | å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ |\n")
                f.write("|---------|------|---------------------|\n")
                cat_sorted = sorted(follower_data["category_by_rate"].items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
                for cat, rates in cat_sorted:
                    avg_rate = sum(rates) / len(rates) if rates else 0
                    f.write(f"| {cat} | {len(rates)}ä»¶ | {avg_rate:.1f}% |\n")
                f.write("\n")
        else:
            f.write("**æ³¨æ„:** ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ï¼ˆã„ã„ã­ + ãƒªãƒã‚¹ãƒˆ*2 + ãƒªãƒ—ãƒ©ã‚¤*3ï¼‰ã§ä»£æ›¿åˆ†æã‚’è¡Œã„ã¾ã™ã€‚\n\n")
            f.write("### ç·åˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ TOP10\n\n")
            f.write("| é †ä½ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ | ã„ã„ã­æ•° | ç·åˆã‚¹ã‚³ã‚¢ | æœ¬æ–‡ |\n")
            f.write("|------|---------|---------|-----------|------|\n")
            for i, item in enumerate(follower_data["top10"], 1):
                f.write(f"| {i} | @{item['user']} | {item['likes']:,} | {item['rate']:.0f} | {item['text']}... |\n")
            f.write("\n")

        # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³9: ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–åˆ†æ ===
        print("ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–ã‚’åˆ†æä¸­...")
        f.write("## 9. ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–åˆ†æ\n\n")

        text_length_data = analyze_text_length(df)
        f.write("### æœ€é©æ–‡å­—æ•°åˆ†æ\n\n")
        f.write(f"- **å…¨ä½“ã®å¹³å‡æ–‡å­—æ•°:** {text_length_data['avg_length']:.0f}å­—\n")
        f.write(f"- **æœ€é©æ–‡å­—æ•°å¸¯:** {text_length_data['best_bucket']}\n")
        f.write(f"- **æ–‡å­—æ•°ã¨ã„ã„ã­æ•°ã®ç›¸é–¢:** r={text_length_data['correlation']:.2f}\n\n")

        f.write("| æ–‡å­—æ•°å¸¯ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        for bucket_name, likes in text_length_data["bucket_data"].items():
            avg = sum(likes) / len(likes) if likes else 0
            f.write(f"| {bucket_name} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        emoji_data = analyze_emoji_usage(df)
        f.write("### çµµæ–‡å­—ä½¿ç”¨åˆ†æ\n\n")
        avg_with = sum(emoji_data["with_emoji"]) / len(emoji_data["with_emoji"]) if emoji_data["with_emoji"] else 0
        avg_without = sum(emoji_data["without_emoji"]) / len(emoji_data["without_emoji"]) if emoji_data["without_emoji"] else 0
        f.write(f"- **çµµæ–‡å­—ã‚ã‚Š:** {len(emoji_data['with_emoji'])}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {avg_with:.0f}ä»¶ï¼‰\n")
        f.write(f"- **çµµæ–‡å­—ãªã—:** {len(emoji_data['without_emoji'])}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {avg_without:.0f}ä»¶ï¼‰\n\n")

        f.write("| çµµæ–‡å­—æ•° | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        for count_label in ["0å€‹", "1-2å€‹", "3-5å€‹", "6å€‹ä»¥ä¸Š"]:
            likes = emoji_data["emoji_count_data"].get(count_label, [])
            avg = sum(likes) / len(likes) if likes else 0
            f.write(f"| {count_label} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        if emoji_data["top_emoji"]:
            f.write("**äººæ°—çµµæ–‡å­—TOP5:**\n\n")
            for i, (emoji, count) in enumerate(emoji_data["top_emoji"], 1):
                f.write(f"{i}. {emoji} ({count}ä»¶)\n")
            f.write("\n")

        hashtag_data = analyze_hashtag_usage(df)
        f.write("### ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°åˆ†æ\n\n")
        avg_with_ht = sum(hashtag_data["with_hashtag"]) / len(hashtag_data["with_hashtag"]) if hashtag_data["with_hashtag"] else 0
        avg_without_ht = sum(hashtag_data["without_hashtag"]) / len(hashtag_data["without_hashtag"]) if hashtag_data["without_hashtag"] else 0
        f.write(f"- **ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚ã‚Š:** {len(hashtag_data['with_hashtag'])}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {avg_with_ht:.0f}ä»¶ï¼‰\n")
        f.write(f"- **ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãªã—:** {len(hashtag_data['without_hashtag'])}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {avg_without_ht:.0f}ä»¶ï¼‰\n\n")

        if hashtag_data["top_hashtags"]:
            f.write("**äººæ°—ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°TOP10:**\n\n")
            for i, (tag, count) in enumerate(hashtag_data["top_hashtags"], 1):
                f.write(f"{i}. {tag} ({count}ä»¶)\n")
            f.write("\n")

        # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³10: ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ ===
        print("ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
        f.write("## 10. ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢\n\n")

        # ã‚¹ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®š
        score_params = {}
        tl = text_length_data
        if tl["best_bucket"]:
            buckets_ranges = {
                "0-50å­—": (0, 50), "51-100å­—": (51, 100), "101-150å­—": (101, 150),
                "151-200å­—": (151, 200), "201-300å­—": (201, 300),
                "301-500å­—": (301, 500), "500å­—ä»¥ä¸Š": (501, 1000),
            }
            rng = buckets_ranges.get(tl["best_bucket"], (100, 300))
            score_params["optimal_min"] = rng[0]
            score_params["optimal_max"] = rng[1]

        buzz_data = analyze_buzz_scores(df, score_params)

        f.write("### ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«\n\n")
        f.write("| è¦ç´  | é…ç‚¹ | èª¬æ˜ |\n")
        f.write("|------|------|------|\n")
        f.write("| å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ | 20ç‚¹ | æ•°å­—æç¤ºãƒ»ç–‘å•å½¢ãŒé«˜å¾—ç‚¹ |\n")
        f.write("| ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ– | 15ç‚¹ | æœ€é©æ–‡å­—æ•°ç¯„å›²å†…ã‹ã©ã†ã‹ |\n")
        f.write("| ã‚«ãƒ†ã‚´ãƒª | 15ç‚¹ | é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚«ãƒ†ã‚´ãƒªã‹ |\n")
        f.write("| æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ | 10ç‚¹ | æ„Ÿæƒ…ã‚’åˆºæ¿€ã™ã‚‹è¦ç´  |\n")
        f.write("| CTA | 10ç‚¹ | è¡Œå‹•å–šèµ·ã®æœ‰ç„¡ |\n")
        f.write("| ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ | 10ç‚¹ | ç‰©èªçš„è¦ç´ ã®æœ‰ç„¡ |\n")
        f.write("| çµµæ–‡å­—ãƒ»æ›¸å¼ | 10ç‚¹ | é©åˆ‡ãªçµµæ–‡å­—ä½¿ç”¨ |\n")
        f.write("| èª­ã¿ã‚„ã™ã• | 10ç‚¹ | æ”¹è¡Œãƒ»ç®‡æ¡æ›¸ãã®ä½¿ç”¨ |\n")
        f.write("\n")

        corr = buzz_data["correlation"]
        strength = "å¼·ã„" if abs(corr) > 0.5 else "ä¸­ç¨‹åº¦ã®" if abs(corr) > 0.3 else "å¼±ã„"
        f.write("### ãƒ¢ãƒ‡ãƒ«ç²¾åº¦\n\n")
        f.write(f"- **äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¨å®Ÿéš›ã®ã„ã„ã­æ•°ã®ç›¸é–¢:** r={corr:.2f}\n")
        f.write(f"- **åˆ¤å®š:** {strength}ç›¸é–¢\n\n")

        f.write("### ã‚¹ã‚³ã‚¢å¸¯åˆ¥ã®å®Ÿéš›ã®ã„ã„ã­æ•°\n\n")
        f.write("| ã‚¹ã‚³ã‚¢å¸¯ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        for bucket_name in ["80-100ç‚¹", "60-79ç‚¹", "40-59ç‚¹", "0-39ç‚¹"]:
            likes = buzz_data["score_buckets"].get(bucket_name, [])
            avg = sum(likes) / len(likes) if likes else 0
            f.write(f"| {bucket_name} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        f.write("### è¦ç´ åˆ¥ã®å¹³å‡ã‚¹ã‚³ã‚¢ï¼ˆå½±éŸ¿åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰\n\n")
        factor_sorted = sorted(buzz_data["factor_avg"].items(), key=lambda x: x[1], reverse=True)
        for i, (factor, avg_score) in enumerate(factor_sorted, 1):
            f.write(f"{i}. **{factor}** - å¹³å‡{avg_score:.1f}ç‚¹\n")
        f.write("\n")

        f.write("### TOP10æŠ•ç¨¿ã®ã‚¹ã‚³ã‚¢åˆ†æ\n\n")
        f.write("| é †ä½ | ã„ã„ã­æ•° | ãƒã‚ºã‚¹ã‚³ã‚¢ | ä¸»ãªé«˜å¾—ç‚¹è¦å›  |\n")
        f.write("|------|---------|-----------|-------------|\n")
        top_by_likes = sorted(buzz_data["scores"], key=lambda x: x["likes"], reverse=True)[:10]
        for i, item in enumerate(top_by_likes, 1):
            top_factors = sorted(item["factors"].items(), key=lambda x: x[1], reverse=True)[:2]
            factor_str = ", ".join(f"{f[0]}({f[1]}ç‚¹)" for f in top_factors)
            f.write(f"| {i} | {item['likes']:,} | {item['score']}ç‚¹ | {factor_str} |\n")
        f.write("\n")

        # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³11: ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ ===
        if df_raw is not None:
            print("ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æä¸­...")
            f.write("## 11. ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ\n\n")
            f.write(f"**æ³¨æ„:** ã“ã®åˆ†æã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ{len(df_raw)}ä»¶ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n\n")

            user_data = analyze_users(df_raw, df)

            if user_data["repeat_buzzers"]:
                f.write("### ãƒªãƒ”ãƒ¼ãƒˆãƒã‚ºãƒ¦ãƒ¼ã‚¶ãƒ¼\n\n")
                f.write("è¤‡æ•°ã®é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿ã‚’æŒã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼š\n\n")
                f.write("| ãƒ¦ãƒ¼ã‚¶ãƒ¼ | æŠ•ç¨¿æ•° | å¹³å‡ã„ã„ã­ | æœ€å¤§ã„ã„ã­ | åˆè¨ˆã„ã„ã­ |\n")
                f.write("|---------|--------|-----------|-----------|----------|\n")
                for u in user_data["repeat_buzzers"][:15]:
                    f.write(f"| @{u['user']} | {u['post_count']}ä»¶ | {u['avg_likes']:.0f} | {u['max_likes']:,} | {u['total_likes']:,} |\n")
                f.write("\n")

            traits = user_data["common_traits"]
            if traits["total"] > 0:
                f.write("### å¸¸é€£ãƒã‚ºã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å…±é€šç‰¹å¾´\n\n")
                avg_len = sum(traits["text_lengths"]) / len(traits["text_lengths"]) if traits["text_lengths"] else 0
                f.write(f"- **å¹³å‡æŠ•ç¨¿æ–‡å­—æ•°:** {avg_len:.0f}å­—\n")
                if traits["categories"]:
                    top_cat = traits["categories"].most_common(1)[0]
                    f.write(f"- **æœ€å¤šã‚«ãƒ†ã‚´ãƒª:** {top_cat[0]}ï¼ˆ{top_cat[1]}ä»¶ï¼‰\n")
                if traits["openings"]:
                    top_open = traits["openings"].most_common(1)[0]
                    f.write(f"- **æœ€å¤šå†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³:** {top_open[0]}ï¼ˆ{top_open[1]}ä»¶ï¼‰\n")
                cta_rate = (traits["cta_count"] / traits["total"]) * 100 if traits["total"] > 0 else 0
                f.write(f"- **CTAä½¿ç”¨ç‡:** {cta_rate:.0f}%\n\n")

            f.write("### æŠ•ç¨¿é »åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚\n\n")
            f.write("| æŠ•ç¨¿æ•° | ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•° | å¹³å‡ã„ã„ã­ |\n")
            f.write("|--------|-----------|----------|\n")
            for freq_label in ["1ä»¶", "2ä»¶", "3ä»¶ä»¥ä¸Š"]:
                likes = user_data["freq_data"].get(freq_label, [])
                avg = sum(likes) / len(likes) if likes else 0
                f.write(f"| {freq_label} | {len(likes)}äºº | {avg:.0f} |\n")
            f.write("\n")

        # === ã¾ã¨ã‚ ===
        f.write("## ã¾ã¨ã‚\n\n")
        f.write("### ãƒã‚ºã‚‹æŠ•ç¨¿ã®å¿…é ˆè¦ç´ \n\n")
        f.write("1. **å†’é ­ã§å¿ƒã‚’æ´ã‚€** - æ•°å­—ã€ç–‘å•ã€ç…½ã‚Šã€å…±æ„Ÿã®ã„ãšã‚Œã‹ã§é–‹å§‹\n")
        f.write("2. **å…·ä½“çš„ãªæ•°å­—** - ã€Œæœˆ30ä¸‡å††ã€ã€Œ3ãƒ¶æœˆã€ãªã©æ˜ç¢ºãªå®Ÿç¸¾\n")
        f.write("3. **èª­ã¿ã‚„ã™ã•** - æ”¹è¡Œãƒ»ç®‡æ¡æ›¸ããƒ»çµµæ–‡å­—ã§è¦–è¦šçš„ã«æ•´ç†\n")
        f.write("4. **å†ç¾æ€§** - ã€Œè‡ªåˆ†ã«ã‚‚ã§ããã†ã€ã¨æ€ã‚ã›ã‚‹\n")
        f.write("5. **æ„Ÿæƒ…ã‚’åˆºæ¿€** - æœŸå¾…ãƒ»é©šããƒ»å…±æ„Ÿã®ã„ãšã‚Œã‹ã‚’å«ã‚ã‚‹\n")
        f.write(f"6. **æœ€é©ãªæ–‡å­—æ•°** - {text_length_data['best_bucket']}ãŒæœ€ã‚‚ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãŒé«˜ã„\n")
        f.write(f"7. **ãƒã‚ºäºˆæ¸¬ã‚¹ã‚³ã‚¢** - ã‚¹ã‚³ã‚¢ã¨å®Ÿã„ã„ã­æ•°ã®ç›¸é–¢ r={corr:.2f}\n\n")
        f.write("ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãŒå°‘ãªãã¦ã‚‚ãƒã‚ºã‚‹å¯èƒ½æ€§ãŒé«˜ã¾ã‚Šã¾ã™ã€‚\n\n")

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³12: ãƒã‚ºãƒã‚¹ãƒˆè‡ªå‹•ç”Ÿæˆ
        print("ãƒã‚ºãƒã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        try:
            from generate_posts import generate_posts, format_posts_markdown
            posts, tools, works, ctas = generate_posts(df)
            md = format_posts_markdown(posts, tools, works, ctas, section_num=12)
            f.write(md)
        except Exception as e:
            print(f"  æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³13: ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
        print("ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­...")
        try:
            from visualize import generate_all_charts
            chart_dir = os.path.join(os.path.dirname(output_filename), "charts")
            chart_paths = generate_all_charts(df, chart_dir, df_raw=df_raw)
            f.write("\n## 13. å¯è¦–åŒ–ã‚°ãƒ©ãƒ•\n\n")
            f.write("ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼š\n\n")
            for name, path in chart_paths.items():
                f.write(f"- **{name}**: `{path}`\n")
            f.write("\n")
        except Exception as e:
            print(f"  ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

    print(f"\nãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_filename}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    input_file = "output/buzz_posts_20260215.xlsx"
    today = datetime.now().strftime("%Y%m%d")
    output_file = f"output/analyze_report_è©³ç´°_v2_{today}.md"

    if not os.path.exists(input_file):
        print(f"ã‚¨ãƒ©ãƒ¼: {input_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"åˆ†æå¯¾è±¡: {input_file}")
    df = load_excel(input_file)

    if df is None or len(df) == 0:
        print("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ã®ã¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æç”¨ï¼‰
    df_keyword_filtered = filter_keywords(df)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ + é‡è¤‡é™¤å»ï¼‰
    df_filtered, original_count, excluded_count = filter_data(df)

    if len(df_filtered) == 0:
        print("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    generate_report(df_filtered, output_file, original_count, excluded_count, df_raw=df_keyword_filtered)


if __name__ == "__main__":
    main()
