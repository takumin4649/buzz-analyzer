"""ãƒã‚ºãƒã‚¹ãƒˆã®è©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆv2 - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime

import pandas as pd


def load_excel(filename):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        df = pd.read_excel(filename)
        print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒã‚¹ãƒˆ")
        return df
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def filter_data(df):
    """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    original_count = len(df)

    # 1. ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã‚’é™¤å¤–
    exclude_keywords = [
        "è‘—ä½œæ¨©", "ç‰ˆæ¨©", "æµ·è³Šç‰ˆ", "åç›ŠåŒ–åœæ­¢", "åç›ŠåŒ–ãŒåœæ­¢",
        "å‰¥å¥ª", "ä¾µå®³", "ã‚¤ãƒ³ãƒ—ãƒ¬ã‚¾ãƒ³ãƒ“"
    ]

    def should_exclude(text):
        for keyword in exclude_keywords:
            if keyword in text:
                return True
        return False

    df_filtered = df[~df["æœ¬æ–‡"].apply(should_exclude)].copy()
    excluded_by_keyword = original_count - len(df_filtered)
    print(f"ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã‚’é™¤å¤–: {excluded_by_keyword}ä»¶")

    # 2. åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã¯æœ€ã‚‚ã„ã„ã­æ•°ãŒé«˜ã„1ä»¶ã®ã¿æ®‹ã™
    df_filtered = df_filtered.sort_values("ã„ã„ã­æ•°", ascending=False)
    df_filtered = df_filtered.drop_duplicates(subset=["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"], keep="first")
    excluded_by_user = len(df) - excluded_by_keyword - len(df_filtered)
    print(f"åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é‡è¤‡æŠ•ç¨¿ã‚’é™¤å¤–: {excluded_by_user}ä»¶")

    total_excluded = original_count - len(df_filtered)
    print(f"æœ€çµ‚åˆ†æå¯¾è±¡: {len(df_filtered)}ä»¶ï¼ˆ{total_excluded}ä»¶é™¤å¤–ï¼‰")

    return df_filtered, original_count, total_excluded


def safe_get(row, column, default=""):
    """å®‰å…¨ã«ã‚«ãƒ©ãƒ ã®å€¤ã‚’å–å¾—"""
    try:
        value = row.get(column, default)
        return value if pd.notna(value) else default
    except:
        return default


def analyze_line_breaks(df):
    """æ”¹è¡Œã®åˆ†æ"""
    results = []
    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        line_count = text.count("\n")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        results.append({"line_count": line_count, "likes": likes})

    df_lines = pd.DataFrame(results)
    avg_lines = df_lines["line_count"].mean()

    # ä¸Šä½25%ã¨ä¸‹ä½25%ã§æ¯”è¼ƒ
    top_25 = df.nlargest(len(df) // 4, "ã„ã„ã­æ•°")
    bottom_25 = df.nsmallest(len(df) // 4, "ã„ã„ã­æ•°")

    top_avg_lines = sum(safe_get(row, "æœ¬æ–‡", "").count("\n") for _, row in top_25.iterrows()) / len(top_25) if len(top_25) > 0 else 0
    bottom_avg_lines = sum(safe_get(row, "æœ¬æ–‡", "").count("\n") for _, row in bottom_25.iterrows()) / len(bottom_25) if len(bottom_25) > 0 else 0

    return avg_lines, top_avg_lines, bottom_avg_lines


def analyze_bullet_points(df):
    """ç®‡æ¡æ›¸ãã®åˆ†æ"""
    bullet_pattern = re.compile(r'^[ãƒ»\-\*â‘ -â“1-9]\s', re.MULTILINE)

    with_bullets = []
    without_bullets = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        if bullet_pattern.search(text):
            with_bullets.append(likes)
        else:
            without_bullets.append(likes)

    return len(with_bullets), len(without_bullets), with_bullets, without_bullets


def analyze_symbols(df):
    """è¨˜å·ã®ä½¿ç”¨åˆ†æ"""
    symbol_patterns = {
        "â†’": r"â†’",
        "ï¼": r"[ï¼=]{2,}",
        "ï½œ": r"ï½œ",
        "ã€ã€‘": r"ã€.*?ã€‘",
    }

    symbol_usage = defaultdict(int)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        for symbol, pattern in symbol_patterns.items():
            if re.search(pattern, text):
                symbol_usage[symbol] += 1

    return symbol_usage


def analyze_urls(df):
    """URLæœ‰ç„¡ã®åˆ†æ"""
    url_pattern = re.compile(r'https?://\S+')

    with_url = []
    without_url = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        if url_pattern.search(text):
            with_url.append(likes)
        else:
            without_url.append(likes)

    return len(with_url), len(without_url), with_url, without_url


def classify_opening_pattern(first_line):
    """å†’é ­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡"""
    if not first_line:
        return "ãã®ä»–"

    if re.search(r'[ï¼Ÿ?]', first_line):
        return "ç–‘å•å½¢"
    elif re.search(r'^[0-9â‘ -â“]|[0-9]+ã¤|[0-9]+å€‹|[0-9]+é¸', first_line):
        return "æ•°å­—æç¤º"
    elif re.search(r'ã¯ï¼Ÿ|ã¾ã˜ã§|ã‚„ã°ã„|æœ€æ‚ª|ã‚ã‚Šãˆãªã„', first_line, re.IGNORECASE):
        return "ç…½ã‚Š"
    elif re.search(r'ã‚ã‹ã‚‹|å…±æ„Ÿ|åŒã˜|ã‚ã‚‹ã‚ã‚‹', first_line, re.IGNORECASE):
        return "å…±æ„Ÿ"
    elif re.search(r'ã§ã™|ã¾ã™|ã§ã‚ã‚‹|ã ã€‚', first_line):
        return "æ–­å®šå½¢"
    elif re.search(r'ã¿ãªã•ã‚“|ã‚ãªãŸ|çš†ã•ã‚“', first_line, re.IGNORECASE):
        return "å‘¼ã³ã‹ã‘"
    else:
        return "ãã®ä»–"


def analyze_opening_patterns(df):
    """å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
    pattern_data = defaultdict(list)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        first_line = text.split("\n")[0] if text else ""

        pattern = classify_opening_pattern(first_line)
        pattern_data[pattern].append(likes)

    return pattern_data


def analyze_cta(df):
    """CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã®åˆ†æ"""
    cta_patterns = {
        "ã„ã„ã­ç³»": r'ã„ã„ã­|ğŸ‘|ãƒãƒ¼ãƒˆ',
        "ä¿å­˜ç³»": r'ä¿å­˜|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯',
        "ãƒ•ã‚©ãƒ­ãƒ¼ç³»": r'ãƒ•ã‚©ãƒ­ãƒ¼|follow',
        "ã‚·ã‚§ã‚¢ç³»": r'ãƒªãƒã‚¹ãƒˆ|RT|ã‚·ã‚§ã‚¢|æ‹¡æ•£',
        "ã‚³ãƒ¡ãƒ³ãƒˆç³»": r'ã‚³ãƒ¡ãƒ³ãƒˆ|è¿”ä¿¡|æ•™ãˆã¦',
    }

    cta_data = defaultdict(list)
    no_cta = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        has_cta = False
        for cta_type, pattern in cta_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                cta_data[cta_type].append(likes)
                has_cta = True

        if not has_cta:
            no_cta.append(likes)

    return cta_data, no_cta


def analyze_emotion(df):
    """æ„Ÿæƒ…åˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰- æ€’ã‚Šã‚«ãƒ†ã‚´ãƒªé™¤å¤–"""
    emotion_patterns = {
        "æœŸå¾…": r'ãƒãƒ£ãƒ³ã‚¹|å¯èƒ½æ€§|ç¨¼ã’ã‚‹|å„²ã‹ã‚‹|æˆåŠŸ|é”æˆ|å®Ÿç¾|ã§ãã‚‹',
        "é©šã": r'ã¾ã•ã‹|ã³ã£ãã‚Š|é©šã|ã™ã”ã„|ã‚„ã°ã„',
        "å…±æ„Ÿ": r'ã‚ã‹ã‚‹|ãã†ãã†|ã‚ã‚‹ã‚ã‚‹|åŒã˜|ç§ã‚‚',
        "ææ€–": r'å±é™º|æ€–ã„|ãƒªã‚¹ã‚¯|å¤±æ•—|æ|ãƒ¤ãƒã„|æœ€æ‚ª',
    }

    emotion_data = defaultdict(list)

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        for emotion, pattern in emotion_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                emotion_data[emotion].append(likes)

    return emotion_data


def has_story(text):
    """ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®åˆ¤å®š"""
    story_keywords = [
        r'ã¾ãš|æ¬¡ã«|ãã—ã¦|æœ€å¾Œã«',
        r'before|after|â†’',
        r'æ˜”|ä»¥å‰|æœ€åˆ|ä»Šã§ã¯|ç¾åœ¨',
        r'ç§|åƒ•|è‡ªåˆ†|å®Ÿéš›ã«|ã‚„ã£ã¦ã¿ãŸ',
    ]

    for pattern in story_keywords:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


def analyze_story(df):
    """ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®åˆ†æ"""
    with_story = []
    without_story = []

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        if has_story(text):
            with_story.append(likes)
        else:
            without_story.append(likes)

    return len(with_story), len(without_story), with_story, without_story


def analyze_engagement_ratio(df):
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡ã®åˆ†æ"""
    high_reply_ratio = []  # ãƒªãƒ—ãƒ©ã‚¤ãŒå¤šã„
    high_retweet_ratio = []  # ãƒªãƒã‚¹ãƒˆãŒå¤šã„

    for _, row in df.iterrows():
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        replies = safe_get(row, "ãƒªãƒ—ãƒ©ã‚¤æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)
        text = safe_get(row, "æœ¬æ–‡", "")

        if likes > 0:
            reply_ratio = replies / likes
            retweet_ratio = retweets / likes

            if reply_ratio > 0.05:  # ãƒªãƒ—ãƒ©ã‚¤ç‡5%ä»¥ä¸Š
                high_reply_ratio.append({"text": text[:50], "likes": likes, "replies": replies})

            if retweet_ratio > 0.2:  # ãƒªãƒã‚¹ãƒˆç‡20%ä»¥ä¸Š
                high_retweet_ratio.append({"text": text[:50], "likes": likes, "retweets": retweets})

    return high_reply_ratio, high_retweet_ratio


def classify_category(text):
    """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰"""
    # å„ªå…ˆé †ä½ã‚’æŒãŸã›ã¦åˆ¤å®š

    # å®Ÿç¸¾å ±å‘Šç³»ï¼ˆæœ€å„ªå…ˆï¼‰
    if re.search(r'é”æˆ|åç›Š|ç¨¼ã’ãŸ|ç¨¼ã„ã |æˆåŠŸ|å®Ÿç¸¾|å„²ã‹ã£ãŸ|ã€œä¸‡å††|æœˆå|å¹´å|å£²ä¸Š|å ±é…¬|åˆ©ç›Š', text, re.IGNORECASE):
        return "å®Ÿç¸¾å ±å‘Šç³»"

    # ãƒã‚¦ãƒã‚¦ç³»
    if re.search(r'æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—|ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯|æ”»ç•¥|ãƒãƒ‹ãƒ¥ã‚¢ãƒ«|ã‚¬ã‚¤ãƒ‰|ã€œã™ã‚‹æ–¹æ³•|ã€œã®ã‚„ã‚Šæ–¹', text, re.IGNORECASE):
        return "ãƒã‚¦ãƒã‚¦ç³»"

    # ä½“é¨“è«‡ç³»
    if re.search(r'ç§ãŒ|åƒ•ãŒ|è‡ªåˆ†ãŒ|å®Ÿéš›ã«|ã‚„ã£ã¦ã¿ãŸ|è©¦ã—ã¦ã¿ãŸ|ä½“é¨“|çµŒé¨“|ã€œã—ãŸã‚‰|ã€œã—ã¦ã¿ãŸ', text, re.IGNORECASE):
        return "ä½“é¨“è«‡ç³»"

    # å•é¡Œæèµ·ç³»
    if re.search(r'ã¯ï¼Ÿ|å•é¡Œ|å±é™º|æ³¨æ„|è­¦å‘Š|ã€æ‚²å ±ã€‘|ã€œã™ãã‚‹|ãƒ¤ãƒã„|ãŠã‹ã—ã„', text, re.IGNORECASE):
        return "å•é¡Œæèµ·ç³»"

    # ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»
    if re.search(r'ãƒ„ãƒ¼ãƒ«|ã‚¢ãƒ—ãƒª|ã‚µãƒ¼ãƒ“ã‚¹|ãƒ—ãƒ©ã‚°ã‚¤ãƒ³|æ‹¡å¼µæ©Ÿèƒ½|ãŠã™ã™ã‚|ç´¹ä»‹|AI|Claude|ChatGPT|GPT', text, re.IGNORECASE):
        return "ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»"

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»
    if re.search(r'ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|é–‹å§‹|é–‹å‚¬|é€Ÿå ±|æœ€æ–°|ãƒ‹ãƒ¥ãƒ¼ã‚¹|å…¬é–‹', text, re.IGNORECASE):
        return "ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»"

    return "ãã®ä»–"


def analyze_categories(df):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ"""
    category_data = defaultdict(lambda: {"likes": [], "retweets": []})

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)
        retweets = safe_get(row, "ãƒªãƒã‚¹ãƒˆæ•°", 0)

        category = classify_category(text)
        category_data[category]["likes"].append(likes)
        category_data[category]["retweets"].append(retweets)

    return category_data


def parse_datetime(dt_str):
    """æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹"""
    try:
        return pd.to_datetime(dt_str)
    except:
        return None


def analyze_time(df):
    """æ™‚é–“å¸¯åˆ†æ"""
    time_slots = {
        "æœ(6-9æ™‚)": [],
        "æ˜¼(9-12æ™‚)": [],
        "åˆå¾Œ(12-18æ™‚)": [],
        "å¤œ(18-22æ™‚)": [],
        "æ·±å¤œ(22-6æ™‚)": [],
    }

    weekday_data = defaultdict(list)

    for _, row in df.iterrows():
        dt_str = safe_get(row, "æŠ•ç¨¿æ—¥æ™‚", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        dt = parse_datetime(dt_str)
        if dt:
            # JSTå¤‰æ›ï¼ˆ+9æ™‚é–“ï¼‰
            hour = (dt.hour + 9) % 24
            weekday = dt.weekday()

            if 6 <= hour < 9:
                time_slots["æœ(6-9æ™‚)"].append(likes)
            elif 9 <= hour < 12:
                time_slots["æ˜¼(9-12æ™‚)"].append(likes)
            elif 12 <= hour < 18:
                time_slots["åˆå¾Œ(12-18æ™‚)"].append(likes)
            elif 18 <= hour < 22:
                time_slots["å¤œ(18-22æ™‚)"].append(likes)
            else:
                time_slots["æ·±å¤œ(22-6æ™‚)"].append(likes)

            weekday_names = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
            weekday_data[weekday_names[weekday]].append(likes)

    return time_slots, weekday_data


def analyze_time_category_cross(df):
    """æ™‚é–“å¸¯Ã—ã‚«ãƒ†ã‚´ãƒªã®ã‚¯ãƒ­ã‚¹åˆ†æ"""
    cross_data = defaultdict(lambda: defaultdict(list))

    for _, row in df.iterrows():
        text = safe_get(row, "æœ¬æ–‡", "")
        dt_str = safe_get(row, "æŠ•ç¨¿æ—¥æ™‚", "")
        likes = safe_get(row, "ã„ã„ã­æ•°", 0)

        category = classify_category(text)
        dt = parse_datetime(dt_str)

        if dt:
            hour = (dt.hour + 9) % 24
            if 6 <= hour < 12:
                time_slot = "æœã€œæ˜¼"
            elif 12 <= hour < 18:
                time_slot = "åˆå¾Œ"
            elif 18 <= hour < 22:
                time_slot = "å¤œ"
            else:
                time_slot = "æ·±å¤œ"

            cross_data[time_slot][category].append(likes)

    return cross_data


def generate_report(df, output_filename, original_count, excluded_count):
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print("\nåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")

    with open(output_filename, "w", encoding="utf-8") as f:
        f.write("# AIå‰¯æ¥­ç³»ãƒã‚ºãƒã‚¹ãƒˆ è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ v2\n\n")
        f.write(f"**åˆ†ææ—¥æ™‚:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n\n")
        f.write(f"**å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°:** {original_count}ä»¶\n\n")
        f.write(f"**é™¤å¤–ä»¶æ•°:** {excluded_count}ä»¶\n")
        f.write(f"- ç‚ä¸Šç³»ãƒ»è‘—ä½œæ¨©å•é¡Œç³»ã®æŠ•ç¨¿ã‚’é™¤å¤–\n")
        f.write(f"- åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é‡è¤‡æŠ•ç¨¿ã‚’é™¤å¤–ï¼ˆæœ€ã‚‚ã„ã„ã­æ•°ãŒé«˜ã„1ä»¶ã®ã¿æ®‹ã™ï¼‰\n\n")
        f.write(f"**æœ€çµ‚åˆ†æå¯¾è±¡:** {len(df)}ä»¶ã®ãƒã‚¹ãƒˆ\n\n")
        f.write("---\n\n")

        # åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        print("åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’åˆ†æä¸­...")
        f.write("## 1. åŸºæœ¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n")

        f.write("### ã„ã„ã­æ•° TOP10\n\n")
        top10_likes = df.nlargest(10, "ã„ã„ã­æ•°")
        for i, (_, row) in enumerate(top10_likes.iterrows(), 1):
            f.write(f"#### {i}ä½: {row['ã„ã„ã­æ•°']:,}ã„ã„ã­\n\n")
            f.write(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼:** @{row['ãƒ¦ãƒ¼ã‚¶ãƒ¼å']}\n\n")
            f.write(f"**æœ¬æ–‡:**\n```\n{row['æœ¬æ–‡'][:200]}{'...' if len(str(row['æœ¬æ–‡'])) > 200 else ''}\n```\n\n")
            f.write(f"- ãƒªãƒã‚¹ãƒˆ: {row['ãƒªãƒã‚¹ãƒˆæ•°']:,}ä»¶\n")
            f.write(f"- URL: {row['ãƒã‚¹ãƒˆURL']}\n\n")

        f.write("### ãƒªãƒã‚¹ãƒˆæ•° TOP10\n\n")
        top10_retweets = df.nlargest(10, "ãƒªãƒã‚¹ãƒˆæ•°")
        for i, (_, row) in enumerate(top10_retweets.iterrows(), 1):
            f.write(f"{i}. **{row['ãƒªãƒã‚¹ãƒˆæ•°']:,}RT** - ã„ã„ã­{row['ã„ã„ã­æ•°']:,}ä»¶ - {row['æœ¬æ–‡'][:60]}...\n")
        f.write("\n")

        # æ§‹æˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æ
        print("æ§‹æˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’åˆ†æä¸­...")
        f.write("## 2. æ§‹æˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ†æ\n\n")

        avg_lines, top_avg_lines, bottom_avg_lines = analyze_line_breaks(df)
        f.write("### æ”¹è¡Œã®ä½¿ç”¨å‚¾å‘\n\n")
        f.write(f"- **å…¨ä½“ã®å¹³å‡æ”¹è¡Œæ•°:** {avg_lines:.1f}å›\n")
        f.write(f"- **ã„ã„ã­æ•°ä¸Šä½25%ã®å¹³å‡æ”¹è¡Œæ•°:** {top_avg_lines:.1f}å›\n")
        f.write(f"- **ã„ã„ã­æ•°ä¸‹ä½25%ã®å¹³å‡æ”¹è¡Œæ•°:** {bottom_avg_lines:.1f}å›\n\n")
        if top_avg_lines > bottom_avg_lines:
            f.write(f"**å‚¾å‘:** ãƒã‚ºã‚‹ãƒã‚¹ãƒˆã¯å¹³å‡{top_avg_lines - bottom_avg_lines:.1f}å›å¤šãæ”¹è¡Œã—ã¦ã„ã¾ã™ã€‚èª­ã¿ã‚„ã™ã•ãŒé‡è¦ã€‚\n\n")
        else:
            f.write(f"**å‚¾å‘:** ãƒã‚ºã‚‹ãƒã‚¹ãƒˆã¯æ”¹è¡ŒãŒå°‘ãªã‚ã€‚ç°¡æ½”ã•ãŒå¥½ã¾ã‚Œã‚‹å‚¾å‘ã€‚\n\n")

        bullet_count, non_bullet_count, bullet_likes, non_bullet_likes = analyze_bullet_points(df)
        f.write("### ç®‡æ¡æ›¸ãã®ä½¿ç”¨\n\n")
        f.write(f"- **ç®‡æ¡æ›¸ãã‚ã‚Š:** {bullet_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(bullet_likes)/len(bullet_likes) if bullet_likes else 0:.0f}ä»¶ï¼‰\n")
        f.write(f"- **ç®‡æ¡æ›¸ããªã—:** {non_bullet_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(non_bullet_likes)/len(non_bullet_likes) if non_bullet_likes else 0:.0f}ä»¶ï¼‰\n\n")

        symbol_usage = analyze_symbols(df)
        f.write("### è¨˜å·ã®ä½¿ç”¨å‚¾å‘\n\n")
        for symbol, count in symbol_usage.items():
            f.write(f"- **{symbol}** ã‚’ä½¿ç”¨: {count}ä»¶\n")
        f.write("\n")

        url_count, non_url_count, url_likes, non_url_likes = analyze_urls(df)
        f.write("### URL/ãƒªãƒ³ã‚¯ã®å½±éŸ¿\n\n")
        f.write(f"- **URLã‚ã‚Š:** {url_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(url_likes)/len(url_likes) if url_likes else 0:.0f}ä»¶ï¼‰\n")
        f.write(f"- **URLãªã—:** {non_url_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(non_url_likes)/len(non_url_likes) if non_url_likes else 0:.0f}ä»¶ï¼‰\n\n")

        # å¿ƒç†ãƒ»ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°åˆ†æ
        print("å¿ƒç†ãƒ»ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã‚’åˆ†æä¸­...")
        f.write("## 3. å¿ƒç†ãƒ»ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°åˆ†æ\n\n")

        pattern_data = analyze_opening_patterns(df)
        f.write("### å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®ã„ã„ã­æ•°\n\n")
        f.write("| ãƒ‘ã‚¿ãƒ¼ãƒ³ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        pattern_sorted = sorted(pattern_data.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
        for pattern, likes in pattern_sorted:
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {pattern} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")
        if pattern_sorted:
            best_pattern = pattern_sorted[0][0]
            f.write(f"**æœ€ã‚‚åŠ¹æœçš„:** {best_pattern}å‹ã®å†’é ­ãŒæœ€ã‚‚é«˜ã„ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ\n\n")

        cta_data, no_cta = analyze_cta(df)
        f.write("### CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã®åŠ¹æœ\n\n")
        f.write("| CTAç¨®é¡ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|---------|------|-------------|\n")
        for cta_type, likes in cta_data.items():
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {cta_type} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        no_cta_avg = sum(no_cta)/len(no_cta) if no_cta else 0
        f.write(f"| CTAãªã— | {len(no_cta)}ä»¶ | {no_cta_avg:.0f}ä»¶ |\n")
        f.write("\n")

        emotion_data = analyze_emotion(df)
        f.write("### æ„Ÿæƒ…åˆ¥ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ\n\n")
        f.write("| æ„Ÿæƒ… | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|------|------|-------------|\n")
        emotion_sorted = sorted(emotion_data.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
        for emotion, likes in emotion_sorted:
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {emotion} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        story_count, non_story_count, story_likes, non_story_likes = analyze_story(df)
        f.write("### ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã®æœ‰ç„¡\n\n")
        f.write(f"- **ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚ã‚Š:** {story_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(story_likes)/len(story_likes) if story_likes else 0:.0f}ä»¶ï¼‰\n")
        f.write(f"- **ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãªã—:** {non_story_count}ä»¶ï¼ˆå¹³å‡ã„ã„ã­: {sum(non_story_likes)/len(non_story_likes) if non_story_likes else 0:.0f}ä»¶ï¼‰\n\n")

        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡åˆ†æ
        print("ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡ã‚’åˆ†æä¸­...")
        f.write("## 4. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¯”ç‡åˆ†æ\n\n")

        high_reply, high_retweet = analyze_engagement_ratio(df)
        f.write("### ãƒªãƒ—ãƒ©ã‚¤ç‡ãŒé«˜ã„æŠ•ç¨¿ï¼ˆè­°è«–ãƒ»å…±æ„Ÿå‹ï¼‰\n\n")
        for item in high_reply[:5]:
            f.write(f"- **{item['likes']:,}ã„ã„ã­ / {item['replies']}ãƒªãƒ—ãƒ©ã‚¤** - {item['text']}...\n")
        f.write("\n")

        f.write("### ãƒªãƒã‚¹ãƒˆç‡ãŒé«˜ã„æŠ•ç¨¿ï¼ˆæ‹¡æ•£å‹ï¼‰\n\n")
        for item in high_retweet[:5]:
            f.write(f"- **{item['likes']:,}ã„ã„ã­ / {item['retweets']}RT** - {item['text']}...\n")
        f.write("\n")

        # ã‚«ãƒ†ã‚´ãƒªåˆ†æ
        print("ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æä¸­...")
        f.write("## 5. ãƒ†ãƒ¼ãƒãƒ»ã‚¸ãƒ£ãƒ³ãƒ«åˆ†æ\n\n")

        category_data = analyze_categories(df)
        f.write("### ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ\n\n")
        f.write("| ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° | å¹³å‡ã„ã„ã­ | å¹³å‡RT |\n")
        f.write("|---------|------|-----------|--------|\n")
        category_sorted = sorted(category_data.items(), key=lambda x: sum(x[1]["likes"])/len(x[1]["likes"]) if x[1]["likes"] else 0, reverse=True)
        for category, data in category_sorted:
            avg_likes = sum(data["likes"])/len(data["likes"]) if data["likes"] else 0
            avg_rt = sum(data["retweets"])/len(data["retweets"]) if data["retweets"] else 0
            f.write(f"| {category} | {len(data['likes'])}ä»¶ | {avg_likes:.0f}ä»¶ | {avg_rt:.0f}ä»¶ |\n")
        f.write("\n")

        # æ™‚é–“åˆ†æ
        print("æ™‚é–“å¸¯ã‚’åˆ†æä¸­...")
        f.write("## 6. æ™‚é–“ãƒ»ã‚¿ã‚¤ãƒŸãƒ³ã‚°åˆ†æ\n\n")

        time_slots, weekday_data = analyze_time(df)
        f.write("### æŠ•ç¨¿æ™‚é–“å¸¯åˆ¥ã®å¹³å‡ã„ã„ã­æ•°\n\n")
        f.write("| æ™‚é–“å¸¯ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|--------|------|-------------|\n")
        for slot, likes in time_slots.items():
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {slot} | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        f.write("### æ›œæ—¥åˆ¥ã®å¹³å‡ã„ã„ã­æ•°\n\n")
        f.write("| æ›œæ—¥ | ä»¶æ•° | å¹³å‡ã„ã„ã­æ•° |\n")
        f.write("|------|------|-------------|\n")
        for day in ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]:
            likes = weekday_data.get(day, [])
            avg = sum(likes)/len(likes) if likes else 0
            f.write(f"| {day}æ›œæ—¥ | {len(likes)}ä»¶ | {avg:.0f}ä»¶ |\n")
        f.write("\n")

        cross_data = analyze_time_category_cross(df)
        f.write("### æ™‚é–“å¸¯Ã—ã‚«ãƒ†ã‚´ãƒªã®ã‚¯ãƒ­ã‚¹åˆ†æ\n\n")
        for time_slot, categories in cross_data.items():
            f.write(f"#### {time_slot}\n\n")
            cat_sorted = sorted(categories.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True)
            for cat, likes in cat_sorted[:3]:
                avg = sum(likes)/len(likes) if likes else 0
                f.write(f"- **{cat}:** å¹³å‡{avg:.0f}ã„ã„ã­ ({len(likes)}ä»¶)\n")
            f.write("\n")

        # ç·åˆã¾ã¨ã‚
        print("ç·åˆã¾ã¨ã‚ã‚’ç”Ÿæˆä¸­...")
        f.write("## 7. AIå‰¯æ¥­ç³»ã§ãƒã‚ºã‚‹ãƒã‚¹ãƒˆã®é»„é‡‘ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n")
        f.write("å…¨åˆ†æçµæœã‚’è¸ã¾ãˆãŸã€å†ç¾æ€§ã®é«˜ã„ãƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³5é¸:\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ•°å­—æç¤ºÃ—å®Ÿç¸¾å ±å‘Š
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ•°å­—æç¤ºÃ—å®Ÿç¸¾å ±å‘Šå‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- å†’é ­ã«å…·ä½“çš„ãªæ•°å­—ã‚’æç¤º\n")
        f.write("- å®Ÿç¸¾ã‚„æˆæœã‚’æ˜ç¢ºã«ç¤ºã™\n")
        f.write("- ç®‡æ¡æ›¸ãã§æƒ…å ±ã‚’æ•´ç†\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("AIå‰¯æ¥­ã§æœˆå30ä¸‡å††é”æˆã—ã¾ã—ãŸğŸ‰\n\n")
        f.write("å®Ÿè·µã—ãŸ3ã¤ã®ã“ã¨:\n")
        f.write("â‘  ChatGPTã§è¨˜äº‹ä½œæˆä»£è¡Œ\n")
        f.write("â‘¡ Midjourneyã§ãƒ­ã‚´ãƒ‡ã‚¶ã‚¤ãƒ³\n")
        f.write("â‘¢ Claude Codeã§è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«è²©å£²\n\n")
        f.write("åˆæœˆã¯5ä¸‡å††â†’3ãƒ¶æœˆã§30ä¸‡å††ã«ã€‚\n")
        f.write("å‰¯æ¥­ã§ã‚‚ååˆ†ç¨¼ã’ã¾ã™ğŸ’ª\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å•é¡Œæèµ·Ã—å…±æ„Ÿ
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³2: å•é¡Œæèµ·Ã—å…±æ„Ÿå‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- ã€Œã¯ï¼Ÿã€ã€Œã‚„ã°ã„ã€ãªã©æ„Ÿæƒ…çš„ãªå†’é ­\n")
        f.write("- èª­è€…ã®æ‚©ã¿ã«å…±æ„Ÿ\n")
        f.write("- è§£æ±ºç­–ã‚’æç¤º\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("ã¯ï¼ŸAIã§å‰¯æ¥­ã¨ã‹æ€ªã—ã„ã£ã¦æ€ã£ã¦ã¾ã—ãŸã€‚\n\n")
        f.write("ã§ã‚‚å®Ÿéš›ã‚„ã£ã¦ã¿ãŸã‚‰...\n")
        f.write("â†’ 1æ—¥2æ™‚é–“ã§æœˆ10ä¸‡å††ç¨¼ã’ãŸ\n")
        f.write("â†’ ã‚¹ã‚­ãƒ«ä¸è¦ã§åˆå¿ƒè€…ã§ã‚‚OK\n")
        f.write("â†’ åœ¨å®…ã§å®Œçµ\n\n")
        f.write("ãƒã‚¤ãƒˆã‚ˆã‚Šå…¨ç„¶åŠ¹ç‡ã„ã„ã€‚\n")
        f.write("ã‚‚ã£ã¨æ—©ãå§‹ã‚ã‚Œã°ã‚ˆã‹ã£ãŸğŸ˜­\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¦ãƒã‚¦Ã—ç®‡æ¡æ›¸ã
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¦ãƒã‚¦Ã—ç®‡æ¡æ›¸ãå‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- ã€Œã€œã™ã‚‹æ–¹æ³•ã€ãªã©ä¾¡å€¤æç¤º\n")
        f.write("- ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¢ºã«\n")
        f.write("- å†ç¾æ€§ã‚’å¼·èª¿\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("åˆå¿ƒè€…ãŒAIå‰¯æ¥­ã§æœˆ5ä¸‡å††ç¨¼ãæ–¹æ³•\n\n")
        f.write("ã€ã‚¹ãƒ†ãƒƒãƒ—ã€‘\n")
        f.write("1. ChatGPTã«ç„¡æ–™ç™»éŒ²\n")
        f.write("2. ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¯ãƒ¼ã‚¯ã‚¹ã§ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ¡ˆä»¶æ¢ã™\n")
        f.write("3. AIã§ä¸‹æ›¸ãâ†’è‡ªåˆ†ã§ä»•ä¸Šã’\n")
        f.write("4. ç´å“ã—ã¦å ±é…¬ã‚²ãƒƒãƒˆ\n\n")
        f.write("ã“ã‚Œã ã‘ã€‚\n")
        f.write("ã‚¹ã‚­ãƒ«ã‚¼ãƒ­ã‹ã‚‰å§‹ã‚ã¦2é€±é–“ã§åˆåç›Šå‡ºã¾ã—ãŸâœ¨\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä½“é¨“è«‡Ã—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³4: ä½“é¨“è«‡Ã—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- è‡ªåˆ†ã®çµŒé¨“ã‚’æ™‚ç³»åˆ—ã§èªã‚‹\n")
        f.write("- Beforeâ†’Afterã‚’æ˜ç¢ºã«\n")
        f.write("- ãƒªã‚¢ãƒ«ãªæ•°å­—ã‚’å«ã‚ã‚‹\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("3ãƒ¶æœˆå‰: ãƒã‚¤ãƒˆæœˆ8ä¸‡å††ã§æ¶ˆè€—\n")
        f.write("2ãƒ¶æœˆå‰: AIå‰¯æ¥­é–‹å§‹â†’åˆæœˆ3ä¸‡å††\n")
        f.write("1ãƒ¶æœˆå‰: ã‚³ãƒ„æ´ã‚“ã§æœˆ12ä¸‡å††\n")
        f.write("ä»Š: ãƒã‚¤ãƒˆè¾ã‚ã¦AIå‰¯æ¥­ã®ã¿ã§æœˆ18ä¸‡å††ğŸš€\n\n")
        f.write("ä½¿ã£ã¦ã‚‹ã®ã¯ChatGPTã¨Canvaã ã‘ã€‚\n")
        f.write("äººç”Ÿå¤‰ã‚ã‚Šã¾ã—ãŸã€‚\n")
        f.write("```\n\n")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒ„ãƒ¼ãƒ«ç´¹ä»‹Ã—ç·Šæ€¥æ€§
        f.write("### ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒ„ãƒ¼ãƒ«ç´¹ä»‹Ã—ç·Šæ€¥æ€§å‹\n\n")
        f.write("**ç‰¹å¾´:**\n")
        f.write("- ã€Œä»Šã™ãã€ã€Œã¾ã é–“ã«åˆã†ã€ãªã©ç·Šæ€¥æ€§\n")
        f.write("- å…·ä½“çš„ãªãƒ„ãƒ¼ãƒ«å\n")
        f.write("- ç°¡æ½”ã«ãƒ¡ãƒªãƒƒãƒˆæç¤º\n\n")
        f.write("**æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**\n")
        f.write("```\n")
        f.write("Claude Codeã€ã¾ã ä½¿ã£ã¦ãªã„äººã¯æã—ã¦ã¾ã™ã€‚\n\n")
        f.write("ã“ã‚Œ1ã¤ã§:\n")
        f.write("ãƒ»ã‚³ãƒ¼ãƒ‰è‡ªå‹•ç”Ÿæˆ\n")
        f.write("ãƒ»ãƒã‚°ä¿®æ­£ã‚‚ç§’é€Ÿ\n")
        f.write("ãƒ»ãƒ„ãƒ¼ãƒ«é–‹ç™ºãŒçˆ†é€ŸåŒ–\n\n")
        f.write("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã§ã‚‚\n")
        f.write("Webã‚¢ãƒ—ãƒªä½œã‚Œã‚‹ãƒ¬ãƒ™ãƒ«ã€‚\n\n")
        f.write("ã¿ã‚“ãªãŒæ°—ã¥ãå‰ã«ä½¿ã„å€’ã™ã¹ãğŸ”¥\n")
        f.write("```\n\n")

        f.write("## ã¾ã¨ã‚\n\n")
        f.write("### ãƒã‚ºã‚‹æŠ•ç¨¿ã®å¿…é ˆè¦ç´ \n\n")
        f.write("1. **å†’é ­ã§å¿ƒã‚’æ´ã‚€** - æ•°å­—ã€ç–‘å•ã€ç…½ã‚Šã€å…±æ„Ÿã®ã„ãšã‚Œã‹ã§é–‹å§‹\n")
        f.write("2. **å…·ä½“çš„ãªæ•°å­—** - ã€Œæœˆ30ä¸‡å††ã€ã€Œ3ãƒ¶æœˆã€ãªã©æ˜ç¢ºãªå®Ÿç¸¾\n")
        f.write("3. **èª­ã¿ã‚„ã™ã•** - æ”¹è¡Œãƒ»ç®‡æ¡æ›¸ããƒ»çµµæ–‡å­—ã§è¦–è¦šçš„ã«æ•´ç†\n")
        f.write("4. **å†ç¾æ€§** - ã€Œè‡ªåˆ†ã«ã‚‚ã§ããã†ã€ã¨æ€ã‚ã›ã‚‹\n")
        f.write("5. **æ„Ÿæƒ…ã‚’åˆºæ¿€** - æœŸå¾…ãƒ»é©šããƒ»å…±æ„Ÿã®ã„ãšã‚Œã‹ã‚’å«ã‚ã‚‹\n\n")
        f.write("ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãŒå°‘ãªãã¦ã‚‚ãƒã‚ºã‚‹å¯èƒ½æ€§ãŒé«˜ã¾ã‚Šã¾ã™ã€‚\n")

    print(f"\nãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_filename}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    input_file = "output/buzz_posts_20260215.xlsx"
    today = datetime.now().strftime("%Y%m%d")
    output_file = f"output/analyze_report_è©³ç´°_v2_{today}.md"

    if not os.path.exists(input_file):
        print(f"ã‚¨ãƒ©ãƒ¼: {input_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"åˆ†æå¯¾è±¡: {input_file}")
    df = load_excel(input_file)

    if df is None or len(df) == 0:
        print("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    df_filtered, original_count, excluded_count = filter_data(df)

    if len(df_filtered) == 0:
        print("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    generate_report(df_filtered, output_file, original_count, excluded_count)


if __name__ == "__main__":
    main()
