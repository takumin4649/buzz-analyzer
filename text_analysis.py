"""ãƒã‚ºãƒã‚¹ãƒˆ ãƒ†ã‚­ã‚¹ãƒˆè©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime

import pandas as pd


def load_and_filter(filepath):
    """Excelã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿76ä»¶ã‚’è¿”ã™"""
    df = pd.read_excel(filepath)
    print(f"èª­ã¿è¾¼ã¿: {len(df)}ä»¶")

    # ç‚ä¸Šç³»é™¤å¤–
    exclude_keywords = [
        "è‘—ä½œæ¨©", "ç‰ˆæ¨©", "æµ·è³Šç‰ˆ", "åç›ŠåŒ–åœæ­¢", "åç›ŠåŒ–ãŒåœæ­¢",
        "å‰¥å¥ª", "ä¾µå®³", "ã‚¤ãƒ³ãƒ—ãƒ¬ã‚¾ãƒ³ãƒ“"
    ]
    def should_exclude(text):
        for kw in exclude_keywords:
            if kw in str(text):
                return True
        return False
    df = df[~df["æœ¬æ–‡"].apply(should_exclude)].copy()

    # åŒä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼é‡è¤‡é™¤å»
    df = df.sort_values("ã„ã„ã­æ•°", ascending=False)
    df = df.drop_duplicates(subset=["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"], keep="first")
    print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(df)}ä»¶")
    return df.reset_index(drop=True)


def extract_opening_phrases(df):
    """1. å†’é ­ãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡"""
    categories = defaultdict(list)

    for _, row in df.iterrows():
        text = str(row["æœ¬æ–‡"]).strip()
        first_line = text.split("\n")[0].strip()
        likes = row["ã„ã„ã­æ•°"]

        # åˆ†é¡
        if re.search(r"[\?ï¼Ÿ]", first_line):
            cat = "ç–‘å•ãƒ»å•ã„ã‹ã‘å‹"
        elif re.search(r"[!ï¼]{1,}", first_line) and len(first_line) < 30:
            cat = "ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçŸ­æ–‡å‹"
        elif re.search(r"\d+[é¸ã¤å€‹ä¸‡å††%]", first_line):
            cat = "æ•°å­—ãƒªã‚¹ãƒˆå‹"
        elif re.search(r"(çŸ¥ã‚‰ãªã„|çŸ¥ã‚‰ãªã‹ã£ãŸ|ã¾ã |å®Ÿã¯|ã¶ã£ã¡ã‚ƒã‘|æ­£ç›´|ã‚¬ãƒã§|ãƒã‚¸ã§)", first_line):
            cat = "ç§˜åŒ¿ãƒ»è¡æ’ƒäº‹å®Ÿå‹"
        elif re.search(r"(ã‚„ã‚|ã™ã‚‹ãª|ãƒ€ãƒ¡|ç¦æ­¢|æ³¨æ„|å±é™º|ãƒ¤ãƒã„|ã‚„ã°ã„)", first_line):
            cat = "è­¦å‘Šãƒ»å¦å®šå‹"
        elif re.search(r"(æ–¹æ³•|ã‚„ã‚Šæ–¹|ã‚³ãƒ„|ã‚¹ãƒ†ãƒƒãƒ—|æ‰‹é †|å§‹ã‚æ–¹|ç¨¼ãæ–¹|ç¨¼ã’ã‚‹)", first_line):
            cat = "ãƒã‚¦ãƒã‚¦æç¤ºå‹"
        elif re.search(r"(ã“ã‚Œ|ã“ã®|ã‚ã®|ã‚ã‚Œ)", first_line) and len(first_line) < 30:
            cat = "æŒ‡ç¤ºèªãƒ•ãƒƒã‚¯å‹"
        elif re.search(r"(åƒ•|ç§|ä¿º|è‡ªåˆ†|ãƒ¯ã‚¤)", first_line):
            cat = "ä½“é¨“è«‡ãƒ»è‡ªå·±é–‹ç¤ºå‹"
        elif re.search(r"(ãŠã™ã™ã‚|æœ€å¼·|ç¥|ä¾¿åˆ©|ç„¡æ–™|0å††)", first_line):
            cat = "æ¨è–¦ãƒ»çµ¶è³›å‹"
        else:
            cat = "ãã®ä»–"

        categories[cat].append({
            "first_line": first_line[:80],
            "likes": likes
        })

    return categories


def classify_structure(df):
    """2. æ–‡ç« æ§‹æˆã®å‹ã‚’å…¨æŠ•ç¨¿ã§åˆ†é¡"""
    structures = defaultdict(list)

    for _, row in df.iterrows():
        text = str(row["æœ¬æ–‡"]).strip()
        likes = row["ã„ã„ã­æ•°"]
        lines = [l.strip() for l in text.split("\n") if l.strip()]

        has_url = bool(re.search(r"https?://", text))
        has_list = bool(re.search(r"^[ãƒ»\-ãƒ»â–¶â–¸âœ…â˜‘âœ“â—†â– â—â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\d+[\.\)ï¼‰]]", text, re.MULTILINE))
        has_question = bool(re.search(r"[\?ï¼Ÿ]", lines[0] if lines else ""))
        has_cta = bool(re.search(r"(ãƒ•ã‚©ãƒ­ãƒ¼|ã„ã„ã­|ãƒªãƒ—|RT|ä¿å­˜|ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ|æ‹¡æ•£|ã‚·ã‚§ã‚¢|ãƒ–ã‚¯ãƒ|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯|ãƒ—ãƒ­ãƒ•|å›ºãƒ„ã‚¤|ãƒªãƒ³ã‚¯|è©³ç´°|ç¶šã)", text))
        has_self_story = bool(re.search(r"(åƒ•ã¯|ç§ã¯|ä¿ºã¯|è‡ªåˆ†ã¯|ãƒ¯ã‚¤ã¯|ã€œã—ãŸ|ã€œã ã£ãŸ|ã€œã—ã¦ãŸ|çµŒé¨“|ä½“é¨“)", text))

        if has_list and has_url:
            struct = "ãƒªã‚¹ãƒˆå‹ â†’ URLèª˜å°"
        elif has_list and has_cta:
            struct = "ãƒªã‚¹ãƒˆå‹ â†’ CTA"
        elif has_list:
            struct = "ãƒªã‚¹ãƒˆå‹ï¼ˆå˜ç‹¬ï¼‰"
        elif has_question and has_cta:
            struct = "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ CTA"
        elif has_question and has_url:
            struct = "å•é¡Œæèµ· â†’ è§£æ±ºç­– â†’ URL"
        elif has_question:
            struct = "å•é¡Œæèµ· â†’ å›ç­”"
        elif has_self_story and has_cta:
            struct = "ä½“é¨“è«‡ â†’ æ•™è¨“ â†’ CTA"
        elif has_self_story and has_url:
            struct = "ä½“é¨“è«‡ â†’ URLèª˜å°"
        elif has_self_story:
            struct = "ä½“é¨“è«‡ â†’ æ•™è¨“"
        elif has_url and has_cta:
            struct = "ä¸»å¼µ â†’ URL + CTA"
        elif has_url:
            struct = "ä¸»å¼µ â†’ URLèª˜å°"
        elif has_cta:
            struct = "ä¸»å¼µ â†’ CTA"
        else:
            struct = "ä¸»å¼µã®ã¿ï¼ˆçŸ­æ–‡å®Œçµï¼‰"

        structures[struct].append({
            "first_line": lines[0][:60] if lines else "",
            "likes": likes
        })

    return structures


def extract_cta_phrases(df):
    """3. CTAãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è¦§ã®æŠ½å‡º"""
    cta_patterns = [
        (r"ãƒ•ã‚©ãƒ­ãƒ¼[ã—ã¦ãã ã•ã„ã—ã‚ã‚ˆã‚ã—ããŠé¡˜ã„]*", "ãƒ•ã‚©ãƒ­ãƒ¼ç³»"),
        (r"ã„ã„ã­[ã—ã¦ãã ã•ã„ã—ã‚ãŠé¡˜ã„]*", "ã„ã„ã­ç³»"),
        (r"(ãƒªãƒ—|ãƒªãƒ—ãƒ©ã‚¤|ã‚³ãƒ¡ãƒ³ãƒˆ)[ã—ã¦ãã ã•ã„ã—ã‚ãŠé¡˜ã„æ¬„ã§]*", "ãƒªãƒ—ãƒ©ã‚¤ç³»"),
        (r"(RT|ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ|ãƒªãƒã‚¹ãƒˆ|æ‹¡æ•£)[ã—ã¦ãã ã•ã„ã—ã‚ãŠé¡˜ã„]*", "RTãƒ»æ‹¡æ•£ç³»"),
        (r"(ä¿å­˜|ãƒ–ã‚¯ãƒ|ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯)[ã—ã¦ãã ã•ã„ã—ã‚ãŠé¡˜ã„æ¨å¥¨]*", "ä¿å­˜ç³»"),
        (r"(ãƒ—ãƒ­ãƒ•|ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«)[ã‹ã‚‰è¦‹ã¦ãƒã‚§ãƒƒã‚¯ç¢ºèª]*", "ãƒ—ãƒ­ãƒ•èª˜å°ç³»"),
        (r"(å›ºãƒ„ã‚¤|å›ºå®šãƒ„ã‚¤ãƒ¼ãƒˆ)[ã‹ã‚‰è¦‹ã¦ãƒã‚§ãƒƒã‚¯ç¢ºèª]*", "å›ºãƒ„ã‚¤èª˜å°ç³»"),
        (r"(ãƒªãƒ³ã‚¯|URL|è©³ç´°|ç¶šã|ã“ã¡ã‚‰).{0,10}(ã‹ã‚‰|ã¯|ã‚’|ã«|ã§|ğŸ‘‡|â†“|â¬‡)", "ãƒªãƒ³ã‚¯èª˜å°ç³»"),
        (r"(ç„¡æ–™|0å††|ã‚¿ãƒ€).{0,20}(é…å¸ƒ|ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ|ã‚ã’|DM)", "ç„¡æ–™é…å¸ƒç³»"),
        (r"(DM|ãƒ‡ã‚£ãƒ¼ã‚¨ãƒ ).{0,10}(ãã ã•ã„|ãã‚Œ|ã—ã¦|é€)", "DMèª˜å°ç³»"),
    ]

    results = defaultdict(list)

    for _, row in df.iterrows():
        text = str(row["æœ¬æ–‡"]).strip()
        likes = row["ã„ã„ã­æ•°"]

        for pattern, category in cta_patterns:
            matches = re.findall(pattern, text)
            if matches:
                # å®Ÿéš›ã®CTAæ–‡è„ˆã‚’æŠ½å‡ºï¼ˆå‰å¾Œå«ã‚€ï¼‰
                for m in re.finditer(pattern, text):
                    start = max(0, m.start() - 15)
                    end = min(len(text), m.end() + 15)
                    context = text[start:end].replace("\n", " ").strip()
                    results[category].append({
                        "phrase": context,
                        "likes": likes
                    })

    return results


def extract_frequent_keywords(df, top_n=20):
    """4. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ•ãƒ¬ãƒ¼ã‚ºTOP20"""
    # å˜èªãƒ¬ãƒ™ãƒ«ï¼ˆ2-6æ–‡å­—ã®ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãƒ•ãƒ¬ãƒ¼ã‚ºï¼‰
    all_text = " ".join(df["æœ¬æ–‡"].astype(str).tolist())

    # ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºï¼ˆã‚ˆãä½¿ã‚ã‚Œã‚‹æ„å‘³ã®ã‚ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºï¼‰
    # ã‚«ã‚¿ã‚«ãƒŠèª
    katakana = re.findall(r"[ã‚¡-ãƒ¶ãƒ¼]{3,}", all_text)
    # æ¼¢å­—ãƒ•ãƒ¬ãƒ¼ã‚º
    kanji = re.findall(r"[ä¸€-é¾¥]{2,6}", all_text)
    # è‹±æ•°å­—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    english = re.findall(r"[A-Za-z]{3,}", all_text)

    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
    stop_words = {
        "ã™ã‚‹", "ã§ãã‚‹", "ã‚ã‚‹", "ã„ã‚‹", "ãªã‚‹", "ã‚„ã‚‹", "ãã‚‹", "ã‚‚ã®", "ã“ã¨",
        "ãã‚Œ", "ã“ã‚Œ", "ã‚ã‚Œ", "ãŸã‚", "ã‚ˆã†", "ã¨ã", "ã¨ã“ã‚", "ã»ã†",
        "ã¦ã‚‹", "ã—ã¦ã„ã‚‹", "ã—ãŸ", "ã—ã¦", "ã‹ã‚‰", "ã¾ã§", "ã®ã§", "ã‘ã©",
        "ã¨ã„ã†", "ã¦ã„ã‚‹", "ãŸã¡", "ãªã©", "ã£ã¦", "ãªã„", "ã¾ã›ã‚“",
        "https", "http", "www", "com", "the", "and", "for", "you",
    }

    all_words = katakana + kanji + english
    filtered = [w for w in all_words if w not in stop_words and len(w) >= 2]
    counter = Counter(filtered)

    # ãƒã‚ºæ–‡è„ˆã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆ2-3èªã®è¤‡åˆï¼‰
    phrase_patterns = [
        r"AIå‰¯æ¥­", r"å‰¯æ¥­[ã§æœˆ]", r"æœˆ[0-9ï¼-ï¼™]+ä¸‡",
        r"[0-9ï¼-ï¼™]+ä¸‡å††", r"ChatGPT", r"AI[ã§ã‚’ä½¿]",
        r"å®Œå…¨ç„¡æ–™", r"çŸ¥ã‚‰ãªã„ã¨æ", r"ä»Šã™ã",
        r"å‰¯æ¥­åˆå¿ƒè€…", r"ç¨¼[ã„ã’ã]", r"åç›ŠåŒ–",
        r"è‡ªå‹•åŒ–", r"ä¸åŠ´æ‰€å¾—", r"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è²©å£²",
        r"SNSé‹ç”¨", r"å€‹äººã§ç¨¼", r"è„±ã‚µãƒ©",
        r"AIæ™‚ä»£", r"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ä¸è¦", r"ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
    ]
    phrase_counter = Counter()
    for pattern in phrase_patterns:
        count = len(re.findall(pattern, all_text))
        if count > 0:
            phrase_counter[pattern.replace(r"[ã§ã‚’ä½¿]", "æ´»ç”¨").replace(r"[ã„ã’ã]", "ã").replace(r"[0-9ï¼-ï¼™]+", "N")] = count

    return counter.most_common(top_n), phrase_counter.most_common(20)


def decompose_top_posts(df, top_n=5):
    """5. ã„ã„ã­æ•°TOP5ã®æŠ•ç¨¿ã‚’æ§‹é€ åˆ†è§£"""
    top_df = df.nlargest(top_n, "ã„ã„ã­æ•°")
    results = []

    for _, row in top_df.iterrows():
        text = str(row["æœ¬æ–‡"]).strip()
        likes = row["ã„ã„ã­æ•°"]
        rts = row["ãƒªãƒã‚¹ãƒˆæ•°"]
        replies = row["ãƒªãƒ—ãƒ©ã‚¤æ•°"]
        user = row["ãƒ¦ãƒ¼ã‚¶ãƒ¼å"]
        url = str(row.get("ãƒã‚¹ãƒˆURL", ""))

        lines = [l.strip() for l in text.split("\n") if l.strip()]

        # å†’é ­ï¼ˆ1-2è¡Œç›®ï¼‰
        opening = lines[0] if lines else ""

        # URLæŠ½å‡º
        urls_in_text = re.findall(r"https?://\S+", text)

        # CTAæŠ½å‡ºï¼ˆæœ€å¾Œã®æ–¹ã®è¡Œã‹ã‚‰ï¼‰
        cta_lines = []
        for line in reversed(lines):
            if re.search(r"(ãƒ•ã‚©ãƒ­ãƒ¼|ã„ã„ã­|ãƒªãƒ—|RT|ä¿å­˜|æ‹¡æ•£|ãƒ—ãƒ­ãƒ•|å›ºãƒ„ã‚¤|ãƒªãƒ³ã‚¯|DM|ğŸ‘‡|â†“|â¬‡|è©³ç´°|ç¶šã|ã“ã¡ã‚‰|ãƒ–ã‚¯ãƒ|ã‚·ã‚§ã‚¢)", line):
                cta_lines.insert(0, line)
            elif urls_in_text and any(u in line for u in urls_in_text):
                continue  # URLè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
            else:
                break

        # æœ¬æ–‡ï¼ˆå†’é ­ã¨CTAã‚’é™¤ã„ãŸä¸­é–“éƒ¨åˆ†ï¼‰
        body_start = 1
        body_end = len(lines) - len(cta_lines)
        body_lines = lines[body_start:body_end]
        # URLè¡Œã‚’é™¤å¤–
        body_lines = [l for l in body_lines if not re.match(r"^https?://", l)]

        results.append({
            "rank": len(results) + 1,
            "likes": likes,
            "rts": rts,
            "replies": replies,
            "user": user,
            "url": url,
            "opening": opening,
            "body": "\n".join(body_lines) if body_lines else "ï¼ˆå†’é ­ã«é›†ç´„ï¼‰",
            "cta": "\n".join(cta_lines) if cta_lines else "ï¼ˆCTAãªã—ï¼‰",
            "urls": urls_in_text,
            "full_text": text
        })

    return results


def generate_report(df, output_path):
    """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    lines = []
    lines.append("# ãƒã‚ºãƒã‚¹ãƒˆ ãƒ†ã‚­ã‚¹ãƒˆè©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append(f"\n**åˆ†ææ—¥**: {datetime.now().strftime('%Y-%m-%d')}")
    lines.append(f"**åˆ†æå¯¾è±¡**: {len(df)}ä»¶ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒã‚¹ãƒˆ")
    lines.append(f"**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: buzz_posts_20260215.xlsx")
    lines.append("")

    # ===== 1. å†’é ­ãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³ =====
    print("1. å†’é ­ãƒ•ãƒ¬ãƒ¼ã‚ºåˆ†æä¸­...")
    categories = extract_opening_phrases(df)
    lines.append("---")
    lines.append("## 1. å†’é ­ãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡")
    lines.append("")
    lines.append("æŠ•ç¨¿ã®æ›¸ãå‡ºã—ï¼ˆ1è¡Œç›®ï¼‰ã‚’åˆ†é¡ã—ã€å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿä¾‹ã¨ã„ã„ã­æ•°ã‚’ç¤ºã™ã€‚")
    lines.append("")

    # ã„ã„ã­å¹³å‡ã§é™é †ã‚½ãƒ¼ãƒˆ
    cat_stats = []
    for cat, items in categories.items():
        avg_likes = sum(i["likes"] for i in items) / len(items)
        cat_stats.append((cat, items, avg_likes))
    cat_stats.sort(key=lambda x: x[2], reverse=True)

    for cat, items, avg_likes in cat_stats:
        lines.append(f"### {cat}ï¼ˆ{len(items)}ä»¶ / å¹³å‡ã„ã„ã­: {avg_likes:.0f}ï¼‰")
        lines.append("")
        # ã„ã„ã­æ•°é †ã§TOP5ã‚’è¡¨ç¤º
        sorted_items = sorted(items, key=lambda x: x["likes"], reverse=True)
        for item in sorted_items[:5]:
            lines.append(f"- **{item['likes']}ã„ã„ã­**: ã€Œ{item['first_line']}ã€")
        if len(items) > 5:
            lines.append(f"- *...ä»–{len(items)-5}ä»¶*")
        lines.append("")

    # ===== 2. æ–‡ç« æ§‹æˆã®å‹ =====
    print("2. æ–‡ç« æ§‹æˆåˆ†æä¸­...")
    structures = classify_structure(df)
    lines.append("---")
    lines.append("## 2. æ–‡ç« æ§‹æˆã®å‹ï¼ˆå…¨æŠ•ç¨¿åˆ†é¡ï¼‰")
    lines.append("")

    struct_stats = []
    for struct, items in structures.items():
        avg_likes = sum(i["likes"] for i in items) / len(items)
        struct_stats.append((struct, items, avg_likes))
    struct_stats.sort(key=lambda x: len(x[1]), reverse=True)

    lines.append("| æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ | ä»¶æ•° | å‰²åˆ | å¹³å‡ã„ã„ã­ | æœ€é«˜ã„ã„ã­ |")
    lines.append("|:---|---:|---:|---:|---:|")
    for struct, items, avg_likes in struct_stats:
        max_likes = max(i["likes"] for i in items)
        pct = len(items) / len(df) * 100
        lines.append(f"| {struct} | {len(items)} | {pct:.1f}% | {avg_likes:.0f} | {max_likes} |")
    lines.append("")

    # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä»£è¡¨ä¾‹
    lines.append("### å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä»£è¡¨ä¾‹ï¼ˆã„ã„ã­æ•°TOPï¼‰")
    lines.append("")
    for struct, items, avg_likes in struct_stats:
        best = max(items, key=lambda x: x["likes"])
        lines.append(f"- **{struct}**: ã€Œ{best['first_line']}...ã€ï¼ˆ{best['likes']}ã„ã„ã­ï¼‰")
    lines.append("")

    # ===== 3. CTAãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è¦§ =====
    print("3. CTAãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºä¸­...")
    cta_data = extract_cta_phrases(df)
    lines.append("---")
    lines.append("## 3. CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è¦§")
    lines.append("")

    cta_stats = [(cat, items) for cat, items in cta_data.items()]
    cta_stats.sort(key=lambda x: len(x[1]), reverse=True)

    for cat, items in cta_stats:
        avg_likes = sum(i["likes"] for i in items) / len(items) if items else 0
        lines.append(f"### {cat}ï¼ˆå‡ºç¾{len(items)}å› / å¹³å‡ã„ã„ã­: {avg_likes:.0f}ï¼‰")
        lines.append("")
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’è¡¨ç¤º
        seen = set()
        unique_items = []
        for item in sorted(items, key=lambda x: x["likes"], reverse=True):
            phrase_key = item["phrase"][:20]
            if phrase_key not in seen:
                seen.add(phrase_key)
                unique_items.append(item)
        for item in unique_items[:8]:
            lines.append(f"- ã€Œ{item['phrase']}ã€ï¼ˆ{item['likes']}ã„ã„ã­ï¼‰")
        lines.append("")

    # CTAãªã—ã®æŠ•ç¨¿æ•°
    posts_with_cta = set()
    for cat, items in cta_data.items():
        for item in items:
            posts_with_cta.add(item["likes"])  # ã„ã„ã­æ•°ã§è­˜åˆ¥ï¼ˆç°¡æ˜“ï¼‰

    lines.append(f"**CTAã‚’å«ã‚€æŠ•ç¨¿ç‡**: æ¦‚ç®—{min(len(posts_with_cta), len(df))}/{len(df)}ä»¶")
    lines.append("")

    # ===== 4. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰TOP20 =====
    print("4. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é›†è¨ˆä¸­...")
    word_top, phrase_top = extract_frequent_keywords(df, top_n=20)
    lines.append("---")
    lines.append("## 4. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ•ãƒ¬ãƒ¼ã‚º TOP20")
    lines.append("")

    lines.append("### å˜èªãƒ¬ãƒ™ãƒ« TOP20")
    lines.append("")
    lines.append("| é †ä½ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | å‡ºç¾å›æ•° |")
    lines.append("|---:|:---|---:|")
    for i, (word, count) in enumerate(word_top, 1):
        lines.append(f"| {i} | {word} | {count} |")
    lines.append("")

    lines.append("### ãƒã‚ºæ–‡è„ˆãƒ•ãƒ¬ãƒ¼ã‚º")
    lines.append("")
    if phrase_top:
        lines.append("| ãƒ•ãƒ¬ãƒ¼ã‚º | å‡ºç¾å›æ•° |")
        lines.append("|:---|---:|")
        for phrase, count in phrase_top:
            lines.append(f"| {phrase} | {count} |")
    else:
        lines.append("ï¼ˆè©²å½“ãƒ•ãƒ¬ãƒ¼ã‚ºãªã—ï¼‰")
    lines.append("")

    # ===== 5. ã„ã„ã­TOP5ã®æ§‹é€ åˆ†è§£ =====
    print("5. TOP5æŠ•ç¨¿ã®æ§‹é€ åˆ†è§£ä¸­...")
    top_posts = decompose_top_posts(df, top_n=5)
    lines.append("---")
    lines.append("## 5. ã„ã„ã­æ•° TOP5 æŠ•ç¨¿ã®æ§‹é€ åˆ†è§£")
    lines.append("")

    for post in top_posts:
        lines.append(f"### ç¬¬{post['rank']}ä½: {post['likes']}ã„ã„ã­ / {post['rts']}RT / {post['replies']}ãƒªãƒ—")
        lines.append(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼**: @{post['user']}")
        if post['url']:
            lines.append(f"**URL**: {post['url']}")
        lines.append("")

        lines.append("#### ã€å†’é ­ï¼ˆãƒ•ãƒƒã‚¯ï¼‰ã€‘")
        lines.append(f"> {post['opening']}")
        lines.append("")

        lines.append("#### ã€æœ¬æ–‡ã€‘")
        lines.append("```")
        lines.append(post['body'])
        lines.append("```")
        lines.append("")

        lines.append("#### ã€CTAï¼ˆè¡Œå‹•å–šèµ·ï¼‰ã€‘")
        lines.append(f"> {post['cta']}")
        lines.append("")

        if post['urls']:
            lines.append("#### ã€URLã€‘")
            for u in post['urls']:
                lines.append(f"- {u}")
            lines.append("")

        lines.append("#### ã€å…¨æ–‡ã€‘")
        lines.append("<details><summary>ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹</summary>")
        lines.append("")
        lines.append("```")
        lines.append(post['full_text'])
        lines.append("```")
        lines.append("</details>")
        lines.append("")
        lines.append("---")
        lines.append("")

    # ã¾ã¨ã‚
    lines.append("## ã¾ã¨ã‚ï¼šãƒã‚ºã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æ³•å‰‡")
    lines.append("")

    # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‹è€…
    best_opening = cat_stats[0] if cat_stats else None
    if best_opening:
        lines.append(f"1. **æœ€å¼·ã®å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³**: ã€Œ{best_opening[0]}ã€ï¼ˆå¹³å‡{best_opening[2]:.0f}ã„ã„ã­ï¼‰")

    # æ§‹æˆã®å‹è€…
    best_struct_items = [(s, items, avg) for s, items, avg in struct_stats if len(items) >= 3]
    if best_struct_items:
        best_struct_items.sort(key=lambda x: x[2], reverse=True)
        lines.append(f"2. **æœ€å¼·ã®æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³**: ã€Œ{best_struct_items[0][0]}ã€ï¼ˆå¹³å‡{best_struct_items[0][2]:.0f}ã„ã„ã­ / {len(best_struct_items[0][1])}ä»¶ï¼‰")

    # CTA
    if cta_stats:
        best_cta = max(cta_stats, key=lambda x: sum(i["likes"] for i in x[1]) / len(x[1]) if x[1] else 0)
        avg = sum(i["likes"] for i in best_cta[1]) / len(best_cta[1]) if best_cta[1] else 0
        lines.append(f"3. **æœ€ã‚‚åŠ¹æœçš„ãªCTA**: ã€Œ{best_cta[0]}ã€ï¼ˆå¹³å‡{avg:.0f}ã„ã„ã­ï¼‰")

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if word_top:
        top3_words = [w[0] for w in word_top[:3]]
        lines.append(f"4. **é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰TOP3**: {', '.join(top3_words)}")

    lines.append("")

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"\nãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å®Œäº†: {output_path}")
    return output_path


if __name__ == "__main__":
    df = load_and_filter("output/buzz_posts_20260215.xlsx")
    if df is not None:
        generate_report(df, "output/buzz_text_analysis_20260217.md")
