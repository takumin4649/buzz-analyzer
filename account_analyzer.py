"""ç‰¹å®šã®Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¦ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã®å‚¾å‘ã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import os
import sys
import time
import re
from datetime import datetime, timedelta
from collections import Counter
from statistics import mean, median

import requests
from dotenv import load_dotenv
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# å¯¾è±¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆ
TARGET_ACCOUNTS = [
    "1banana2546",
    "Naoki_GPT",
    "ethann_AI",
    "maind200",
    "gyarados__AI",
    "ComagerTon79278",
    "mamiya_afi",
    "takkun_life_ea",
    "ai_nepro",
]


def fetch_user_tweets(username, api_key, count=100):
    """æŒ‡å®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã‚’å–å¾—"""
    url = "https://api.twitterapi.io/twitter/user/tweets"
    headers = {"X-API-Key": api_key}
    params = {
        "username": username,
        "count": count,
    }

    print(f"  @{username} ã®æŠ•ç¨¿ã‚’å–å¾—ä¸­...")

    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
    except requests.exceptions.ConnectionError:
        print(f"ã‚¨ãƒ©ãƒ¼: APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ï¼ˆ@{username}ï¼‰")
        return []
    except requests.exceptions.Timeout:
        print(f"ã‚¨ãƒ©ãƒ¼: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰")
        return []
    except requests.exceptions.HTTPError as e:
        if response.status_code == 401:
            print(f"ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ï¼ˆ@{username}ï¼‰")
        elif response.status_code == 429:
            print(f"ã‚¨ãƒ©ãƒ¼: APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰")
        else:
            print(f"ã‚¨ãƒ©ãƒ¼: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰: HTTP {response.status_code}")
        return []
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰: {e}")
        return []

    tweets = data.get("tweets", [])
    print(f"    â†’ {len(tweets)}ä»¶å–å¾—")
    return tweets


def classify_opening_pattern(text):
    """å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡"""
    # æ”¹è¡Œã‚„ç©ºç™½ã‚’é™¤å»ã—ã¦æœ€åˆã®æ–‡ã‚’å–å¾—
    first_line = text.strip().split("\n")[0].strip()

    # æ•°å­—æç¤ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå†’é ­ã«æ•°å­—ãŒã‚ã‚‹ï¼‰
    if re.match(r"^\d+[.ã€ã€‚:ï¼š\s]", first_line):
        return "æ•°å­—æç¤º"

    # ç–‘å•å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆï¼Ÿã§çµ‚ã‚ã‚‹ã€ã¾ãŸã¯ç–‘å•è©ã§å§‹ã¾ã‚‹ï¼‰
    if "ï¼Ÿ" in first_line or "?" in first_line or re.match(r"^(ä½•|ã©ã†|ã„ã¤|ã©ã“|èª°|ãªãœ|ã©ã®)", first_line):
        return "ç–‘å•å½¢"

    # æ–­å®šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã§ã™ã€ã ã€ã§ã‚ã‚‹ã€ã§ã™ã§çµ‚ã‚ã‚‹ï¼‰
    if re.search(r"(ã§ã™|ã |ã§ã‚ã‚‹|ã¾ã™|ã¾ã—ãŸ|ã§ã—ãŸ)[\sã€‚]*$", first_line):
        return "æ–­å®šå½¢"

    # å…±æ„Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ã‹ã‚‹ã€ã‚ã‚‹ã‚ã‚‹ã€ãã†ã€ã»ã‚“ã¨ã€ã¾ã˜ ãªã©ï¼‰
    if re.search(r"(ã‚ã‹ã‚‹|ã‚ã‚‹ã‚ã‚‹|ãã†|ã»ã‚“ã¨|ã¾ã˜|ã‚„ã°|ã™ã”|ãˆã)", first_line, re.IGNORECASE):
        return "å…±æ„Ÿ"

    return "ãã®ä»–"


def detect_cta(text):
    """CTAï¼ˆCall To Actionï¼‰ã®æœ‰ç„¡ã‚’æ¤œå‡º"""
    cta_patterns = [
        r"(ã‚„ã£ã¦|è©¦ã—ã¦|ä½¿ã£ã¦|è¦‹ã¦|èª­ã‚“ã§|ãƒã‚§ãƒƒã‚¯|ã‚¯ãƒªãƒƒã‚¯|ç™»éŒ²|ãƒ•ã‚©ãƒ­ãƒ¼|ãƒªãƒ—|RT|ã‚·ã‚§ã‚¢)",
        r"(ãã ã•ã„|ã—ã¦ã­|ã—ã‚ˆã†|ã—ã¾ã—ã‚‡ã†|ãŠã™ã™ã‚)",
        r"(ã“ã¡ã‚‰|ãƒªãƒ³ã‚¯|ãƒ—ãƒ­ãƒ•|å›ºå®š|bio)",
    ]

    for pattern in cta_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


def count_line_breaks(text):
    """æ”¹è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    return text.count("\n")


def analyze_posting_time(tweets):
    """æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘ã‚’åˆ†æ"""
    hours = []
    for tweet in tweets:
        created_at = tweet.get("createdAt", "")
        if created_at:
            try:
                # ISO 8601å½¢å¼ã®æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
                dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                # æ—¥æœ¬æ™‚é–“ã«å¤‰æ›ï¼ˆ+9æ™‚é–“ï¼‰
                dt_jst = dt + timedelta(hours=9)
                hours.append(dt_jst.hour)
            except Exception:
                continue

    if not hours:
        return {}

    # æ™‚é–“å¸¯ã‚’åˆ†é¡
    morning = sum(1 for h in hours if 6 <= h < 12)  # 6-11æ™‚
    afternoon = sum(1 for h in hours if 12 <= h < 18)  # 12-17æ™‚
    evening = sum(1 for h in hours if 18 <= h < 24)  # 18-23æ™‚
    night = sum(1 for h in hours if 0 <= h < 6)  # 0-5æ™‚

    total = len(hours)
    return {
        "æœï¼ˆ6-11æ™‚ï¼‰": f"{morning}ä»¶ ({morning/total*100:.1f}%)",
        "æ˜¼ï¼ˆ12-17æ™‚ï¼‰": f"{afternoon}ä»¶ ({afternoon/total*100:.1f}%)",
        "å¤•æ–¹ãƒ»å¤œï¼ˆ18-23æ™‚ï¼‰": f"{evening}ä»¶ ({evening/total*100:.1f}%)",
        "æ·±å¤œï¼ˆ0-5æ™‚ï¼‰": f"{night}ä»¶ ({night/total*100:.1f}%)",
    }


def calculate_posting_frequency(tweets):
    """æŠ•ç¨¿é »åº¦ã‚’è¨ˆç®—ï¼ˆ1æ—¥ã‚ãŸã‚Šä½•ä»¶ï¼‰"""
    if not tweets:
        return 0

    dates = []
    for tweet in tweets:
        created_at = tweet.get("createdAt", "")
        if created_at:
            try:
                dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                dates.append(dt.date())
            except Exception:
                continue

    if not dates:
        return 0

    # æœ€å¤ã¨æœ€æ–°ã®æ—¥ä»˜ã®å·®ã‚’è¨ˆç®—
    min_date = min(dates)
    max_date = max(dates)
    days = (max_date - min_date).days + 1

    return len(tweets) / days if days > 0 else 0


def analyze_account(username, tweets):
    """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã®åˆ†æã‚’å®Ÿè¡Œ"""
    if not tweets:
        return None

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—
    likes = [tweet.get("likeCount", 0) for tweet in tweets]
    retweets = [tweet.get("retweetCount", 0) for tweet in tweets]
    replies = [tweet.get("replyCount", 0) for tweet in tweets]

    # å„æŠ•ç¨¿ã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸ï¼ˆã„ã„ã­ + ãƒªãƒã‚¹ãƒˆ*2 + ãƒªãƒ—ãƒ©ã‚¤*3ï¼‰
    for tweet in tweets:
        tweet["engagement_score"] = (
            tweet.get("likeCount", 0) +
            tweet.get("retweetCount", 0) * 2 +
            tweet.get("replyCount", 0) * 3
        )

    # ãƒã‚ºã£ãŸæŠ•ç¨¿TOP3ã¨ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿TOP3
    sorted_tweets = sorted(tweets, key=lambda x: x["engagement_score"], reverse=True)
    top3 = sorted_tweets[:3]
    bottom3 = sorted_tweets[-3:]

    # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
    opening_patterns = [classify_opening_pattern(tweet.get("text", "")) for tweet in tweets]
    pattern_counter = Counter(opening_patterns)

    # CTAä½¿ç”¨ç‡
    cta_count = sum(1 for tweet in tweets if detect_cta(tweet.get("text", "")))
    cta_rate = cta_count / len(tweets) * 100 if tweets else 0

    # æ”¹è¡Œæ•°ã®å‚¾å‘
    line_breaks = [count_line_breaks(tweet.get("text", "")) for tweet in tweets]

    # æŠ•ç¨¿é »åº¦
    posting_freq = calculate_posting_frequency(tweets)

    # æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘
    time_distribution = analyze_posting_time(tweets)

    return {
        "username": username,
        "total_tweets": len(tweets),
        "avg_likes": mean(likes) if likes else 0,
        "median_likes": median(likes) if likes else 0,
        "avg_retweets": mean(retweets) if retweets else 0,
        "median_retweets": median(retweets) if retweets else 0,
        "avg_replies": mean(replies) if replies else 0,
        "top3_tweets": top3,
        "bottom3_tweets": bottom3,
        "opening_patterns": dict(pattern_counter),
        "cta_rate": cta_rate,
        "avg_line_breaks": mean(line_breaks) if line_breaks else 0,
        "median_line_breaks": median(line_breaks) if line_breaks else 0,
        "posting_frequency": posting_freq,
        "time_distribution": time_distribution,
    }


def format_tweet_for_markdown(tweet, rank):
    """æŠ•ç¨¿ã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    text = tweet.get("text", "").replace("\n", " ")[:100]  # æœ€åˆã®100æ–‡å­—
    likes = tweet.get("likeCount", 0)
    retweets = tweet.get("retweetCount", 0)
    replies = tweet.get("replyCount", 0)
    score = tweet.get("engagement_score", 0)

    username = tweet.get("author", {}).get("userName", "")
    tweet_id = tweet.get("id", "")
    url = f"https://x.com/{username}/status/{tweet_id}" if username and tweet_id else ""

    return f"{rank}. **{text}...** \n   ã„ã„ã­: {likes} | RT: {retweets} | ãƒªãƒ—: {replies} | ã‚¹ã‚³ã‚¢: {score}\n   [{url}]({url})\n"


def generate_markdown_report(all_analyses, output_path):
    """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    md_content = f"""# Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {today}

---

## ğŸ“Š å…¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¨ªæ–­æ¯”è¼ƒ

### ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¹³å‡ã„ã„ã­æ•°é †ï¼‰

"""

    # å¹³å‡ã„ã„ã­æ•°ã§ã‚½ãƒ¼ãƒˆ
    sorted_analyses = sorted(all_analyses, key=lambda x: x["avg_likes"], reverse=True)

    for i, analysis in enumerate(sorted_analyses, 1):
        username = analysis["username"]
        avg_likes = analysis["avg_likes"]
        avg_retweets = analysis["avg_retweets"]
        posting_freq = analysis["posting_frequency"]

        md_content += f"{i}. **@{username}**\n"
        md_content += f"   - å¹³å‡ã„ã„ã­: {avg_likes:.1f}\n"
        md_content += f"   - å¹³å‡RT: {avg_retweets:.1f}\n"
        md_content += f"   - æŠ•ç¨¿é »åº¦: {posting_freq:.2f}ä»¶/æ—¥\n\n"

    # å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    md_content += "\n### å…±é€šã—ã¦ãƒã‚ºã£ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n"

    # å…¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®TOP3æŠ•ç¨¿ã‹ã‚‰å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
    all_top_patterns = []
    for analysis in all_analyses:
        for tweet in analysis["top3_tweets"]:
            pattern = classify_opening_pattern(tweet.get("text", ""))
            all_top_patterns.append(pattern)

    top_pattern_counter = Counter(all_top_patterns)
    for pattern, count in top_pattern_counter.most_common():
        md_content += f"- **{pattern}**: {count}ä»¶\n"

    md_content += "\n---\n\n"

    # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è©³ç´°åˆ†æ
    for analysis in all_analyses:
        username = analysis["username"]

        md_content += f"## ğŸ‘¤ @{username}\n\n"

        # åŸºæœ¬çµ±è¨ˆ
        md_content += "### ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ\n\n"
        md_content += f"- **æŠ•ç¨¿æ•°**: {analysis['total_tweets']}ä»¶\n"
        md_content += f"- **å¹³å‡ã„ã„ã­æ•°**: {analysis['avg_likes']:.1f} ï¼ˆä¸­å¤®å€¤: {analysis['median_likes']:.0f}ï¼‰\n"
        md_content += f"- **å¹³å‡RTæ•°**: {analysis['avg_retweets']:.1f} ï¼ˆä¸­å¤®å€¤: {analysis['median_retweets']:.0f}ï¼‰\n"
        md_content += f"- **å¹³å‡ãƒªãƒ—ãƒ©ã‚¤æ•°**: {analysis['avg_replies']:.1f}\n"
        md_content += f"- **æŠ•ç¨¿é »åº¦**: {analysis['posting_frequency']:.2f}ä»¶/æ—¥\n\n"

        # æŠ•ç¨¿æ™‚é–“å¸¯
        md_content += "### â° æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘\n\n"
        for time_slot, value in analysis["time_distribution"].items():
            md_content += f"- {time_slot}: {value}\n"
        md_content += "\n"

        # ãƒã‚ºã£ãŸæŠ•ç¨¿TOP3
        md_content += "### ğŸ”¥ æœ€ã‚‚ãƒã‚ºã£ãŸæŠ•ç¨¿ TOP3\n\n"
        for i, tweet in enumerate(analysis["top3_tweets"], 1):
            md_content += format_tweet_for_markdown(tweet, i)
        md_content += "\n"

        # ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿TOP3
        md_content += "### ğŸ“‰ æœ€ã‚‚ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ TOP3\n\n"
        for i, tweet in enumerate(analysis["bottom3_tweets"], 1):
            md_content += format_tweet_for_markdown(tweet, i)
        md_content += "\n"

        # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        md_content += "### ğŸ¯ å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n\n"
        total_patterns = sum(analysis["opening_patterns"].values())
        for pattern, count in sorted(analysis["opening_patterns"].items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_patterns * 100 if total_patterns > 0 else 0
            md_content += f"- **{pattern}**: {count}ä»¶ ({percentage:.1f}%)\n"
        md_content += "\n"

        # CTAä½¿ç”¨ç‡
        md_content += f"### ğŸ“¢ CTAä½¿ç”¨ç‡: {analysis['cta_rate']:.1f}%\n\n"

        # æ”¹è¡Œæ•°ã®å‚¾å‘
        md_content += f"### ğŸ“ æ”¹è¡Œæ•°ã®å‚¾å‘\n\n"
        md_content += f"- **å¹³å‡æ”¹è¡Œæ•°**: {analysis['avg_line_breaks']:.1f}\n"
        md_content += f"- **ä¸­å¤®å€¤**: {analysis['median_line_breaks']:.0f}\n\n"

        # å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        md_content += "### ğŸ’¡ ã“ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n"

        # TOP3æŠ•ç¨¿ã®å…±é€šç‚¹ã‚’åˆ†æ
        top3_patterns = [classify_opening_pattern(tweet.get("text", "")) for tweet in analysis["top3_tweets"]]
        top3_pattern_counter = Counter(top3_patterns)
        most_common_pattern = top3_pattern_counter.most_common(1)[0][0] if top3_pattern_counter else "ä¸æ˜"

        top3_has_cta = sum(1 for tweet in analysis["top3_tweets"] if detect_cta(tweet.get("text", "")))

        md_content += f"- **åŠ¹æœçš„ãªå†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³**: {most_common_pattern}\n"
        md_content += f"- **TOP3ã§ã®CTAä½¿ç”¨**: {top3_has_cta}/3ä»¶\n"

        # å¹³å‡ã‚ˆã‚Šé«˜ã„ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆè¦å› 
        avg_engagement = mean([t["engagement_score"] for t in analysis["top3_tweets"]])
        md_content += f"- **TOP3å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢**: {avg_engagement:.0f}\n"

        md_content += "\n---\n\n"

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(md_content)

    print(f"Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_path}")


def save_to_excel(all_tweets_by_account, output_path):
    """å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’Excelã«ä¿å­˜ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã«ã‚·ãƒ¼ãƒˆåˆ†ã‘ï¼‰"""
    wb = Workbook()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒ¼ãƒˆã‚’å‰Šé™¤
    if "Sheet" in wb.sheetnames:
        wb.remove(wb["Sheet"])

    for username, tweets in all_tweets_by_account.items():
        # ã‚·ãƒ¼ãƒˆåï¼ˆæœ€å¤§31æ–‡å­—åˆ¶é™ï¼‰
        sheet_name = f"@{username}"[:31]
        ws = wb.create_sheet(title=sheet_name)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = ["æœ¬æ–‡", "ã„ã„ã­æ•°", "RTæ•°", "ãƒªãƒ—ãƒ©ã‚¤æ•°", "æŠ•ç¨¿æ—¥æ™‚", "æŠ•ç¨¿URL", "å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³", "CTAæœ‰ç„¡", "æ”¹è¡Œæ•°"]
        ws.append(headers)

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")

        for col_num, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col_num)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal="center", vertical="center")

        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’è¿½åŠ 
        for tweet in tweets:
            text = tweet.get("text", "")
            tweet_id = tweet.get("id", "")
            post_url = f"https://x.com/{username}/status/{tweet_id}" if tweet_id else ""

            ws.append([
                text,
                tweet.get("likeCount", 0),
                tweet.get("retweetCount", 0),
                tweet.get("replyCount", 0),
                tweet.get("createdAt", ""),
                post_url,
                classify_opening_pattern(text),
                "ã‚ã‚Š" if detect_cta(text) else "ãªã—",
                count_line_breaks(text),
            ])

        # æœ¬æ–‡åˆ—ï¼ˆAåˆ—ï¼‰ã®æŠ˜ã‚Šè¿”ã—è¨­å®š
        for row in range(2, len(tweets) + 2):
            cell = ws.cell(row=row, column=1)
            cell.alignment = Alignment(wrap_text=True, vertical="top")

        # åˆ—å¹…ã®èª¿æ•´
        column_widths = {
            1: 60,  # æœ¬æ–‡
            2: 12,  # ã„ã„ã­æ•°
            3: 12,  # RTæ•°
            4: 12,  # ãƒªãƒ—ãƒ©ã‚¤æ•°
            5: 25,  # æŠ•ç¨¿æ—¥æ™‚
            6: 50,  # æŠ•ç¨¿URL
            7: 15,  # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³
            8: 12,  # CTAæœ‰ç„¡
            9: 10,  # æ”¹è¡Œæ•°
        }

        for col_num, width in column_widths.items():
            ws.column_dimensions[get_column_letter(col_num)].width = width

        # ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
        ws.auto_filter.ref = f"A1:{get_column_letter(len(headers))}{len(tweets) + 1}"

    # ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    wb.save(output_path)
    print(f"Excelä¿å­˜å®Œäº†: {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # APIã‚­ãƒ¼å–å¾—
    api_key = os.environ.get("TWITTER_API_KEY")
    if not api_key:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° TWITTER_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("è¨­å®šä¾‹: export TWITTER_API_KEY='your-api-key'")
        sys.exit(1)

    print("=" * 60)
    print("Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print(f"å¯¾è±¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°: {len(TARGET_ACCOUNTS)}å€‹")
    print()

    # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚’å–å¾—
    all_tweets_by_account = {}
    all_analyses = []

    for i, username in enumerate(TARGET_ACCOUNTS):
        print(f"[{i+1}/{len(TARGET_ACCOUNTS)}] @{username} ã‚’å‡¦ç†ä¸­...")

        tweets = fetch_user_tweets(username, api_key, count=100)

        if tweets:
            all_tweets_by_account[username] = tweets

            # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æã‚’å®Ÿè¡Œ
            analysis = analyze_account(username, tweets)
            if analysis:
                all_analyses.append(analysis)

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼šæœ€å¾Œã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä»¥å¤–ã¯å¾…æ©Ÿ
        if i < len(TARGET_ACCOUNTS) - 1:
            print("    ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§3ç§’å¾…æ©Ÿä¸­...")
            time.sleep(3)

        print()

    if not all_analyses:
        print("åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(0)

    print(f"åˆ†æå®Œäº†: {len(all_analyses)}ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")
    print()

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    today = datetime.now().strftime("%Y%m%d")
    output_dir = "output"
    md_path = os.path.join(output_dir, f"account_analysis_{today}.md")
    excel_path = os.path.join(output_dir, f"account_posts_{today}.xlsx")

    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    generate_markdown_report(all_analyses, md_path)

    # Excelä¿å­˜
    print("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...")
    save_to_excel(all_tweets_by_account, excel_path)

    print()
    print("=" * 60)
    print("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
