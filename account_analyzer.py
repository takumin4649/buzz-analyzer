"""ç‰¹å®šã®Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¦ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã®å‚¾å‘ã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import os
import sys
import time
import re
import argparse
from datetime import datetime, timedelta
from collections import Counter
from statistics import mean, median

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

import requests
from dotenv import load_dotenv
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# å¯¾è±¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆ
TARGET_ACCOUNTS = [
    "1banana2546",
    "Naoki_GPT",
    "ethann_AI",
    "maind200",
    "gyarados__AI",
    "ComagerTon79278",
    "mamiya_afi",
    "takkun_life_ea",
    "ai_nepro",
]


def load_test_data_from_excel(excel_path):
    """ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šæ—¢å­˜ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {excel_path} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

    if not os.path.exists(excel_path):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {excel_path}")
        return {}

    wb = load_workbook(excel_path)
    ws = wb[wb.sheetnames[0]]  # æœ€åˆã®ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’èª­ã¿è¾¼ã¿
    headers = [cell.value for cell in ws[1]]

    # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæŸ”è»Ÿã«å¯¾å¿œï¼‰
    column_mapping = {}
    for i, header in enumerate(headers, 1):
        if header in ["æœ¬æ–‡", "text", "ãƒ†ã‚­ã‚¹ãƒˆ"]:
            column_mapping["text"] = i
        elif header in ["ã„ã„ã­æ•°", "likeCount", "likes"]:
            column_mapping["likeCount"] = i
        elif header in ["ãƒªãƒã‚¹ãƒˆæ•°", "RTæ•°", "retweetCount", "retweets"]:
            column_mapping["retweetCount"] = i
        elif header in ["ãƒªãƒ—ãƒ©ã‚¤æ•°", "replyCount", "replies"]:
            column_mapping["replyCount"] = i
        elif header in ["æŠ•ç¨¿æ—¥æ™‚", "createdAt", "created_at"]:
            column_mapping["createdAt"] = i
        elif header in ["ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "userName", "username", "user_name"]:
            column_mapping["userName"] = i
        elif header in ["ãƒã‚¹ãƒˆURL", "æŠ•ç¨¿URL", "url", "tweet_url"]:
            column_mapping["url"] = i

    print(f"  ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°: {column_mapping}")

    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
    tweets_by_user = {}
    total_tweets = 0

    for row in range(2, ws.max_row + 1):
        # å„ã‚«ãƒ©ãƒ ã®å€¤ã‚’å–å¾—
        text = ws.cell(row, column_mapping.get("text", 1)).value or ""
        like_count = ws.cell(row, column_mapping.get("likeCount", 2)).value or 0
        retweet_count = ws.cell(row, column_mapping.get("retweetCount", 3)).value or 0
        reply_count = ws.cell(row, column_mapping.get("replyCount", 4)).value or 0
        created_at = ws.cell(row, column_mapping.get("createdAt", 5)).value or ""
        username = ws.cell(row, column_mapping.get("userName", 6)).value or "unknown"
        post_url = ws.cell(row, column_mapping.get("url", 8)).value or ""

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æ­£è¦åŒ–ï¼ˆ@ã‚’é™¤å»ï¼‰
        if isinstance(username, str):
            username = username.lstrip("@")

        # ãƒã‚¹ãƒˆURLã‹ã‚‰IDã‚’æŠ½å‡º
        tweet_id = ""
        if post_url:
            match = re.search(r"/status/(\d+)", post_url)
            if match:
                tweet_id = match.group(1)

        # Tweetå½¢å¼ã«å¤‰æ›
        tweet = {
            "text": text,
            "likeCount": int(like_count) if isinstance(like_count, (int, float)) else 0,
            "retweetCount": int(retweet_count) if isinstance(retweet_count, (int, float)) else 0,
            "replyCount": int(reply_count) if isinstance(reply_count, (int, float)) else 0,
            "createdAt": str(created_at),
            "id": tweet_id,
            "author": {
                "userName": username,
            },
        }

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«åˆ†é¡
        if username not in tweets_by_user:
            tweets_by_user[username] = []
        tweets_by_user[username].append(tweet)
        total_tweets += 1

    print(f"  âœ… èª­ã¿è¾¼ã¿å®Œäº†: {total_tweets}ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã€{len(tweets_by_user)}ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")
    for username, tweets in tweets_by_user.items():
        print(f"     - @{username}: {len(tweets)}ä»¶")

    return tweets_by_user


def fetch_user_tweets(username, api_key, count=100):
    """æŒ‡å®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ã‚’å–å¾—"""
    url = "https://api.twitterapi.io/twitter/user/last_tweets"
    headers = {"X-API-Key": api_key}
    params = {
        "userName": username,  # æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å
        "includeReplies": False,  # ãƒªãƒ—ãƒ©ã‚¤ã‚’é™¤å¤–
    }

    print(f"  @{username} ã®æŠ•ç¨¿ã‚’å–å¾—ä¸­...")

    all_tweets = []
    cursor = None

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€å¤§countä»¶ã¾ã§å–å¾—
    while len(all_tweets) < count:
        if cursor:
            params["cursor"] = cursor

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒã‚§ãƒƒã‚¯
            if data.get("status") == "error":
                print(f"ã‚¨ãƒ©ãƒ¼: {data.get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}ï¼ˆ@{username}ï¼‰")
                return all_tweets

            # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ï¼ˆdata.data.tweetsã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
            data_content = data.get("data", {})
            tweets = data_content.get("tweets", [])
            if not tweets:
                break

            all_tweets.extend(tweets)

            # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‹ã‚‰å–å¾—ï¼‰
            has_next = data.get("has_next_page", False)
            if not has_next:
                break

            cursor = data.get("next_cursor")
            if not cursor:
                break

        except requests.exceptions.ConnectionError:
            print(f"ã‚¨ãƒ©ãƒ¼: APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ï¼ˆ@{username}ï¼‰")
            return all_tweets
        except requests.exceptions.Timeout:
            print(f"ã‚¨ãƒ©ãƒ¼: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰")
            return all_tweets
        except requests.exceptions.HTTPError as e:
            if response.status_code == 401:
                print(f"ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ï¼ˆ@{username}ï¼‰")
            elif response.status_code == 429:
                print(f"ã‚¨ãƒ©ãƒ¼: APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰")
            else:
                print(f"ã‚¨ãƒ©ãƒ¼: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰: HTTP {response.status_code}")
            return all_tweets
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆ@{username}ï¼‰: {e}")
            return all_tweets

    # å¿…è¦ãªä»¶æ•°ã«åˆ¶é™
    all_tweets = all_tweets[:count]
    print(f"    â†’ {len(all_tweets)}ä»¶å–å¾—")
    return all_tweets


def classify_opening_pattern(text):
    """å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡"""
    # æ”¹è¡Œã‚„ç©ºç™½ã‚’é™¤å»ã—ã¦æœ€åˆã®æ–‡ã‚’å–å¾—
    first_line = text.strip().split("\n")[0].strip()

    # æ•°å­—æç¤ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå†’é ­ã«æ•°å­—ãŒã‚ã‚‹ï¼‰
    if re.match(r"^\d+[.ã€ã€‚:ï¼š\s]", first_line):
        return "æ•°å­—æç¤º"

    # ç–‘å•å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆï¼Ÿã§çµ‚ã‚ã‚‹ã€ã¾ãŸã¯ç–‘å•è©ã§å§‹ã¾ã‚‹ï¼‰
    if "ï¼Ÿ" in first_line or "?" in first_line or re.match(r"^(ä½•|ã©ã†|ã„ã¤|ã©ã“|èª°|ãªãœ|ã©ã®)", first_line):
        return "ç–‘å•å½¢"

    # æ–­å®šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã§ã™ã€ã ã€ã§ã‚ã‚‹ã€ã§ã™ã§çµ‚ã‚ã‚‹ï¼‰
    if re.search(r"(ã§ã™|ã |ã§ã‚ã‚‹|ã¾ã™|ã¾ã—ãŸ|ã§ã—ãŸ)[\sã€‚]*$", first_line):
        return "æ–­å®šå½¢"

    # å…±æ„Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ã‹ã‚‹ã€ã‚ã‚‹ã‚ã‚‹ã€ãã†ã€ã»ã‚“ã¨ã€ã¾ã˜ ãªã©ï¼‰
    if re.search(r"(ã‚ã‹ã‚‹|ã‚ã‚‹ã‚ã‚‹|ãã†|ã»ã‚“ã¨|ã¾ã˜|ã‚„ã°|ã™ã”|ãˆã)", first_line, re.IGNORECASE):
        return "å…±æ„Ÿ"

    return "ãã®ä»–"


def detect_cta(text):
    """CTAï¼ˆCall To Actionï¼‰ã®æœ‰ç„¡ã‚’æ¤œå‡º"""
    cta_patterns = [
        r"(ã‚„ã£ã¦|è©¦ã—ã¦|ä½¿ã£ã¦|è¦‹ã¦|èª­ã‚“ã§|ãƒã‚§ãƒƒã‚¯|ã‚¯ãƒªãƒƒã‚¯|ç™»éŒ²|ãƒ•ã‚©ãƒ­ãƒ¼|ãƒªãƒ—|RT|ã‚·ã‚§ã‚¢)",
        r"(ãã ã•ã„|ã—ã¦ã­|ã—ã‚ˆã†|ã—ã¾ã—ã‚‡ã†|ãŠã™ã™ã‚)",
        r"(ã“ã¡ã‚‰|ãƒªãƒ³ã‚¯|ãƒ—ãƒ­ãƒ•|å›ºå®š|bio)",
    ]

    for pattern in cta_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


def analyze_emotion(text):
    """æ„Ÿæƒ…åˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
    emotions = []

    # æœŸå¾…
    if re.search(r"(æ¥½ã—ã¿|ãƒ¯ã‚¯ãƒ¯ã‚¯|æœŸå¾…|ã§ãã‚‹|å¯èƒ½|ãƒãƒ£ãƒ³ã‚¹|ã‚„ã°ã„|ã™ã”ã„|ã„ã‘ã‚‹)", text, re.IGNORECASE):
        emotions.append("æœŸå¾…")

    # é©šã
    if re.search(r"(ã³ã£ãã‚Š|é©šã|ã¾ã•ã‹|ä¿¡ã˜ã‚‰ã‚Œ|ãƒ•ã‚¡ãƒƒ|ãˆã‡|ãƒã‚¸|ã»ã‚“ã¨|!|ï¼)", text, re.IGNORECASE):
        emotions.append("é©šã")

    # å…±æ„Ÿ
    if re.search(r"(ã‚ã‹ã‚‹|ã‚ã‚‹ã‚ã‚‹|ãã†|ã»ã‚“ã¨|åŒã˜|ç§ã‚‚|ä¿ºã‚‚|ç¢ºã‹ã«)", text, re.IGNORECASE):
        emotions.append("å…±æ„Ÿ")

    # ææ€–ãƒ»ä¸å®‰
    if re.search(r"(æ€–ã„|ä¸å®‰|å¿ƒé…|å±é™º|ã‚„ã°ã„|ãƒ€ãƒ¡|å¤±æ•—)", text, re.IGNORECASE):
        emotions.append("ææ€–")

    # æ€’ã‚Šãƒ»ä¸æº€
    if re.search(r"(è…¹ç«‹|ãƒ ã‚«ã¤|ã‚¤ãƒ©ã‚¤ãƒ©|æœ€æ‚ª|ã²ã©ã„)", text, re.IGNORECASE):
        emotions.append("æ€’ã‚Š")

    return emotions if emotions else ["ä¸­ç«‹"]


def classify_theme(text):
    """ãƒ†ãƒ¼ãƒåˆ†é¡ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
    # ãƒã‚¦ãƒã‚¦ç³»
    if re.search(r"(æ–¹æ³•|ã‚„ã‚Šæ–¹|æ‰‹é †|ã‚³ãƒ„|ãƒã‚¤ãƒ³ãƒˆ|ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯|æ´»ç”¨|ä½¿ã„æ–¹|~ã™ã‚‹)", text, re.IGNORECASE):
        return "ãƒã‚¦ãƒã‚¦"

    # å®Ÿç¸¾å ±å‘Šç³»
    if re.search(r"(é”æˆ|çªç ´|è¨˜éŒ²|ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|åç›Š|å£²ä¸Š|æˆæœ|çµæœ|~ã—ãŸ|~ã§ããŸ)", text, re.IGNORECASE):
        return "å®Ÿç¸¾å ±å‘Š"

    # ä½“é¨“è«‡ç³»
    if re.search(r"(~ã—ã¦ã¿ãŸ|ä½“é¨“|çµŒé¨“|å®Ÿéš›|ã‚„ã£ã¦ã¿|è©¦ã—ã¦|ä½¿ã£ã¦)", text, re.IGNORECASE):
        return "ä½“é¨“è«‡"

    # ãƒ„ãƒ¼ãƒ«ç´¹ä»‹ç³»
    if re.search(r"(ãƒ„ãƒ¼ãƒ«|AI|ChatGPT|Claude|Grok|ã‚¢ãƒ—ãƒª|ã‚µãƒ¼ãƒ“ã‚¹|ãŠã™ã™ã‚)", text, re.IGNORECASE):
        return "ãƒ„ãƒ¼ãƒ«ç´¹ä»‹"

    # å•é¡Œæèµ·ç³»
    if re.search(r"(ãªãœ|ã©ã†ã—ã¦|å•é¡Œ|èª²é¡Œ|æ‚©ã¿|å›°ã£|\?|ï¼Ÿ)", text, re.IGNORECASE):
        return "å•é¡Œæèµ·"

    # æ—¥å¸¸ç³»
    if re.search(r"(ä»Šæ—¥|æ˜¨æ—¥|æ˜æ—¥|æœ|æ˜¼|å¤œ|ãƒ©ãƒ³ãƒ|ã”é£¯|ã‚³ãƒ¼ãƒ’ãƒ¼)", text, re.IGNORECASE):
        return "æ—¥å¸¸"

    return "ãã®ä»–"


def detect_url(text):
    """URLæœ‰ç„¡ã‚’æ¤œå‡º"""
    return bool(re.search(r"https?://", text))


def detect_special_chars(text):
    """ç‰¹æ®Šè¨˜å·ã®ä½¿ç”¨ã‚’æ¤œå‡º"""
    chars = {
        "çŸ¢å°": bool(re.search(r"[â†’â‡’â¡]", text)),
        "æ‹¬å¼§": bool(re.search(r"[ã€ã€‘ã€ã€ã€Œã€]", text)),
        "è¨˜å·": bool(re.search(r"[â˜…â˜†â—†â—‡â– â–¡â–¼â–²]", text)),
    }
    return chars


def count_chars(text):
    """æ–‡å­—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç©ºç™½ãƒ»æ”¹è¡Œé™¤ãï¼‰"""
    return len(re.sub(r"\s", "", text))


def count_line_breaks(text):
    """æ”¹è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    return text.count("\n")


def analyze_posting_time(tweets):
    """æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘ã‚’åˆ†æ"""
    hours = []
    for tweet in tweets:
        created_at = tweet.get("createdAt", "")
        if created_at:
            try:
                # ISO 8601å½¢å¼ã®æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
                dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                # æ—¥æœ¬æ™‚é–“ã«å¤‰æ›ï¼ˆ+9æ™‚é–“ï¼‰
                dt_jst = dt + timedelta(hours=9)
                hours.append(dt_jst.hour)
            except Exception:
                continue

    if not hours:
        return {}

    # æ™‚é–“å¸¯ã‚’åˆ†é¡
    morning = sum(1 for h in hours if 6 <= h < 12)  # 6-11æ™‚
    afternoon = sum(1 for h in hours if 12 <= h < 18)  # 12-17æ™‚
    evening = sum(1 for h in hours if 18 <= h < 24)  # 18-23æ™‚
    night = sum(1 for h in hours if 0 <= h < 6)  # 0-5æ™‚

    total = len(hours)
    return {
        "æœï¼ˆ6-11æ™‚ï¼‰": f"{morning}ä»¶ ({morning/total*100:.1f}%)",
        "æ˜¼ï¼ˆ12-17æ™‚ï¼‰": f"{afternoon}ä»¶ ({afternoon/total*100:.1f}%)",
        "å¤•æ–¹ãƒ»å¤œï¼ˆ18-23æ™‚ï¼‰": f"{evening}ä»¶ ({evening/total*100:.1f}%)",
        "æ·±å¤œï¼ˆ0-5æ™‚ï¼‰": f"{night}ä»¶ ({night/total*100:.1f}%)",
    }


def calculate_posting_frequency(tweets):
    """æŠ•ç¨¿é »åº¦ã‚’è¨ˆç®—ï¼ˆ1æ—¥ã‚ãŸã‚Šä½•ä»¶ï¼‰"""
    if not tweets:
        return 0

    dates = []
    for tweet in tweets:
        created_at = tweet.get("createdAt", "")
        if created_at:
            try:
                dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                dates.append(dt.date())
            except Exception:
                continue

    if not dates:
        return 0

    # æœ€å¤ã¨æœ€æ–°ã®æ—¥ä»˜ã®å·®ã‚’è¨ˆç®—
    min_date = min(dates)
    max_date = max(dates)
    days = (max_date - min_date).days + 1

    return len(tweets) / days if days > 0 else 0


def analyze_account(username, tweets):
    """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã®åˆ†æã‚’å®Ÿè¡Œ"""
    if not tweets:
        return None

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—
    likes = [tweet.get("likeCount", 0) for tweet in tweets]
    retweets = [tweet.get("retweetCount", 0) for tweet in tweets]
    replies = [tweet.get("replyCount", 0) for tweet in tweets]

    # å„æŠ•ç¨¿ã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã¨è©³ç´°åˆ†æã‚’ä»˜ä¸
    for tweet in tweets:
        text = tweet.get("text", "")
        tweet["engagement_score"] = (
            tweet.get("likeCount", 0) +
            tweet.get("retweetCount", 0) * 2 +
            tweet.get("replyCount", 0) * 3
        )
        tweet["char_count"] = count_chars(text)
        tweet["line_breaks"] = count_line_breaks(text)
        tweet["opening_pattern"] = classify_opening_pattern(text)
        tweet["has_cta"] = detect_cta(text)
        tweet["emotions"] = analyze_emotion(text)
        tweet["theme"] = classify_theme(text)
        tweet["has_url"] = detect_url(text)
        tweet["special_chars"] = detect_special_chars(text)

    # ãƒã‚ºã£ãŸæŠ•ç¨¿TOP5ã¨ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿TOP5
    sorted_tweets = sorted(tweets, key=lambda x: x["engagement_score"], reverse=True)
    top5 = sorted_tweets[:5]
    bottom5 = sorted_tweets[-5:] if len(sorted_tweets) >= 5 else sorted_tweets[:1]

    # å„ç¨®çµ±è¨ˆ
    opening_patterns = [t["opening_pattern"] for t in tweets]
    pattern_counter = Counter(opening_patterns)

    emotions_all = [e for t in tweets for e in t["emotions"]]
    emotion_counter = Counter(emotions_all)

    themes = [t["theme"] for t in tweets]
    theme_counter = Counter(themes)

    # CTAä½¿ç”¨ç‡
    cta_count = sum(1 for t in tweets if t["has_cta"])
    cta_rate = cta_count / len(tweets) * 100 if tweets else 0

    # URLä½¿ç”¨ç‡
    url_count = sum(1 for t in tweets if t["has_url"])
    url_rate = url_count / len(tweets) * 100 if tweets else 0

    # æ”¹è¡Œæ•°ã®å‚¾å‘
    line_breaks = [t["line_breaks"] for t in tweets]

    # æ–‡å­—æ•°ã®å‚¾å‘
    char_counts = [t["char_count"] for t in tweets]

    # æŠ•ç¨¿é »åº¦
    posting_freq = calculate_posting_frequency(tweets)

    # æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘
    time_distribution = analyze_posting_time(tweets)

    return {
        "username": username,
        "total_tweets": len(tweets),
        "avg_likes": mean(likes) if likes else 0,
        "median_likes": median(likes) if likes else 0,
        "avg_retweets": mean(retweets) if retweets else 0,
        "median_retweets": median(retweets) if retweets else 0,
        "avg_replies": mean(replies) if replies else 0,
        "median_replies": median(replies) if replies else 0,
        "top5_tweets": top5,
        "bottom5_tweets": bottom5,
        "all_tweets": tweets,  # å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚ç³»åˆ—åˆ†æç”¨ï¼‰
        "opening_patterns": dict(pattern_counter),
        "emotions": dict(emotion_counter),
        "themes": dict(theme_counter),
        "cta_rate": cta_rate,
        "url_rate": url_rate,
        "avg_line_breaks": mean(line_breaks) if line_breaks else 0,
        "median_line_breaks": median(line_breaks) if line_breaks else 0,
        "avg_char_count": mean(char_counts) if char_counts else 0,
        "median_char_count": median(char_counts) if char_counts else 0,
        "posting_frequency": posting_freq,
        "time_distribution": time_distribution,
    }


def format_tweet_for_markdown(tweet, rank):
    """æŠ•ç¨¿ã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    text = tweet.get("text", "").replace("\n", " ")[:100]  # æœ€åˆã®100æ–‡å­—
    likes = tweet.get("likeCount", 0)
    retweets = tweet.get("retweetCount", 0)
    replies = tweet.get("replyCount", 0)
    score = tweet.get("engagement_score", 0)

    username = tweet.get("author", {}).get("userName", "")
    tweet_id = tweet.get("id", "")
    url = f"https://x.com/{username}/status/{tweet_id}" if username and tweet_id else ""

    return f"{rank}. **{text}...** \n   ã„ã„ã­: {likes} | RT: {retweets} | ãƒªãƒ—: {replies} | ã‚¹ã‚³ã‚¢: {score}\n   [{url}]({url})\n"


def compare_top_bottom_posts(analysis):
    """ãƒã‚ºã£ãŸæŠ•ç¨¿ã¨ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã‚’æ¯”è¼ƒåˆ†æ"""
    top5 = analysis["top5_tweets"]
    bottom5 = analysis["bottom5_tweets"]

    def analyze_group(posts):
        """æŠ•ç¨¿ã‚°ãƒ«ãƒ¼ãƒ—ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        if not posts:
            return {}
        return {
            "avg_chars": mean([p["char_count"] for p in posts]),
            "avg_line_breaks": mean([p["line_breaks"] for p in posts]),
            "opening_patterns": Counter([p["opening_pattern"] for p in posts]),
            "cta_rate": sum(1 for p in posts if p["has_cta"]) / len(posts) * 100,
            "emotions": Counter([e for p in posts for e in p["emotions"]]),
            "themes": Counter([p["theme"] for p in posts]),
            "url_rate": sum(1 for p in posts if p["has_url"]) / len(posts) * 100,
            "arrow_rate": sum(1 for p in posts if p["special_chars"]["çŸ¢å°"]) / len(posts) * 100,
            "bracket_rate": sum(1 for p in posts if p["special_chars"]["æ‹¬å¼§"]) / len(posts) * 100,
            "symbol_rate": sum(1 for p in posts if p["special_chars"]["è¨˜å·"]) / len(posts) * 100,
        }

    top_stats = analyze_group(top5)
    bottom_stats = analyze_group(bottom5)

    return {
        "top": top_stats,
        "bottom": bottom_stats,
    }


def analyze_cross_account_patterns(all_analyses):
    """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé–“ã®æ¨ªæ–­åˆ†æ"""
    if not all_analyses:
        return {}

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    rankings = {
        "likes": sorted(all_analyses, key=lambda x: x["avg_likes"], reverse=True),
        "retweets": sorted(all_analyses, key=lambda x: x["avg_retweets"], reverse=True),
        "replies": sorted(all_analyses, key=lambda x: x["avg_replies"], reverse=True),
    }

    # æŠ•ç¨¿é »åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚
    freq_engagement = [
        {
            "username": a["username"],
            "posting_freq": a["posting_frequency"],
            "avg_likes": a["avg_likes"],
        }
        for a in all_analyses
    ]

    # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å¾—æ„ãƒ‘ã‚¿ãƒ¼ãƒ³
    strong_patterns = []
    for analysis in all_analyses:
        top5 = analysis["top5_tweets"]
        if top5:
            top_patterns = Counter([t["opening_pattern"] for t in top5])
            top_themes = Counter([t["theme"] for t in top5])
            top_emotions = Counter([e for t in top5 for e in t["emotions"]])

            strong_patterns.append({
                "username": analysis["username"],
                "best_opening": top_patterns.most_common(1)[0][0] if top_patterns else "ä¸æ˜",
                "best_theme": top_themes.most_common(1)[0][0] if top_themes else "ä¸æ˜",
                "best_emotion": top_emotions.most_common(1)[0][0] if top_emotions else "ä¸æ˜",
            })

    return {
        "rankings": rankings,
        "freq_engagement": freq_engagement,
        "strong_patterns": strong_patterns,
    }


def generate_recommendations(all_analyses):
    """æ‹“å·³ã¸ã®å…·ä½“çš„ææ¡ˆã‚’ç”Ÿæˆ"""
    if not all_analyses:
        return {"should_do": [], "should_avoid": []}

    # å…¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®TOP5æŠ•ç¨¿ã‚’é›†è¨ˆ
    all_top_tweets = []
    for analysis in all_analyses:
        all_top_tweets.extend(analysis["top5_tweets"])

    # å…¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®BOTTOM5æŠ•ç¨¿ã‚’é›†è¨ˆ
    all_bottom_tweets = []
    for analysis in all_analyses:
        all_bottom_tweets.extend(analysis["bottom5_tweets"])

    # ãƒã‚ºã£ãŸæŠ•ç¨¿ã®å…±é€šç‚¹ã‚’åˆ†æ
    if all_top_tweets:
        top_patterns = Counter([t["opening_pattern"] for t in all_top_tweets])
        top_themes = Counter([t["theme"] for t in all_top_tweets])
        top_emotions = Counter([e for t in all_top_tweets for e in t["emotions"]])
        top_cta_rate = sum(1 for t in all_top_tweets if t["has_cta"]) / len(all_top_tweets) * 100
        top_url_rate = sum(1 for t in all_top_tweets if t["has_url"]) / len(all_top_tweets) * 100
        top_avg_chars = mean([t["char_count"] for t in all_top_tweets])
        top_avg_line_breaks = mean([t["line_breaks"] for t in all_top_tweets])

    # ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®å…±é€šç‚¹ã‚’åˆ†æ
    if all_bottom_tweets:
        bottom_patterns = Counter([t["opening_pattern"] for t in all_bottom_tweets])
        bottom_themes = Counter([t["theme"] for t in all_bottom_tweets])
        bottom_cta_rate = sum(1 for t in all_bottom_tweets if t["has_cta"]) / len(all_bottom_tweets) * 100
        bottom_avg_chars = mean([t["char_count"] for t in all_bottom_tweets])

    # ææ¡ˆã‚’ç”Ÿæˆ
    should_do = []
    should_avoid = []

    # çœŸä¼¼ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
    if all_top_tweets:
        # 1. å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³
        best_pattern = top_patterns.most_common(1)[0]
        should_do.append({
            "priority": 1,
            "action": f"å†’é ­ã¯ã€Œ{best_pattern[0]}ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã†",
            "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã®{best_pattern[1]/len(all_top_tweets)*100:.1f}%ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨",
        })

        # 2. ãƒ†ãƒ¼ãƒ
        best_theme = top_themes.most_common(1)[0]
        should_do.append({
            "priority": 2,
            "action": f"ã€Œ{best_theme[0]}ã€ç³»ã®æŠ•ç¨¿ã‚’å¢—ã‚„ã™",
            "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã®{best_theme[1]/len(all_top_tweets)*100:.1f}%ãŒã“ã®ãƒ†ãƒ¼ãƒ",
        })

        # 3. æ„Ÿæƒ…
        best_emotion = top_emotions.most_common(1)[0]
        should_do.append({
            "priority": 3,
            "action": f"ã€Œ{best_emotion[0]}ã€ã®æ„Ÿæƒ…ã‚’å‘¼ã³èµ·ã“ã™è¡¨ç¾ã‚’ä½¿ã†",
            "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã§æœ€ã‚‚å¤šã„æ„Ÿæƒ…ï¼ˆ{best_emotion[1]}å›å‡ºç¾ï¼‰",
        })

        # 4. æ–‡å­—æ•°
        should_do.append({
            "priority": 4,
            "action": f"æ–‡å­—æ•°ã¯{top_avg_chars:.0f}æ–‡å­—å‰å¾Œã‚’ç›®å®‰ã«ã™ã‚‹",
            "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã®å¹³å‡æ–‡å­—æ•°",
        })

        # 5. CTAã¾ãŸã¯URL
        if top_cta_rate > 40:
            should_do.append({
                "priority": 5,
                "action": "CTAã‚’ç©æ¥µçš„ã«å…¥ã‚Œã‚‹",
                "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã®{top_cta_rate:.1f}%ãŒCTAã‚’ä½¿ç”¨",
            })
        elif top_url_rate > 30:
            should_do.append({
                "priority": 5,
                "action": "URLã‚’å«ã‚ã‚‹ï¼ˆå‚è€ƒãƒªãƒ³ã‚¯ã‚„è‡ªåˆ†ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰",
                "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã®{top_url_rate:.1f}%ãŒURLã‚’å«ã‚€",
            })
        else:
            should_do.append({
                "priority": 5,
                "action": f"æ”¹è¡Œã‚’{top_avg_line_breaks:.0f}å›ç¨‹åº¦ä½¿ã£ã¦èª­ã¿ã‚„ã™ãã™ã‚‹",
                "reason": f"ãƒã‚ºã£ãŸæŠ•ç¨¿ã®å¹³å‡æ”¹è¡Œæ•°",
            })

    # é¿ã‘ã‚‹ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³
    if all_bottom_tweets:
        # 1. ãƒã‚ºã‚‰ãªã‹ã£ãŸãƒ‘ã‚¿ãƒ¼ãƒ³
        worst_pattern = bottom_patterns.most_common(1)[0]
        should_avoid.append({
            "action": f"ã€Œ{worst_pattern[0]}ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é¿ã‘ã‚‹",
            "reason": f"ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®{worst_pattern[1]/len(all_bottom_tweets)*100:.1f}%ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³",
        })

        # 2. ãƒã‚ºã‚‰ãªã‹ã£ãŸãƒ†ãƒ¼ãƒ
        worst_theme = bottom_themes.most_common(1)[0]
        should_avoid.append({
            "action": f"ã€Œ{worst_theme[0]}ã€ç³»ã®æŠ•ç¨¿ã¯æ§ãˆã‚ã«ã™ã‚‹",
            "reason": f"ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®{worst_theme[1]/len(all_bottom_tweets)*100:.1f}%ãŒã“ã®ãƒ†ãƒ¼ãƒ",
        })

        # 3. æ–‡å­—æ•°
        if bottom_avg_chars < top_avg_chars * 0.7:
            should_avoid.append({
                "action": f"{bottom_avg_chars:.0f}æ–‡å­—ä»¥ä¸‹ã®çŸ­ã™ãã‚‹æŠ•ç¨¿ã¯é¿ã‘ã‚‹",
                "reason": "ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®å¹³å‡æ–‡å­—æ•°ãŒå°‘ãªã™ãã‚‹",
            })
        else:
            should_avoid.append({
                "action": f"CTAãªã—ã®æŠ•ç¨¿ã‚’é€£æŠ•ã—ãªã„",
                "reason": f"ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®CTAä½¿ç”¨ç‡ã¯{bottom_cta_rate:.1f}%ã®ã¿",
            })

    return {
        "should_do": should_do[:5],
        "should_avoid": should_avoid[:3],
    }


def generate_markdown_report(all_analyses, output_path):
    """Markdownå½¢å¼ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    md_content = f"""# Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆè©³ç´°ç‰ˆï¼‰

**ç”Ÿæˆæ—¥æ™‚**: {today}
**åˆ†æã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°**: {len(all_analyses)}å€‹

---

"""

    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé–“æ¯”è¼ƒåˆ†æ
    if len(all_analyses) > 1:
        md_content += "## ğŸ“Š å…¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¨ªæ–­æ¯”è¼ƒ\n\n"
        cross_analysis = analyze_cross_account_patterns(all_analyses)

        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        md_content += "### ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n"
        md_content += "#### å¹³å‡ã„ã„ã­æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n"
        for i, analysis in enumerate(cross_analysis["rankings"]["likes"], 1):
            md_content += f"{i}. **@{analysis['username']}**: {analysis['avg_likes']:.1f}ä»¶\n"
        md_content += "\n"

        md_content += "#### å¹³å‡RTæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n"
        for i, analysis in enumerate(cross_analysis["rankings"]["retweets"], 1):
            md_content += f"{i}. **@{analysis['username']}**: {analysis['avg_retweets']:.1f}ä»¶\n"
        md_content += "\n"

        md_content += "#### å¹³å‡ãƒªãƒ—ãƒ©ã‚¤æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n"
        for i, analysis in enumerate(cross_analysis["rankings"]["replies"], 1):
            md_content += f"{i}. **@{analysis['username']}**: {analysis['avg_replies']:.1f}ä»¶\n"
        md_content += "\n"

        # æŠ•ç¨¿é »åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚
        md_content += "### æŠ•ç¨¿é »åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚\n\n"
        md_content += "| ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ | æŠ•ç¨¿é »åº¦ï¼ˆä»¶/æ—¥ï¼‰ | å¹³å‡ã„ã„ã­æ•° |\n"
        md_content += "|-----------|----------------|-------------|\n"
        for item in cross_analysis["freq_engagement"]:
            md_content += f"| @{item['username']} | {item['posting_freq']:.2f} | {item['avg_likes']:.1f} |\n"
        md_content += "\n"

        # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å¾—æ„ãƒ‘ã‚¿ãƒ¼ãƒ³
        md_content += "### å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å¾—æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è¦§\n\n"
        md_content += "| ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ | åŠ¹æœçš„ãªå†’é ­ | åŠ¹æœçš„ãªãƒ†ãƒ¼ãƒ | åŠ¹æœçš„ãªæ„Ÿæƒ… |\n"
        md_content += "|-----------|------------|--------------|------------|\n"
        for item in cross_analysis["strong_patterns"]:
            md_content += f"| @{item['username']} | {item['best_opening']} | {item['best_theme']} | {item['best_emotion']} |\n"
        md_content += "\n---\n\n"

    # æ‹“å·³ã¸ã®å…·ä½“çš„ææ¡ˆ
    recommendations = generate_recommendations(all_analyses)
    md_content += "## ğŸ’¡ æ‹“å·³ã¸ã®å…·ä½“çš„ææ¡ˆ\n\n"
    md_content += "### âœ… çœŸä¼¼ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆé †ä½é †ï¼‰\n\n"
    for rec in recommendations["should_do"]:
        md_content += f"**{rec['priority']}. {rec['action']}**\n"
        md_content += f"   - æ ¹æ‹ : {rec['reason']}\n\n"

    md_content += "### âŒ é¿ã‘ã‚‹ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³\n\n"
    for rec in recommendations["should_avoid"]:
        md_content += f"- **{rec['action']}**\n"
        md_content += f"  - ç†ç”±: {rec['reason']}\n\n"

    md_content += "---\n\n"

    # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è©³ç´°åˆ†æ
    for analysis in all_analyses:
        username = analysis["username"]

        md_content += f"## ğŸ‘¤ @{username}\n\n"

        # åŸºæœ¬çµ±è¨ˆ
        md_content += "### ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ\n\n"
        md_content += f"- **æŠ•ç¨¿æ•°**: {analysis['total_tweets']}ä»¶\n"
        md_content += f"- **å¹³å‡ã„ã„ã­æ•°**: {analysis['avg_likes']:.1f} ï¼ˆä¸­å¤®å€¤: {analysis['median_likes']:.0f}ï¼‰\n"
        md_content += f"- **å¹³å‡RTæ•°**: {analysis['avg_retweets']:.1f} ï¼ˆä¸­å¤®å€¤: {analysis['median_retweets']:.0f}ï¼‰\n"
        md_content += f"- **å¹³å‡ãƒªãƒ—ãƒ©ã‚¤æ•°**: {analysis['avg_replies']:.1f} ï¼ˆä¸­å¤®å€¤: {analysis['median_replies']:.0f}ï¼‰\n"
        md_content += f"- **å¹³å‡æ–‡å­—æ•°**: {analysis['avg_char_count']:.0f}æ–‡å­— ï¼ˆä¸­å¤®å€¤: {analysis['median_char_count']:.0f}æ–‡å­—ï¼‰\n"
        md_content += f"- **æŠ•ç¨¿é »åº¦**: {analysis['posting_frequency']:.2f}ä»¶/æ—¥\n\n"

        # æŠ•ç¨¿æ™‚é–“å¸¯
        if analysis["time_distribution"]:
            md_content += "### â° æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘\n\n"
            for time_slot, value in analysis["time_distribution"].items():
                md_content += f"- {time_slot}: {value}\n"
            md_content += "\n"

        # ãƒã‚ºã£ãŸ vs ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®æ¯”è¼ƒåˆ†æ
        comparison = compare_top_bottom_posts(analysis)
        md_content += "### ğŸ”¥ vs ğŸ“‰ ãƒã‚ºã£ãŸæŠ•ç¨¿ vs ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ã®æ¯”è¼ƒ\n\n"
        md_content += "| é …ç›® | TOP5 | BOTTOM5 | å·®åˆ† |\n"
        md_content += "|------|------|---------|------|\n"

        top = comparison["top"]
        bottom = comparison["bottom"]

        md_content += f"| å¹³å‡æ–‡å­—æ•° | {top['avg_chars']:.0f}æ–‡å­— | {bottom['avg_chars']:.0f}æ–‡å­— | {top['avg_chars'] - bottom['avg_chars']:+.0f} |\n"
        md_content += f"| å¹³å‡æ”¹è¡Œæ•° | {top['avg_line_breaks']:.1f}å› | {bottom['avg_line_breaks']:.1f}å› | {top['avg_line_breaks'] - bottom['avg_line_breaks']:+.1f} |\n"
        md_content += f"| CTAä½¿ç”¨ç‡ | {top['cta_rate']:.0f}% | {bottom['cta_rate']:.0f}% | {top['cta_rate'] - bottom['cta_rate']:+.0f}% |\n"
        md_content += f"| URLä½¿ç”¨ç‡ | {top['url_rate']:.0f}% | {bottom['url_rate']:.0f}% | {top['url_rate'] - bottom['url_rate']:+.0f}% |\n"
        md_content += f"| çŸ¢å°ä½¿ç”¨ç‡ | {top['arrow_rate']:.0f}% | {bottom['arrow_rate']:.0f}% | {top['arrow_rate'] - bottom['arrow_rate']:+.0f}% |\n"
        md_content += f"| æ‹¬å¼§ä½¿ç”¨ç‡ | {top['bracket_rate']:.0f}% | {bottom['bracket_rate']:.0f}% | {top['bracket_rate'] - bottom['bracket_rate']:+.0f}% |\n"
        md_content += f"| è¨˜å·ä½¿ç”¨ç‡ | {top['symbol_rate']:.0f}% | {bottom['symbol_rate']:.0f}% | {top['symbol_rate'] - bottom['symbol_rate']:+.0f}% |\n"
        md_content += "\n"

        # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒ
        md_content += "**å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ:**\n\n"
        md_content += "- TOP5: " + ", ".join([f"{k}({v}ä»¶)" for k, v in top["opening_patterns"].most_common(3)]) + "\n"
        md_content += "- BOTTOM5: " + ", ".join([f"{k}({v}ä»¶)" for k, v in bottom["opening_patterns"].most_common(3)]) + "\n\n"

        # æ„Ÿæƒ…ã®æ¯”è¼ƒ
        md_content += "**æ„Ÿæƒ…ã®æ¯”è¼ƒ:**\n\n"
        md_content += "- TOP5: " + ", ".join([f"{k}({v}å›)" for k, v in top["emotions"].most_common(3)]) + "\n"
        md_content += "- BOTTOM5: " + ", ".join([f"{k}({v}å›)" for k, v in bottom["emotions"].most_common(3)]) + "\n\n"

        # ãƒ†ãƒ¼ãƒã®æ¯”è¼ƒ
        md_content += "**ãƒ†ãƒ¼ãƒã®æ¯”è¼ƒ:**\n\n"
        md_content += "- TOP5: " + ", ".join([f"{k}({v}ä»¶)" for k, v in top["themes"].most_common(3)]) + "\n"
        md_content += "- BOTTOM5: " + ", ".join([f"{k}({v}ä»¶)" for k, v in bottom["themes"].most_common(3)]) + "\n\n"

        # ãƒã‚ºã£ãŸæŠ•ç¨¿TOP5
        md_content += "### ğŸ”¥ æœ€ã‚‚ãƒã‚ºã£ãŸæŠ•ç¨¿ TOP5\n\n"
        for i, tweet in enumerate(analysis["top5_tweets"], 1):
            md_content += format_tweet_for_markdown(tweet, i)
        md_content += "\n"

        # ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿BOTTOM5
        md_content += "### ğŸ“‰ æœ€ã‚‚ãƒã‚ºã‚‰ãªã‹ã£ãŸæŠ•ç¨¿ BOTTOM5\n\n"
        for i, tweet in enumerate(analysis["bottom5_tweets"], 1):
            md_content += format_tweet_for_markdown(tweet, i)
        md_content += "\n"

        # æ™‚ç³»åˆ—åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        all_tweets = analysis["all_tweets"]
        if all_tweets:
            # æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
            sorted_by_time = sorted(all_tweets, key=lambda x: x.get("createdAt", ""))

            md_content += "### ğŸ“ˆ æ™‚ç³»åˆ—åˆ†æï¼ˆã„ã„ã­æ•°ã®æ¨ç§»ï¼‰\n\n"
            md_content += "ç›´è¿‘10ä»¶ã®æŠ•ç¨¿ã®ã„ã„ã­æ•°æ¨ç§»:\n\n"
            md_content += "| é †ä½ | æŠ•ç¨¿æ—¥æ™‚ | ã„ã„ã­æ•° | æœ¬æ–‡ï¼ˆæŠœç²‹ï¼‰ |\n"
            md_content += "|------|---------|---------|-------------|\n"

            for i, tweet in enumerate(sorted_by_time[-10:], 1):
                created = tweet.get("createdAt", "")[:10] if tweet.get("createdAt") else "ä¸æ˜"
                likes = tweet.get("likeCount", 0)
                text = tweet.get("text", "")[:30].replace("\n", " ")
                md_content += f"| {i} | {created} | {likes} | {text}... |\n"
            md_content += "\n"

            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            if len(sorted_by_time) >= 10:
                recent_5 = sorted_by_time[-5:]
                older_5 = sorted_by_time[-10:-5]
                recent_avg = mean([t.get("likeCount", 0) for t in recent_5])
                older_avg = mean([t.get("likeCount", 0) for t in older_5])
                trend = "ä¸Šæ˜‡å‚¾å‘" if recent_avg > older_avg else "ä¸‹é™å‚¾å‘" if recent_avg < older_avg else "æ¨ªã°ã„"

                md_content += f"**ãƒˆãƒ¬ãƒ³ãƒ‰**: {trend} ï¼ˆç›´è¿‘5ä»¶å¹³å‡: {recent_avg:.1f}ã€ãã®å‰5ä»¶å¹³å‡: {older_avg:.1f}ï¼‰\n\n"

        # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        md_content += "### ğŸ¯ å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n\n"
        total_patterns = sum(analysis["opening_patterns"].values())
        for pattern, count in sorted(analysis["opening_patterns"].items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_patterns * 100 if total_patterns > 0 else 0
            md_content += f"- **{pattern}**: {count}ä»¶ ({percentage:.1f}%)\n"
        md_content += "\n"

        # æ„Ÿæƒ…åˆ†æ
        md_content += "### ğŸ˜Š æ„Ÿæƒ…åˆ†æ\n\n"
        total_emotions = sum(analysis["emotions"].values())
        for emotion, count in sorted(analysis["emotions"].items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_emotions * 100 if total_emotions > 0 else 0
            md_content += f"- **{emotion}**: {count}å› ({percentage:.1f}%)\n"
        md_content += "\n"

        # ãƒ†ãƒ¼ãƒåˆ†é¡
        md_content += "### ğŸ“š ãƒ†ãƒ¼ãƒåˆ†é¡\n\n"
        total_themes = sum(analysis["themes"].values())
        for theme, count in sorted(analysis["themes"].items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_themes * 100 if total_themes > 0 else 0
            md_content += f"- **{theme}**: {count}ä»¶ ({percentage:.1f}%)\n"
        md_content += "\n"

        # ãã®ä»–ã®çµ±è¨ˆ
        md_content += "### ğŸ“Š ãã®ä»–ã®çµ±è¨ˆ\n\n"
        md_content += f"- **CTAä½¿ç”¨ç‡**: {analysis['cta_rate']:.1f}%\n"
        md_content += f"- **URLä½¿ç”¨ç‡**: {analysis['url_rate']:.1f}%\n"
        md_content += f"- **å¹³å‡æ”¹è¡Œæ•°**: {analysis['avg_line_breaks']:.1f} ï¼ˆä¸­å¤®å€¤: {analysis['median_line_breaks']:.0f}ï¼‰\n\n"

        md_content += "---\n\n"

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(md_content)

    print(f"è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_path}")


def save_to_excel(all_tweets_by_account, output_path):
    """å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’Excelã«ä¿å­˜ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã«ã‚·ãƒ¼ãƒˆåˆ†ã‘ï¼‰"""
    wb = Workbook()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒ¼ãƒˆã‚’å‰Šé™¤
    if "Sheet" in wb.sheetnames:
        wb.remove(wb["Sheet"])

    for username, tweets in all_tweets_by_account.items():
        # ã‚·ãƒ¼ãƒˆåï¼ˆæœ€å¤§31æ–‡å­—åˆ¶é™ï¼‰
        sheet_name = f"@{username}"[:31]
        ws = wb.create_sheet(title=sheet_name)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = ["æœ¬æ–‡", "ã„ã„ã­æ•°", "RTæ•°", "ãƒªãƒ—ãƒ©ã‚¤æ•°", "æŠ•ç¨¿æ—¥æ™‚", "æŠ•ç¨¿URL", "å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³", "CTAæœ‰ç„¡", "æ”¹è¡Œæ•°"]
        ws.append(headers)

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")

        for col_num, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col_num)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal="center", vertical="center")

        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’è¿½åŠ 
        for tweet in tweets:
            text = tweet.get("text", "")
            tweet_id = tweet.get("id", "")
            post_url = f"https://x.com/{username}/status/{tweet_id}" if tweet_id else ""

            ws.append([
                text,
                tweet.get("likeCount", 0),
                tweet.get("retweetCount", 0),
                tweet.get("replyCount", 0),
                tweet.get("createdAt", ""),
                post_url,
                classify_opening_pattern(text),
                "ã‚ã‚Š" if detect_cta(text) else "ãªã—",
                count_line_breaks(text),
            ])

        # æœ¬æ–‡åˆ—ï¼ˆAåˆ—ï¼‰ã®æŠ˜ã‚Šè¿”ã—è¨­å®š
        for row in range(2, len(tweets) + 2):
            cell = ws.cell(row=row, column=1)
            cell.alignment = Alignment(wrap_text=True, vertical="top")

        # åˆ—å¹…ã®èª¿æ•´
        column_widths = {
            1: 60,  # æœ¬æ–‡
            2: 12,  # ã„ã„ã­æ•°
            3: 12,  # RTæ•°
            4: 12,  # ãƒªãƒ—ãƒ©ã‚¤æ•°
            5: 25,  # æŠ•ç¨¿æ—¥æ™‚
            6: 50,  # æŠ•ç¨¿URL
            7: 15,  # å†’é ­ãƒ‘ã‚¿ãƒ¼ãƒ³
            8: 12,  # CTAæœ‰ç„¡
            9: 10,  # æ”¹è¡Œæ•°
        }

        for col_num, width in column_widths.items():
            ws.column_dimensions[get_column_letter(col_num)].width = width

        # ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
        ws.auto_filter.ref = f"A1:{get_column_letter(len(headers))}{len(tweets) + 1}"

    # ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    wb.save(output_path)
    print(f"Excelä¿å­˜å®Œäº†: {output_path}")


def main(test_mode=False, test_file=None):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    if test_mode:
        print("ã€ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã€‘")
    print("=" * 60)
    print()

    # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ã‚’å–å¾—
    all_tweets_by_account = {}
    all_analyses = []

    if test_mode:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        if not test_file:
            test_file = "output/buzz_posts_20260215.xlsx"

        all_tweets_by_account = load_test_data_from_excel(test_file)
        print()

        if not all_tweets_by_account:
            print("ã‚¨ãƒ©ãƒ¼: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
            sys.exit(1)

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã«åˆ†æ
        print("åˆ†æã‚’å®Ÿè¡Œä¸­...")
        for username, tweets in all_tweets_by_account.items():
            print(f"  @{username} ã‚’åˆ†æä¸­...")
            analysis = analyze_account(username, tweets)
            if analysis:
                all_analyses.append(analysis)
        print()

    else:
        # APIãƒ¢ãƒ¼ãƒ‰: é€šå¸¸ã®APIå–å¾—
        api_key = os.environ.get("TWITTER_API_KEY")
        if not api_key:
            print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° TWITTER_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print(".envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)

        print(f"å¯¾è±¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°: {len(TARGET_ACCOUNTS)}å€‹")
        print()

        for i, username in enumerate(TARGET_ACCOUNTS):
            print(f"[{i+1}/{len(TARGET_ACCOUNTS)}] @{username} ã‚’å‡¦ç†ä¸­...")

            tweets = fetch_user_tweets(username, api_key, count=100)

            if tweets:
                all_tweets_by_account[username] = tweets

                # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æã‚’å®Ÿè¡Œ
                analysis = analyze_account(username, tweets)
                if analysis:
                    all_analyses.append(analysis)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼šæœ€å¾Œã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä»¥å¤–ã¯å¾…æ©Ÿ
            if i < len(TARGET_ACCOUNTS) - 1:
                print("    ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§10ç§’å¾…æ©Ÿä¸­...")
                time.sleep(10)

            print()

    if not all_analyses:
        print("åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(0)

    print(f"åˆ†æå®Œäº†: {len(all_analyses)}ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")
    print()

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    today = datetime.now().strftime("%Y%m%d")
    output_dir = "output"
    md_path = os.path.join(output_dir, f"account_analysis_è©³ç´°_{today}.md")
    excel_path = os.path.join(output_dir, f"account_posts_{today}.xlsx")

    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    generate_markdown_report(all_analyses, md_path)

    # Excelä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼‰
    if not test_mode:
        print("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...")
        save_to_excel(all_tweets_by_account, excel_path)
    else:
        print("ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã€Excelç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    print()
    print("=" * 60)
    print("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)


if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(description="Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--test",
        action="store_true",
        help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã€APIå‘¼ã³å‡ºã—ãªã—ï¼‰",
    )
    parser.add_argument(
        "--test-file",
        type=str,
        default="output/buzz_posts_20260215.xlsx",
        help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ã™ã‚‹Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
    )

    args = parser.parse_args()

    main(test_mode=args.test, test_file=args.test_file)
